{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho0uIOH1_fLL"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<h1><center><font size=10>Artificial Intelligence and Machine Learning</center></font></h1>\n",
        "<h1><center>LLMs and Prompt Engineering</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI4_2mwl_ARc"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://images.pexels.com/photos/4185957/pexels-photo-4185957.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\" width=\"720\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=6>News Article Categorization and Summarization</font></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjncuDf2qugI"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VSf2D_F5iU"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLApwDNbFDEK"
      },
      "source": [
        "In the dynamic landscape of the media and news industry, the ability to swiftly categorize and curate content has become a strategic imperative. The vast volume of information demands efficient systems to organize and present content to the audience.\n",
        "\n",
        "The media industry, being the pulse of information dissemination, grapples with the continuous influx of news articles spanning diverse topics. Ensuring that the right articles reach the right audience promptly is not just a logistical necessity but a critical component in retaining and engaging audiences in an age of information overload.\n",
        "\n",
        "Common Industry Challenges:\n",
        "Amidst the ceaseless flow of news, organizations encounter challenges such as:\n",
        "- Information Overload: The sheer volume of news articles makes manual categorization impractical.\n",
        "- Timeliness: Delays in categorizing news articles can result in outdated or misplaced content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzSKXh2LsOvd"
      },
      "source": [
        "### Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdEWl1qZFNWM"
      },
      "source": [
        "E-news Express, a news aggregation startup, faces the challenge of categorizing the news articles collected. With news articles covering sports, entertainment, politics, and more, the need for an advanced and automated system to categorize them has become increasingly evident. The manual efforts required for categorizing such a diverse range of news articles are substantial, and human errors in the categorization of news articles can lead to reputational damage for the startup. There is also the factor of delays and potential inaccuracies. To streamline and optimize this process, the organization recognizes the imperative of adopting cutting-edge technologies, particularly machine learning, to automate and enhance the categorization of content.\n",
        "\n",
        "As a data scientist on the E-news Express data team, the task is to analyze the text in news articles and build an unsupervised learning model for categorizing them. The categorization will be followed up by a summarization of the news article so that users can read through the summary first and then click on a relevant link to dive in deeper if needed. The goal is to optimize the categorization and summarization process, ensuring timely and personalized delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saFx1pbT_zTP"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rJpifbLGVDe"
      },
      "source": [
        "- **Article**: The main body of the news article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNdcQy4-GSaG"
      },
      "source": [
        "## Installing and Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjdSxmUYGgt5",
        "outputId": "787b8286-46b7-44b8-b9c5-9a2ffa4b8ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installation for Google Colab with GPU (CUDA) support\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEoS7f7YG1m1",
        "outputId": "684861fe-049e-4ccb-a756-1aefeddf1a26"
      },
      "outputs": [],
      "source": [
        "# For downloading the models from HF Hub\n",
        "!pip install huggingface_hub datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jeipsrMkG29f"
      },
      "outputs": [],
      "source": [
        "# Importing library for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Function to download the model from the Hugging Face model hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Importing the Llama class from the llama_cpp module\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Importing the json library\n",
        "import json\n",
        "\n",
        "#Importing the datasets library from huggingface\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttQK4FHqHTar"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Download from a public URL (GitHub, S3, etc.)\n",
        "# Replace the URL below with your file's public URL\n",
        "!wget -q \"https://raw.githubusercontent.com/visubramaniam/AI-RAG-GENAI/main/data/articles.csv\" -O articles.csv\n",
        "\n",
        "# Option 2: Download from Google Drive using gdown (if file is shared publicly)\n",
        "# First, get the shareable link from Google Drive, extract the file ID\n",
        "# Example link: https://drive.google.com/file/d/FILE_ID/view?usp=sharing\n",
        "# !pip install gdown -q\n",
        "# !gdown \"FILE_ID\" -O articles.csv\n",
        "\n",
        "# Option 3: Download from AWS S3 (public bucket)\n",
        "# !wget -q \"https://YOUR_BUCKET.s3.amazonaws.com/articles.csv\" -O articles.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n2nMOzu6gsSn"
      },
      "outputs": [],
      "source": [
        "# Load the uploaded CSV file\n",
        "data = pd.read_csv(\"articles.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY7NrV60HeJS"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83KKgodxMWc6",
        "outputId": "cb04b526-816e-46b7-c638-c2d9bbc98e77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "718ccc0b-e9d9-40c1-a310-7a8e1c1f2fd8",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses."
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services."
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow."
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon."
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday."
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e1c18f5a-30a1-4b31-94d2-d2906ef21b31\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1c18f5a-30a1-4b31-94d2-d2906ef21b31')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1c18f5a-30a1-4b31-94d2-d2906ef21b31 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1c18f5a-30a1-4b31-94d2-d2906ef21b31');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article\n",
              "0  A New Push to Loosen New York's Divorce Law La...\n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...\n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...\n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...\n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl..."
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Printing the first few rows.\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwqyhbHEhzP2",
        "outputId": "c6c32a3d-b1ef-414f-dbbb-8375bbfeb95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "#Number of news articles.\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT3Vzwl1G8VZ"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSt6RorEHC3_"
      },
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WKPWkxzSHAQS"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\" # the model is in gguf format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ddaf5ffd281d451aa4b670747fbdb394",
            "d5b91c25e6954f32ad2544a65f0329ba",
            "f6cd5e994f4947cbb52d97c74f4ef2f6",
            "674582a985be4560a0aaf05d243a0d73",
            "e0de0dbf405a447e80208d19c9c5a3c0",
            "4b042fc0a45b48c3909406a751542461",
            "acd033b9252f4428a9cb4852c802ba9d",
            "94a1117cfd4747d38b6547e073061da1",
            "52ce15578f764c31b30e1ca603a48a5c",
            "bc4ea4ed529a4822b978d9b5b0182425",
            "2776eb66d0404149b1e0cac94a83d014"
          ]
        },
        "id": "XqNTzSVaHGBf",
        "outputId": "a7d0462a-ac27-4662-83e1-07491684137f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d6d5dadaf5a4ecda9aff8f73b1e6a4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llama-2-13b-chat.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Using hf_hub_download to download a model from the Hugging Face model hub\n",
        "# The repo_id parameter specifies the model name or path in the Hugging Face repository\n",
        "# The filename parameter specifies the name of the file to download\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
        "    filename=model_basename,\n",
        "    token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mECqmQhHILO",
        "outputId": "07645f9c-12ed-4396-e24f-263caa445eb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q5_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q5_K - Medium\n",
            "print_info: file size   = 8.60 GiB (5.67 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 40\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 5120\n",
            "print_info: n_embd_v_gqa     = 5120\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 13824\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 13B\n",
            "print_info: model params     = 13.02 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 40 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 41/41 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8694.21 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   107.42 MiB\n",
            "...................................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  36: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  37: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  38: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  39: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  3200.00 MiB\n",
            "llama_kv_cache_unified: size = 3200.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 2904\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   388.01 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    38.01 MiB\n",
            "llama_context: graph nodes  = 1406\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # CPU cores\n",
        "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096,  # Context window\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp9rd_qBHKXg"
      },
      "source": [
        "### Defining Model Response Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dgel68_iHM6c"
      },
      "outputs": [],
      "source": [
        "def generate_llama_response(instruction, review):\n",
        "\n",
        "    # System message explicitly instructing not to include the review text\n",
        "    system_message = \"\"\"\n",
        "        [INST]<<SYS>>\n",
        "        {}\n",
        "        <</SYS>>[/INST]\n",
        "    \"\"\".format(instruction)\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = f\"{review}\\n{system_message}\"\n",
        "\n",
        "    # Generate a response from the LLaMA model\n",
        "    response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=1024,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        top_k=50,\n",
        "        stop=['INST'],\n",
        "        echo=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    # Extract the sentiment from the response\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deocpc2ZHQYq"
      },
      "source": [
        "- **`max_tokens`**: This parameter **specifies the maximum number of tokens that the model should generate** in response to the prompt.\n",
        "\n",
        "- **`temperature`**: This parameter **controls the randomness of the generated response**. A higher temperature value will result in a more random response, while a lower temperature value will result in a more predictable response.\n",
        "\n",
        "- **`top_p`**: This parameter **controls the diversity of the generated response by establishing a cumulative probability cutoff for token selection**. A higher value of top_p will result in a more diverse response, while a lower value will result in a less diverse response.\n",
        "\n",
        "- **`repeat_penalty`**: This parameter **controls the penalty for repeating tokens in the generated response**. A higher value of repeat_penalty will result in a lower probability of repeating tokens, while a lower value will result in a higher probability of repeating tokens.\n",
        "\n",
        "- **`top_k`**: This parameter **controls the maximum number of most-likely next tokens to consider** when generating the response at each step.\n",
        "\n",
        "- **`stop`**: This parameter is a **list of tokens that are used to dynamically stop response generation** whenever the tokens in the list are encountered.\n",
        "\n",
        "- **`echo`**: This parameter **controls whether the input (prompt) to the model should be returned** in the model response.\n",
        "\n",
        "- **`seed`**: This parameter **specifies a seed value that helps replicate results**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0T86JLANI5b"
      },
      "source": [
        "## Classifying the news articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wl7GrSMCfqHi"
      },
      "outputs": [],
      "source": [
        "data_1 = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5F2DE__7NKdS"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_1 = \"\"\"\n",
        "    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only depending upon the content of the article:\n",
        "    - World\n",
        "    - Sports\n",
        "    - Business\n",
        "    - Sci/Tech\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve0R-e-kNMtG",
        "outputId": "aeb6f3b7-877b-4459-d782-a25a278a806b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     668.57 ms /   129 tokens (    5.18 ms per token,   192.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1252.28 ms /    21 runs   (   59.63 ms per token,    16.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    1937.91 ms /   150 tokens\n",
            "llama_perf_context_print:    graphs reused =         20\n",
            "Llama.generate: 1 prefix-match hit, remaining 116 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     216.70 ms /   116 tokens (    1.87 ms per token,   535.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8411.17 ms /   137 runs   (   61.40 ms per token,    16.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    8755.94 ms /   253 tokens\n",
            "llama_perf_context_print:    graphs reused =        132\n",
            "Llama.generate: 1 prefix-match hit, remaining 154 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     312.27 ms /   154 tokens (    2.03 ms per token,   493.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6831.98 ms /   106 runs   (   64.45 ms per token,    15.52 tokens per second)\n",
            "llama_perf_context_print:       total time =    7238.16 ms /   260 tokens\n",
            "llama_perf_context_print:    graphs reused =        101\n",
            "Llama.generate: 1 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     234.19 ms /   128 tokens (    1.83 ms per token,   546.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7117.50 ms /   107 runs   (   66.52 ms per token,    15.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    7449.52 ms /   235 tokens\n",
            "llama_perf_context_print:    graphs reused =        103\n",
            "Llama.generate: 1 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     352.70 ms /   162 tokens (    2.18 ms per token,   459.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7235.60 ms /   106 runs   (   68.26 ms per token,    14.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    7684.44 ms /   268 tokens\n",
            "llama_perf_context_print:    graphs reused =        102\n",
            "Llama.generate: 1 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     355.49 ms /   163 tokens (    2.18 ms per token,   458.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8159.97 ms /   116 runs   (   70.34 ms per token,    14.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    8618.63 ms /   279 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 160 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.53 ms /   160 tokens (    2.10 ms per token,   475.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2735.95 ms /    38 runs   (   72.00 ms per token,    13.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    3103.78 ms /   198 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 106 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     215.65 ms /   106 tokens (    2.03 ms per token,   491.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7353.84 ms /   100 runs   (   73.54 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    7661.59 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =         96\n",
            "Llama.generate: 1 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.55 ms /   173 tokens (    2.15 ms per token,   464.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19339.69 ms /   244 runs   (   79.26 ms per token,    12.62 tokens per second)\n",
            "llama_perf_context_print:       total time =   19966.31 ms /   417 tokens\n",
            "llama_perf_context_print:    graphs reused =        235\n",
            "Llama.generate: 1 prefix-match hit, remaining 152 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     352.79 ms /   152 tokens (    2.32 ms per token,   430.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7741.51 ms /    96 runs   (   80.64 ms per token,    12.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    8180.76 ms /   248 tokens\n",
            "llama_perf_context_print:    graphs reused =         92\n",
            "Llama.generate: 1 prefix-match hit, remaining 114 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     239.96 ms /   114 tokens (    2.10 ms per token,   475.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5844.25 ms /    75 runs   (   77.92 ms per token,    12.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    6150.58 ms /   189 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     248.15 ms /   128 tokens (    1.94 ms per token,   515.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10568.91 ms /   138 runs   (   76.59 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =   10945.98 ms /   266 tokens\n",
            "llama_perf_context_print:    graphs reused =        133\n",
            "Llama.generate: 1 prefix-match hit, remaining 132 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     333.99 ms /   132 tokens (    2.53 ms per token,   395.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2776.28 ms /    37 runs   (   75.03 ms per token,    13.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    3141.07 ms /   169 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 127 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     246.30 ms /   127 tokens (    1.94 ms per token,   515.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6525.66 ms /    88 runs   (   74.16 ms per token,    13.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    6849.55 ms /   215 tokens\n",
            "llama_perf_context_print:    graphs reused =         85\n",
            "Llama.generate: 1 prefix-match hit, remaining 151 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     344.99 ms /   151 tokens (    2.28 ms per token,   437.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7255.22 ms /    98 runs   (   74.03 ms per token,    13.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    7688.75 ms /   249 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 198 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     417.93 ms /   198 tokens (    2.11 ms per token,   473.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11256.59 ms /   150 runs   (   75.04 ms per token,    13.33 tokens per second)\n",
            "llama_perf_context_print:       total time =   11818.80 ms /   348 tokens\n",
            "llama_perf_context_print:    graphs reused =        145\n",
            "Llama.generate: 1 prefix-match hit, remaining 111 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     219.74 ms /   111 tokens (    1.98 ms per token,   505.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5432.61 ms /    73 runs   (   74.42 ms per token,    13.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    5715.34 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            "Llama.generate: 1 prefix-match hit, remaining 121 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     242.23 ms /   121 tokens (    2.00 ms per token,   499.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5280.30 ms /    70 runs   (   75.43 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    5582.41 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =         67\n",
            "Llama.generate: 1 prefix-match hit, remaining 152 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     344.30 ms /   152 tokens (    2.27 ms per token,   441.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8826.48 ms /   115 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    9278.15 ms /   267 tokens\n",
            "llama_perf_context_print:    graphs reused =        110\n",
            "Llama.generate: 1 prefix-match hit, remaining 136 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     340.44 ms /   136 tokens (    2.50 ms per token,   399.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4219.45 ms /    55 runs   (   76.72 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    4604.45 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =         53\n",
            "Llama.generate: 1 prefix-match hit, remaining 131 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     340.31 ms /   131 tokens (    2.60 ms per token,   384.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6086.29 ms /    79 runs   (   77.04 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    6498.31 ms /   210 tokens\n",
            "llama_perf_context_print:    graphs reused =         76\n",
            "Llama.generate: 1 prefix-match hit, remaining 135 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.43 ms /   135 tokens (    2.51 ms per token,   398.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7397.17 ms /    96 runs   (   77.05 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    7821.73 ms /   231 tokens\n",
            "llama_perf_context_print:    graphs reused =         92\n",
            "Llama.generate: 1 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.74 ms /   129 tokens (    2.63 ms per token,   380.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5520.81 ms /    72 runs   (   76.68 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    5920.83 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            "Llama.generate: 1 prefix-match hit, remaining 143 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.93 ms /   143 tokens (    2.36 ms per token,   424.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6963.72 ms /    91 runs   (   76.52 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    7380.02 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     342.79 ms /   159 tokens (    2.16 ms per token,   463.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9272.67 ms /   121 runs   (   76.63 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    9726.21 ms /   280 tokens\n",
            "llama_perf_context_print:    graphs reused =        117\n",
            "Llama.generate: 1 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     246.18 ms /   128 tokens (    1.92 ms per token,   519.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6890.91 ms /    91 runs   (   75.72 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    7215.74 ms /   219 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 137 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     339.35 ms /   137 tokens (    2.48 ms per token,   403.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7879.24 ms /   104 runs   (   75.76 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    8313.69 ms /   241 tokens\n",
            "llama_perf_context_print:    graphs reused =        100\n",
            "Llama.generate: 1 prefix-match hit, remaining 131 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.56 ms /   131 tokens (    2.55 ms per token,   391.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6886.89 ms /    91 runs   (   75.68 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    7301.73 ms /   222 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 119 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     238.14 ms /   119 tokens (    2.00 ms per token,   499.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7228.73 ms /    96 runs   (   75.30 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    7552.87 ms /   215 tokens\n",
            "llama_perf_context_print:    graphs reused =         92\n",
            "Llama.generate: 1 prefix-match hit, remaining 172 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     370.33 ms /   172 tokens (    2.15 ms per token,   464.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10244.32 ms /   135 runs   (   75.88 ms per token,    13.18 tokens per second)\n",
            "llama_perf_context_print:       total time =   10739.84 ms /   307 tokens\n",
            "llama_perf_context_print:    graphs reused =        130\n",
            "Llama.generate: 1 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.08 ms /   129 tokens (    2.59 ms per token,   386.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6865.34 ms /    91 runs   (   75.44 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    7281.82 ms /   220 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 135 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     331.88 ms /   135 tokens (    2.46 ms per token,   406.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7045.12 ms /    93 runs   (   75.75 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    7459.09 ms /   228 tokens\n",
            "llama_perf_context_print:    graphs reused =         89\n",
            "Llama.generate: 1 prefix-match hit, remaining 135 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.91 ms /   135 tokens (    2.50 ms per token,   400.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7262.37 ms /    96 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    7685.39 ms /   231 tokens\n",
            "llama_perf_context_print:    graphs reused =         92\n",
            "Llama.generate: 1 prefix-match hit, remaining 123 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     245.72 ms /   123 tokens (    2.00 ms per token,   500.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5903.88 ms /    78 runs   (   75.69 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    6218.14 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 146 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     335.20 ms /   146 tokens (    2.30 ms per token,   435.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7917.32 ms /   104 runs   (   76.13 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    8346.97 ms /   250 tokens\n",
            "llama_perf_context_print:    graphs reused =        100\n",
            "Llama.generate: 1 prefix-match hit, remaining 133 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     339.34 ms /   133 tokens (    2.55 ms per token,   391.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12299.19 ms /   161 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =   12792.93 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =        155\n",
            "Llama.generate: 1 prefix-match hit, remaining 142 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.51 ms /   142 tokens (    2.40 ms per token,   415.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1591.89 ms /    21 runs   (   75.80 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    1950.47 ms /   163 tokens\n",
            "llama_perf_context_print:    graphs reused =         19\n",
            "Llama.generate: 1 prefix-match hit, remaining 119 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     243.69 ms /   119 tokens (    2.05 ms per token,   488.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2037.74 ms /    27 runs   (   75.47 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2303.92 ms /   146 tokens\n",
            "llama_perf_context_print:    graphs reused =         25\n",
            "Llama.generate: 1 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     248.26 ms /   128 tokens (    1.94 ms per token,   515.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8853.06 ms /   116 runs   (   76.32 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    9206.72 ms /   244 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.58 ms /   129 tokens (    2.59 ms per token,   385.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7014.77 ms /    92 runs   (   76.25 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    7431.00 ms /   221 tokens\n",
            "llama_perf_context_print:    graphs reused =         89\n",
            "Llama.generate: 1 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     335.00 ms /   129 tokens (    2.60 ms per token,   385.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6948.08 ms /    91 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    7364.59 ms /   220 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 134 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.27 ms /   134 tokens (    2.51 ms per token,   398.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6947.06 ms /    91 runs   (   76.34 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    7363.26 ms /   225 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 145 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     340.61 ms /   145 tokens (    2.35 ms per token,   425.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7814.58 ms /   102 runs   (   76.61 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    8242.90 ms /   247 tokens\n",
            "llama_perf_context_print:    graphs reused =         98\n",
            "Llama.generate: 1 prefix-match hit, remaining 155 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     345.71 ms /   155 tokens (    2.23 ms per token,   448.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8978.26 ms /   117 runs   (   76.74 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    9431.51 ms /   272 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     369.12 ms /   161 tokens (    2.29 ms per token,   436.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8449.77 ms /   110 runs   (   76.82 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    8921.43 ms /   271 tokens\n",
            "llama_perf_context_print:    graphs reused =        106\n",
            "Llama.generate: 1 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.37 ms /   141 tokens (    2.40 ms per token,   416.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7883.84 ms /   103 runs   (   76.54 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    8313.45 ms /   244 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 132 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.88 ms /   132 tokens (    2.55 ms per token,   391.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6201.48 ms /    81 runs   (   76.56 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    6609.21 ms /   213 tokens\n",
            "llama_perf_context_print:    graphs reused =         78\n",
            "Llama.generate: 1 prefix-match hit, remaining 155 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     346.09 ms /   155 tokens (    2.23 ms per token,   447.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10632.80 ms /   138 runs   (   77.05 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =   11111.06 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =        132\n",
            "Llama.generate: 1 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     380.87 ms /   174 tokens (    2.19 ms per token,   456.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9243.32 ms /   120 runs   (   77.03 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    9738.13 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =        115\n",
            "Llama.generate: 1 prefix-match hit, remaining 132 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     330.12 ms /   132 tokens (    2.50 ms per token,   399.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7194.23 ms /    94 runs   (   76.53 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    7609.04 ms /   226 tokens\n",
            "llama_perf_context_print:    graphs reused =         90\n",
            "Llama.generate: 1 prefix-match hit, remaining 160 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     348.06 ms /   160 tokens (    2.18 ms per token,   459.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5936.15 ms /    77 runs   (   77.09 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    6350.50 ms /   237 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     376.57 ms /   165 tokens (    2.28 ms per token,   438.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10001.77 ms /   130 runs   (   76.94 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =   10501.87 ms /   295 tokens\n",
            "llama_perf_context_print:    graphs reused =        125\n",
            "Llama.generate: 1 prefix-match hit, remaining 112 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     215.65 ms /   112 tokens (    1.93 ms per token,   519.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6337.28 ms /    83 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    6626.00 ms /   195 tokens\n",
            "llama_perf_context_print:    graphs reused =         79\n",
            "Llama.generate: 1 prefix-match hit, remaining 148 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     343.21 ms /   148 tokens (    2.32 ms per token,   431.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8699.03 ms /   113 runs   (   76.98 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    9142.99 ms /   261 tokens\n",
            "llama_perf_context_print:    graphs reused =        108\n",
            "Llama.generate: 1 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.67 ms /   141 tokens (    2.37 ms per token,   421.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7846.58 ms /   102 runs   (   76.93 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    8272.58 ms /   243 tokens\n",
            "llama_perf_context_print:    graphs reused =         98\n",
            "Llama.generate: 1 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.60 ms /   164 tokens (    2.27 ms per token,   441.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4833.02 ms /    63 runs   (   76.71 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    5258.97 ms /   227 tokens\n",
            "llama_perf_context_print:    graphs reused =         60\n",
            "Llama.generate: 1 prefix-match hit, remaining 123 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     246.26 ms /   123 tokens (    2.00 ms per token,   499.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8875.04 ms /   116 runs   (   76.51 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    9228.02 ms /   239 tokens\n",
            "llama_perf_context_print:    graphs reused =        111\n",
            "Llama.generate: 1 prefix-match hit, remaining 145 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.68 ms /   145 tokens (    2.36 ms per token,   424.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7914.28 ms /   103 runs   (   76.84 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    8346.99 ms /   248 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 139 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     343.63 ms /   139 tokens (    2.47 ms per token,   404.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13619.32 ms /   177 runs   (   76.95 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =   14137.34 ms /   316 tokens\n",
            "llama_perf_context_print:    graphs reused =        171\n",
            "Llama.generate: 1 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     241.49 ms /   124 tokens (    1.95 ms per token,   513.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6396.41 ms /    84 runs   (   76.15 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    6711.39 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 142 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.53 ms /   142 tokens (    2.38 ms per token,   419.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7878.22 ms /   103 runs   (   76.49 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    8307.49 ms /   245 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     369.60 ms /   163 tokens (    2.27 ms per token,   441.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7651.98 ms /   100 runs   (   76.52 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    8111.35 ms /   263 tokens\n",
            "llama_perf_context_print:    graphs reused =         96\n",
            "Llama.generate: 1 prefix-match hit, remaining 119 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     238.09 ms /   119 tokens (    2.00 ms per token,   499.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8154.86 ms /   107 runs   (   76.21 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    8486.79 ms /   226 tokens\n",
            "llama_perf_context_print:    graphs reused =        102\n",
            "Llama.generate: 1 prefix-match hit, remaining 146 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     343.63 ms /   146 tokens (    2.35 ms per token,   424.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8244.81 ms /   108 runs   (   76.34 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    8687.27 ms /   254 tokens\n",
            "llama_perf_context_print:    graphs reused =        104\n",
            "Llama.generate: 1 prefix-match hit, remaining 127 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     243.89 ms /   127 tokens (    1.92 ms per token,   520.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6594.73 ms /    87 runs   (   75.80 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    6918.29 ms /   214 tokens\n",
            "llama_perf_context_print:    graphs reused =         84\n",
            "Llama.generate: 1 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.24 ms /   161 tokens (    2.32 ms per token,   430.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =   23439.77 ms /   305 runs   (   76.85 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:       total time =   24145.62 ms /   466 tokens\n",
            "llama_perf_context_print:    graphs reused =        295\n",
            "Llama.generate: 1 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     237.79 ms /   124 tokens (    1.92 ms per token,   521.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6584.73 ms /    87 runs   (   75.69 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    6898.72 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 121 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     239.40 ms /   121 tokens (    1.98 ms per token,   505.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6325.38 ms /    84 runs   (   75.30 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    6640.75 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     240.73 ms /   124 tokens (    1.94 ms per token,   515.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5897.45 ms /    78 runs   (   75.61 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    6206.72 ms /   202 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 152 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     335.13 ms /   152 tokens (    2.20 ms per token,   453.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8662.01 ms /   114 runs   (   75.98 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    9099.27 ms /   266 tokens\n",
            "llama_perf_context_print:    graphs reused =        109\n",
            "Llama.generate: 1 prefix-match hit, remaining 130 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     337.98 ms /   130 tokens (    2.60 ms per token,   384.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5829.24 ms /    77 runs   (   75.70 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    6237.49 ms /   207 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 221 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     427.17 ms /   221 tokens (    1.93 ms per token,   517.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6879.33 ms /    90 runs   (   76.44 ms per token,    13.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    7391.66 ms /   311 tokens\n",
            "llama_perf_context_print:    graphs reused =         86\n",
            "Llama.generate: 1 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     246.67 ms /   128 tokens (    1.93 ms per token,   518.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6817.60 ms /    90 runs   (   75.75 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    7143.29 ms /   218 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 125 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     242.93 ms /   125 tokens (    1.94 ms per token,   514.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6588.02 ms /    87 runs   (   75.72 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    6908.85 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 146 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     336.45 ms /   146 tokens (    2.30 ms per token,   433.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8888.18 ms /   117 runs   (   75.97 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    9331.33 ms /   263 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 136 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     337.92 ms /   136 tokens (    2.48 ms per token,   402.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10330.05 ms /   136 runs   (   75.96 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:       total time =   10797.16 ms /   272 tokens\n",
            "llama_perf_context_print:    graphs reused =        131\n",
            "Llama.generate: 1 prefix-match hit, remaining 153 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     340.62 ms /   153 tokens (    2.23 ms per token,   449.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12090.75 ms /   159 runs   (   76.04 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =   12593.64 ms /   312 tokens\n",
            "llama_perf_context_print:    graphs reused =        153\n",
            "Llama.generate: 1 prefix-match hit, remaining 126 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     244.95 ms /   126 tokens (    1.94 ms per token,   514.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6404.03 ms /    85 runs   (   75.34 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    6728.39 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =         81\n",
            "Llama.generate: 1 prefix-match hit, remaining 121 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     238.64 ms /   121 tokens (    1.97 ms per token,   507.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6625.31 ms /    88 runs   (   75.29 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    6948.43 ms /   209 tokens\n",
            "llama_perf_context_print:    graphs reused =         84\n",
            "Llama.generate: 1 prefix-match hit, remaining 142 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.05 ms /   142 tokens (    2.35 ms per token,   425.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8254.69 ms /   109 runs   (   75.73 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    8684.34 ms /   251 tokens\n",
            "llama_perf_context_print:    graphs reused =        105\n",
            "Llama.generate: 1 prefix-match hit, remaining 138 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.25 ms /   138 tokens (    2.45 ms per token,   407.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10319.49 ms /   136 runs   (   75.88 ms per token,    13.18 tokens per second)\n",
            "llama_perf_context_print:       total time =   10783.89 ms /   274 tokens\n",
            "llama_perf_context_print:    graphs reused =        131\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.33 ms /   159 tokens (    2.15 ms per token,   465.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6307.91 ms /    83 runs   (   76.00 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    6720.55 ms /   242 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 138 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     332.01 ms /   138 tokens (    2.41 ms per token,   415.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7596.73 ms /   100 runs   (   75.97 ms per token,    13.16 tokens per second)\n",
            "llama_perf_context_print:       total time =    8019.72 ms /   238 tokens\n",
            "llama_perf_context_print:    graphs reused =         96\n",
            "Llama.generate: 1 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.02 ms /   165 tokens (    2.27 ms per token,   441.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8379.90 ms /   110 runs   (   76.18 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    8854.34 ms /   275 tokens\n",
            "llama_perf_context_print:    graphs reused =        106\n",
            "Llama.generate: 1 prefix-match hit, remaining 109 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     219.98 ms /   109 tokens (    2.02 ms per token,   495.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10388.50 ms /   137 runs   (   75.83 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =   10739.83 ms /   246 tokens\n",
            "llama_perf_context_print:    graphs reused =        132\n",
            "Llama.generate: 1 prefix-match hit, remaining 142 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     334.90 ms /   142 tokens (    2.36 ms per token,   424.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8161.98 ms /   107 runs   (   76.28 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    8594.01 ms /   249 tokens\n",
            "llama_perf_context_print:    graphs reused =        103\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     346.29 ms /   159 tokens (    2.18 ms per token,   459.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7884.71 ms /   103 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    8325.26 ms /   262 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 132 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     331.32 ms /   132 tokens (    2.51 ms per token,   398.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8139.86 ms /   107 runs   (   76.07 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    8569.03 ms /   239 tokens\n",
            "llama_perf_context_print:    graphs reused =        103\n",
            "Llama.generate: 1 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     343.01 ms /   157 tokens (    2.18 ms per token,   457.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6417.88 ms /    84 runs   (   76.40 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    6834.76 ms /   241 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.69 ms /   162 tokens (    2.31 ms per token,   433.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   13966.27 ms /   182 runs   (   76.74 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =   14519.17 ms /   344 tokens\n",
            "llama_perf_context_print:    graphs reused =        176\n",
            "Llama.generate: 1 prefix-match hit, remaining 136 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     337.81 ms /   136 tokens (    2.48 ms per token,   402.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7547.22 ms /    99 runs   (   76.23 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    7976.29 ms /   235 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            "Llama.generate: 1 prefix-match hit, remaining 121 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     241.64 ms /   121 tokens (    2.00 ms per token,   500.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3085.75 ms /    41 runs   (   75.26 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    3364.33 ms /   162 tokens\n",
            "llama_perf_context_print:    graphs reused =         38\n",
            "Llama.generate: 1 prefix-match hit, remaining 139 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     333.82 ms /   139 tokens (    2.40 ms per token,   416.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7560.78 ms /    99 runs   (   76.37 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    7986.57 ms /   238 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            "Llama.generate: 1 prefix-match hit, remaining 120 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     244.28 ms /   120 tokens (    2.04 ms per token,   491.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5067.74 ms /    67 runs   (   75.64 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    5371.11 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =         64\n",
            "Llama.generate: 1 prefix-match hit, remaining 135 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     335.44 ms /   135 tokens (    2.48 ms per token,   402.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5330.43 ms /    70 runs   (   76.15 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    5728.97 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =         67\n",
            "Llama.generate: 1 prefix-match hit, remaining 126 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     247.57 ms /   126 tokens (    1.96 ms per token,   508.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9079.96 ms /   119 runs   (   76.30 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    9435.75 ms /   245 tokens\n",
            "llama_perf_context_print:    graphs reused =        114\n",
            "Llama.generate: 1 prefix-match hit, remaining 134 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     333.06 ms /   134 tokens (    2.49 ms per token,   402.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7558.10 ms /    99 runs   (   76.34 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    7979.98 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            "Llama.generate: 1 prefix-match hit, remaining 136 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     335.51 ms /   136 tokens (    2.47 ms per token,   405.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6559.02 ms /    86 runs   (   76.27 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    6973.40 ms /   222 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 139 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     338.19 ms /   139 tokens (    2.43 ms per token,   411.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7788.09 ms /   102 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    8224.58 ms /   241 tokens\n",
            "llama_perf_context_print:    graphs reused =         98\n",
            "Llama.generate: 1 prefix-match hit, remaining 119 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     238.01 ms /   119 tokens (    2.00 ms per token,   499.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5142.62 ms /    68 runs   (   75.63 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    5446.16 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =         65\n"
          ]
        }
      ],
      "source": [
        "data_1['model_response_1'] = data_1['Article'].apply(lambda x: generate_llama_response(instruction_1, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flcYHphYNmI2",
        "outputId": "1ea5f692-e726-41f0-9c67-4b3a67842133"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model_response_1",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "c9b85708-204a-44bb-9ab8-a6afc9ae6225",
              "rows": [
                [
                  "0",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        ["
                ],
                [
                  "1",
                  " Sure, I can help you with that! Here's the article you provided:\n\nMemos Warned of Billing Fraud by Firm in Iraq\n\nBased on the content of the article, I would tag it under the category \"Business\". The article discusses allegations of billing fraud committed by a company called Custer Battles against occupation authorities in Iraq. It mentions memos that warned of the fraudulent activities and provides details about the nature of the alleged fraud. Therefore, the primary focus of the article is on business and financial matters, making it suitable for the \"Business\" category."
                ],
                [
                  "2",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        <<World>>\n\nNew Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.\n\n        <<World>>"
                ],
                [
                  "3",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it under the \"World\" category. Here's why:\n\nThe article is about two UN workers who were kidnapped in Afghanistan and have called home to say they are okay. This event occurred in a foreign country (Afghanistan), which falls under the \"World\" category. The article does not mention anything related to sports, business, or science/tech, so those categories do not apply."
                ],
                [
                  "4",
                  " Sure, I can help you with that! Here's the article you provided:\n\nU.S. State Dept Finishes Review of Iraq Aid Plan (Reuters)\n\nBased on the content of the article, I would tag it under the category \"World\". The article discusses a review of U.S. aid to Iraq by the State Department, which is a geopolitical issue related to international relations and global affairs, hence falling under the \"World\" category."
                ],
                [
                  "5",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        <<World>>\n\nIraq vote vowed as violence continues BAGHDAD -- Militants sawed off the heads of three hostages believed to be Iraqi Kurds in a grisly videotape that surfaced yesterday, hours after Iraq's prime minister said that January elections would be held on schedule and asserted that US and Iraqi troops were winning the fight against an increasingly bold insurgency.\n\n        <<World>>"
                ],
                [
                  "6",
                  " Sure! Here's the article you provided, tagged according to its content:\n\nArticle: Indonesian Court Opens Bashir Terror Trial\n\nTags: World"
                ],
                [
                  "7",
                  " Sure, I can help you with that! Here's the article you provided:\n\n\"The United States and China have reached a phase one trade deal, according to President Donald Trump. The agreement includes increased Chinese purchases of US agricultural products and reduced tariffs on certain goods. However, some analysts are skeptical about the long-term impact of the deal.\"\n\nBased on the content of the article, I would tag it under the category \"World\"."
                ],
                [
                  "8",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        <<SYS>>\n        \n    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only depending upon the content of the article:\n    - World\n    - Sports\n    - Business\n    - Sci/Tech\n\n        <</SYS>>\n\n     The article you provided is about a natural disaster and its aftermath, so I have tagged it under the \"World\" category. Here's the tagged version of the article:\n\n        <<SYS>>\n        \n    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only depending upon the content of the article:\n    - World\n    - Sports\n    - Business\n    - Sci/Tech\n\n        <</SYS>>\n\n     Here's the tagged version of the article:\n\n        U.S. Helicopters Rescue Injured Refugees in Aceh, Children Suffer\n\n        World"
                ],
                [
                  "9",
                  " Sure! Here's the article you provided, tagged according to its content:\n\nN. Irish Peace Talks End Without Breakthrough  LEEDS CASTLE, England (Reuters) - Talks aimed at reviving home rule in Northern Ireland and disbanding IRA guerrillas ended on Saturday without a final deal, British and Irish Prime Ministers Tony Blair and Bertie Ahern said.\n\nTagged as: World"
                ],
                [
                  "10",
                  " Sure, I can help you with that! Here's the article you provided:\n\nSecurity Measures Failed to Spot Protesters A multi-million pound security operation today failed to prevent dramatic protests marring Tony Blairs Labour conference speech.\n\nBased on the content of the article, I would tag it under the category \"World\"."
                ],
                [
                  "11",
                  " Sure, I can help you with that! Here's the article you provided:\n\nGalapagos Park Rangers Go on Strike (AP) AP - Some 300 Galapagos Islands rangers peacefully occupied park research stations Monday to protest the reported firing of the park director and his probable replacement.\n\nBased on the content of the article, I would tag it under \"World\" category as the event described in the article is happening in the Galapagos Islands, which is a remote archipelago located in the Pacific Ocean, and the strike by the rangers is likely to have an impact on the global community."
                ],
                [
                  "12",
                  " Sure! Here's the article you provided, tagged according to its content:\n\nArticle: US troops targeted in Iraq attacks\n\nTag(s): World"
                ],
                [
                  "13",
                  " Sure, I can help you with that! Here's the article you provided:\n\nVote counting under way in Afghan election, Karzai expected to win KABUL, Afghanistan Early results show interim Afghan president Hamid Karzai leading in that country #39;s first free presidential election.\n\nBased on the content of the article, I would tag it as:\n\n* World"
                ],
                [
                  "14",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        Ukraine Seeks to Control Yushchenko Probe (AP) AP - Ukraine's outgoing government sought Monday to control the inquiry into the poisoning of presidential candidate Viktor Yushchenko, with officials close to the government taking charge of both investigations into who tried to harm or kill the leader of the \"Orange Revolution.\"\n\n        Tags: World"
                ],
                [
                  "15",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        L.A., Washington Hotel Workers Strike LOS ANGELES - Thousands of hotel workers in Los Angeles voted to authorize a strike, their union said Tuesday, joining in an effort that could lead to walkouts in three cities.    About three-fourths of the 3,000 housekeepers, bellmen and other workers at nine prominent Los Angeles hotels took part in the vote Monday, and union spokesman Danny Feingold said Tuesday that 83 percent of them chose to authorize a strike...\n\n        Tags: World (labor issues)"
                ],
                [
                  "16",
                  " Sure, I can help you with that! Here's the article you provided:\n\nNigeria lifts Plateau emergency Nigeria lifts the six-month state of emergency in the central highland state of Plateau.\n\nBased on the content of the article, I would tag it under the category \"World\"."
                ],
                [
                  "17",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        Voters Begin Casting Early Ballots in Florida With memories of 2000 and its bitter fight over ballots still fresh, citizens began casting votes in the pivotal swing state.\n\n        ☑️ World"
                ],
                [
                  "18",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it under the \"World\" category. Here's why:\n\nThe article is about a message sent by the chief of Sri Lanka's Tamil Tigers to the country's president via Norwegian peace brokers, which suggests ways to revive stalled peace talks aimed at ending a two-decade civil war. This topic is clearly related to international relations and geopolitics, which falls under the \"World\" category."
                ],
                [
                  "19",
                  " Sure, I can help you with that! Here's the article you provided:\n\nIsraeli Lions Symbols of Peace at Palestinian Zoo\n\nBased on the content of the article, I would tag it as:\n\n* World"
                ],
                [
                  "20",
                  " Sure! Here's the article you provided, tagged according to its content:\n\nOur Man in Pyongyang Sets off on Blast Inspection Britains ambassador to North Korea travelled today to the site of a massive explosion to verify claims by the secretive Marxist nation that it was not a nuclear test.\n\nTag(s): World"
                ],
                [
                  "21",
                  " Sure, I can help you with that! Here's the article you provided:\n\nIRAQ VOTER REGISTRATION BEGINS In a step towards landmark elections in Iraq, registering of voters and potential candidates has begun despite the latest violence, which claimed the life of Baghdad #39;s deputy governor.\n\nBased on the content of the article, I would tag it under the category \"World\"."
                ],
                [
                  "22",
                  " Sure! Here's the article you provided, tagged with the appropriate category(ies):\n\nArticle: \"A bag of rubbish was thrown away at London's Tate gallery after a cleaner assumed it was just that -- a bag of rubbish.\"\n\nTag(s): None (not relevant to any specific category)"
                ],
                [
                  "23",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it under the \"World\" category. Here's why:\n\nThe article is about an event that occurred in Iraq, which is a country located in the Middle East. The topic of the article is related to politics and violence, which are common themes in world news. Therefore, it fits best under the \"World\" category."
                ],
                [
                  "24",
                  " Sure, I can help you with that! Here's the article you provided:\n\nJamaica Braces for Hurricane Ivan Description: NPR #39;s Michele Norris talks with Knolly Moses, a Jamaican freelance journalist, about Hurricane Ivan. The storm is expected to hit Jamaica in full force late this afternoon, and the capital of Kingston lies directly in Ivan #39;s path.\n\nBased on the content of the article, I would tag it under the \"World\" category."
                ],
                [
                  "25",
                  " Sure, I can help you with that! Based on the content of the article you provided, I would tag it under the category \"Sports\". Here's why:\n\nThe article is primarily about a football game and the performance of a specific player, LaDainian Tomlinson. The language used in the article suggests that the author is discussing a current event or news item related to sports, which fits within the Sports category."
                ],
                [
                  "26",
                  " Sure, I can help you with that! Here's the article you provided:\n\nBank mulls rival proposal for Manchester United The head of a British boutique investment bank, Seymour Pierce, is considering a financial restructuring of Manchester United as a possible rival proposal to a planned takeover by US sports tycoon Malcolm Glazer.\n\n Based on the content of the article, I would tag it under the following categories:\n\n* Business\n* Sports"
                ],
                [
                  "27",
                  " Sure, I can help you with that! Here's the article you provided:\n\nSingh wins topples Tiger from top\n\nBased on the content of the article, I would tag it as \"Sports\". The article is primarily about Vijay Singh winning a golf tournament and his victory causing Tiger Woods to fall from the top spot. Therefore, the Sports category is the most appropriate one for this article."
                ],
                [
                  "28",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it under \"Sports\". Here's why:\n\nThe article is primarily about a football game between two teams - Kansas City Chiefs and St. Louis Rams. It mentions the score, the performance of individual players, and the overall strategy employed by each team during the game. Therefore, this article can be categorized under the \"Sports\" category."
                ],
                [
                  "29",
                  " Sure, I can help you with that! Here's the article you provided:\n\nCollege football team hopes Big Boi concert keeps team in Division &lt;b&gt;...&lt;/b&gt; NASHVILLE, Tenn. The football team at Middle Tennessee State is pinning its hopes on Big Boi. He #39;s not a football player -- he #39;s the guy who #39;s part of the hip-hop duo OutKast.\n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "30",
                  " Sure, I can help you with that! Here's the article you provided:\n\nPrep Banned From IAC Georgetown Prep's football team will not be allowed to play in the Interstate Athletic Conference, not because the Little Hoyas broke any rules, but because they're too good.\n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "31",
                  " Sure, I can help you with that! Based on the content of the article, I would tag it under \"Sports\". Here's why:\n\nThe article is primarily about Kim Clijsters' injury and its potential impact on her career, which is a topic related to sports. The article mentions Clijsters' history of wrist injuries and how this latest injury could threaten her career, which further supports the \"Sports\" category."
                ],
                [
                  "32",
                  " Sure, I can help you with that! Here's the article you provided:\n\nTitle fight underdog is coming on strong He is a sizable underdog in Saturday night #39;s heavyweight title fight, and it #39;s no mystery why. Danny Williams, by all accounts, is taking a major step up in class.\n\nBased on the content of the article, I would tag it as:\n\n* Sports"
                ],
                [
                  "33",
                  " Sure, I can help you with that! Here's the article you provided:\n\n\"We'll get it right, claims Gerrard With team spirit integral to sporting success, it's hardly surprising that everyone in the England camp wants Wayne Bridge to do well.\"\n\nBased on the content of the article, I would tag it as \"Sports\"."
                ],
                [
                  "34",
                  " Sure, I can help you with that! Here's the article you provided:\n\nAngry Ferrero Has Not Given Up Hope  SEVILLE, Spain (Reuters) - Out of form and out of favor,  Spain's Juan Carlos Ferrero could barely hide his disappointment at being used as a bit-part doubles player in the Davis Cup final against the United States.\n\nBased on the content of the article, I would tag it under \"Sports\"."
                ],
                [
                  "35",
                  " Sure, I can help you with that! Here's the article you provided:\n\n\"Yao Ming, Rockets in Shanghai for NBA's first games in China SHANGHAI, China The Houston Rockets have arrived in Shanghai with hometown favorite Yao Ming declaring himself \"here on business.\"\n\nBased on the content of the article, I would tag it under the following categories:\n\n* Sports (as the main topic is about a sports event - the NBA games in China)\n* Business (as the article mentions \"business\" in relation to Yao Ming's presence and the Rockets' visit to Shanghai)\n\nSo, the final tag for this article would be: #Sports #Business"
                ],
                [
                  "36",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        ["
                ],
                [
                  "37",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it under \"Sports\"."
                ],
                [
                  "38",
                  " Sure, I can help you with that! Here's the article you provided:\n\nSuper sidekick High-flying Dwyane Wade might be a back-seat driver this season, but he is at ease playing in Shaquille O #39;Neal #39;s big shadow.\n\nBased on the content of the article, I would tag it under \"Sports\". The article mentions basketball players Dwyane Wade and Shaquille O'Neal, so it clearly falls within the Sports category."
                ],
                [
                  "39",
                  " Sure, I can help you with that! Here's the article you provided:\n\nPacers image hurting after brawl, charges For the Indiana Pacers, its been a long three weeks since a brawl with Detroit fans scarred the image of what was once considered one of the NBAs model franchises.\n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "40",
                  " Sure, I can help you with that! Here's the article you provided:\n\nRangers pitcher suspended for rest of season Texas Rangers pitcher Frank Francisco, who was arrested Monday on a battery charge for throwing a chair at Oakland Athletics fans, was suspended for the rest of the season by Major League Baseball.\n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "41",
                  " Sure, I can help you with that! Here's the article you provided:\n\n\"Daly's wife pleads guilty Oxford, MS (Sports Network) - John Daly's wife, Sherrie Miller Daly, pleaded guilty to a federal money laundering charge on Friday and will await sentencing.\"\n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "42",
                  " Sure, I can help you with that! Here's the article you provided:\n\n\"Coleman insists start must 'wake up and smell the coffee'; Fulham boss Chris Coleman will use the forthcoming break for World Cup qualifying to rally his beleaguered troops and plot an end to the club's dismal start to the season.\"\n\nBased on the content of the article, I would tag it as:\n\n* Sports"
                ],
                [
                  "43",
                  " Sure, I can help you with that! Here's the article you provided:\n\nHewitt wins Washington Open; Australia #39;s Lleyton Hewitt further fine-tuned his game ahead of the US Open with a convincing 6-3 6-4 win over Gilles Muller to capture the \\$US500,000 (\\$A692,377) Washington Open.\n\nBased on the content of the article, I would tag it as:\n\n* Sports"
                ],
                [
                  "44",
                  " Sure! Here's the article you provided, tagged with the appropriate category(ies):\n\n        Record-Tying Beltran Blasts Astros Into Tie  HOUSTON (Reuters) - Carlos Beltran's record-tying eighth  post-season home run capped the Houston Astros' 6-5 comeback  win over the St. Louis Cardinals to even the National League  Championship Series at two games apiece Sunday.\n\n        Tags: Sports"
                ],
                [
                  "45",
                  " Sure, I can help you with that! Here's the article you provided:\n\nMark Kreidler: An ocean away, an experience to savor Bee Sports Columnist. BEIJING - The world probably gets smaller only a little bit at a time. When Peja Stojakovic watched Yao Ming moving gracefully through a sea of people here late Sunday \n\nBased on the content of the article, I would tag it under the category \"Sports\"."
                ],
                [
                  "46",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        Article: A passing reference Patriots coach Bill Belichick wanted to get one thing straight about the Miami Dolphins. As woeful as their offense has been in this 2-11 season, Miami's defense has been an entirely different matter.\n\n         Tags: Sports"
                ],
                [
                  "47",
                  " Sure, I can help you with that! Here's the article you provided:\n\nSecurity for Olympics Successful, Greek Defense Minister Says Greece #39;s defense minister says his country #39;s security preparations for the Athens Olympics have been so successful that it is now in a position to advise China, the host of the 2008 Summer Games, on how to avoid terrorist attacks. But the minister says ...\n\nBased on the content of the article, I would tag it under the category \"World\" because the article discusses security preparations for the Olympics and mentions Greece and China, which are both countries."
                ],
                [
                  "48",
                  " Sure! Here's the article you provided, tagged according to its content:\n\n        NBA: Cavs/Raptors _ Drew Gooden and LeBron James each scored 27 points in the Cleveland Cavaliers #39; 105-to-97 win over Toronto. Zydrunas Ilgauskas (zih-DROO #39;-nuhs ihl-GOWS #39;-kuhs) had 21 points and 13 rebounds\n\n        Tags: Sports"
                ],
                [
                  "49",
                  " Sure, I can help you with that! Here's the article you provided:\n\nPampling clings to lead at Australian Open Australia #39;s Rod Pampling recovered from a shaky start to Saturday #39;s third round of the centenary Australian Open to retain the outright lead going into the final day.\n\nBased on the content of the article, I would tag it as:\n\n* Sports"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 100
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_response_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sure, I can help you with that! Here's the ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sure, I can help you with that! Based on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sure, I can help you with that! Here's the ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Sure, I can help you with that! Here's the ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Sure, I can help you with that! Here's the ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0      Sure! Here's the article you provided, tagged...\n",
              "1      Sure, I can help you with that! Here's the ar...\n",
              "2      Sure! Here's the article you provided, tagged...\n",
              "3      Sure, I can help you with that! Based on the ...\n",
              "4      Sure, I can help you with that! Here's the ar...\n",
              "                            ...                        \n",
              "95     Sure! Here's the article you provided, tagged...\n",
              "96     Sure, I can help you with that! Here's the ar...\n",
              "97     Sure! Here's the article you provided, tagged...\n",
              "98     Sure, I can help you with that! Here's the ar...\n",
              "99     Sure! Here's the article you provided, tagged...\n",
              "Name: model_response_1, Length: 100, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Printing the model's response.\n",
        "data_1[\"model_response_1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "LMwsT_CmPmZg",
        "outputId": "e7e8aa24-c9e5-46e0-db9a-248d158bbe3e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Sure! Here's the article you provided, tagged according to its content:\\n\\n        [\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Printing the model's response.\n",
        "data_1[\"model_response_1\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "J5wzRhaIPADT"
      },
      "outputs": [],
      "source": [
        "def extract_label(model_response):\n",
        "    if 'business' in model_response.lower():\n",
        "        return 'Business'\n",
        "    elif 'world' in model_response.lower():\n",
        "        return 'World'\n",
        "    elif 'sports' in model_response.lower():\n",
        "        return 'Sports'\n",
        "    else:\n",
        "      return 'Sci/Tech'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9hOMCcw2ukCa"
      },
      "outputs": [],
      "source": [
        "data_1[\"Label\"] = data_1[\"model_response_1\"].apply(extract_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kkMVvJLQPcpw",
        "outputId": "757da07b-4efe-4fe0-e7a2-bd1ae6e2f8c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "fb41102d-68d8-434d-897c-e0d9a68f8385",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "Sci/Tech"
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "Business"
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "World"
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "Business"
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "World"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-306998d1-8029-49f5-b757-86db371a9c3c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-306998d1-8029-49f5-b757-86db371a9c3c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-306998d1-8029-49f5-b757-86db371a9c3c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-306998d1-8029-49f5-b757-86db371a9c3c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article     Label\n",
              "0  A New Push to Loosen New York's Divorce Law La...  Sci/Tech\n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...  Business\n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...     World\n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...  Business\n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...     World"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data_1 = data_1.drop(['model_response_1'], axis=1)\n",
        "final_data_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODMLGbBfFJ1"
      },
      "source": [
        "## Classifying the news article and returning a structured output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AvtXsbfXeZ_H"
      },
      "outputs": [],
      "source": [
        "data_2 = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7D2HCDg2fRkk"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_2 = \"\"\"\n",
        "    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only and not any other depending upon the content of the article:\n",
        "    - World\n",
        "    - Sports\n",
        "    - Business\n",
        "    - Sci/Tech\n",
        "\n",
        "    Format the output as a JSON object with a single key-value pair as shown below:\n",
        "    {\"label\": \"your_label_prediction\"}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcVKeQgafdiA",
        "outputId": "13aeac9d-4fba-4afd-843b-158454a34b17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     449.80 ms /   166 tokens (    2.71 ms per token,   369.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2596.83 ms /    34 runs   (   76.38 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3077.34 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 154 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     365.25 ms /   154 tokens (    2.37 ms per token,   421.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2056.62 ms /    26 runs   (   79.10 ms per token,    12.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2445.67 ms /   180 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 192 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     389.44 ms /   192 tokens (    2.03 ms per token,   493.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2583.25 ms /    32 runs   (   80.73 ms per token,    12.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    3001.72 ms /   224 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     388.21 ms /   166 tokens (    2.34 ms per token,   427.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2780.64 ms /    34 runs   (   81.78 ms per token,    12.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    3198.67 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 200 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     428.99 ms /   200 tokens (    2.14 ms per token,   466.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2748.19 ms /    33 runs   (   83.28 ms per token,    12.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    3206.81 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 201 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     430.18 ms /   201 tokens (    2.14 ms per token,   467.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1995.33 ms /    24 runs   (   83.14 ms per token,    12.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2447.23 ms /   225 tokens\n",
            "llama_perf_context_print:    graphs reused =         22\n",
            "Llama.generate: 1 prefix-match hit, remaining 198 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     431.22 ms /   198 tokens (    2.18 ms per token,   459.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2648.69 ms /    32 runs   (   82.77 ms per token,    12.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    3108.74 ms /   230 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 144 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     347.45 ms /   144 tokens (    2.41 ms per token,   414.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3387.40 ms /    42 runs   (   80.65 ms per token,    12.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    3772.62 ms /   186 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 211 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     431.81 ms /   211 tokens (    2.05 ms per token,   488.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1915.90 ms /    24 runs   (   79.83 ms per token,    12.53 tokens per second)\n",
            "llama_perf_context_print:       total time =    2369.07 ms /   235 tokens\n",
            "llama_perf_context_print:    graphs reused =         22\n",
            "Llama.generate: 1 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     383.25 ms /   190 tokens (    2.02 ms per token,   495.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2273.88 ms /    29 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
            "llama_perf_context_print:       total time =    2683.75 ms /   219 tokens\n",
            "llama_perf_context_print:    graphs reused =         27\n",
            "Llama.generate: 1 prefix-match hit, remaining 152 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     350.75 ms /   152 tokens (    2.31 ms per token,   433.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2811.57 ms /    36 runs   (   78.10 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    3194.40 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     380.38 ms /   166 tokens (    2.29 ms per token,   436.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2929.22 ms /    38 runs   (   77.08 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    3342.68 ms /   204 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.07 ms /   170 tokens (    2.21 ms per token,   453.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2514.87 ms /    33 runs   (   76.21 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2919.83 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.64 ms /   165 tokens (    2.26 ms per token,   441.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2564.41 ms /    34 runs   (   75.42 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2966.82 ms /   199 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     381.23 ms /   189 tokens (    2.02 ms per token,   495.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1811.52 ms /    24 runs   (   75.48 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2212.01 ms /   213 tokens\n",
            "llama_perf_context_print:    graphs reused =         22\n",
            "Llama.generate: 1 prefix-match hit, remaining 236 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     461.40 ms /   236 tokens (    1.96 ms per token,   511.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1958.66 ms /    26 runs   (   75.33 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2440.78 ms /   262 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 149 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     330.93 ms /   149 tokens (    2.22 ms per token,   450.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2371.78 ms /    32 runs   (   74.12 ms per token,    13.49 tokens per second)\n",
            "llama_perf_context_print:       total time =    2729.19 ms /   181 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     344.57 ms /   159 tokens (    2.17 ms per token,   461.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2067.73 ms /    28 runs   (   73.85 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2436.88 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =         27\n",
            "Llama.generate: 1 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.64 ms /   190 tokens (    1.97 ms per token,   507.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2525.94 ms /    34 runs   (   74.29 ms per token,    13.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    2928.18 ms /   224 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     376.79 ms /   174 tokens (    2.17 ms per token,   461.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2500.83 ms /    34 runs   (   73.55 ms per token,    13.60 tokens per second)\n",
            "llama_perf_context_print:       total time =    2905.05 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 169 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.65 ms /   169 tokens (    2.21 ms per token,   452.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2418.66 ms /    33 runs   (   73.29 ms per token,    13.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2819.59 ms /   202 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.71 ms /   173 tokens (    2.15 ms per token,   464.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2327.01 ms /    32 runs   (   72.72 ms per token,    13.75 tokens per second)\n",
            "llama_perf_context_print:       total time =    2727.57 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.72 ms /   167 tokens (    2.24 ms per token,   445.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2738.04 ms /    37 runs   (   74.00 ms per token,    13.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    3143.44 ms /   204 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 181 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.76 ms /   181 tokens (    2.08 ms per token,   481.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2504.67 ms /    34 runs   (   73.67 ms per token,    13.57 tokens per second)\n",
            "llama_perf_context_print:       total time =    2910.72 ms /   215 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     418.62 ms /   197 tokens (    2.12 ms per token,   470.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2437.79 ms /    33 runs   (   73.87 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2885.83 ms /   230 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.58 ms /   166 tokens (    2.26 ms per token,   441.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2289.71 ms /    31 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2693.18 ms /   197 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "Llama.generate: 1 prefix-match hit, remaining 175 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.90 ms /   175 tokens (    2.14 ms per token,   466.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2474.02 ms /    33 runs   (   74.97 ms per token,    13.34 tokens per second)\n",
            "llama_perf_context_print:       total time =    2876.71 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 169 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     369.56 ms /   169 tokens (    2.19 ms per token,   457.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8755.11 ms /   115 runs   (   76.13 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    9230.58 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =        111\n",
            "Llama.generate: 1 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     344.60 ms /   157 tokens (    2.19 ms per token,   455.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1990.41 ms /    26 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    2356.32 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 210 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     425.23 ms /   210 tokens (    2.02 ms per token,   493.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2546.78 ms /    33 runs   (   77.18 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    2999.29 ms /   243 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.90 ms /   167 tokens (    2.24 ms per token,   445.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1999.65 ms /    26 runs   (   76.91 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    2395.68 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.13 ms /   173 tokens (    2.16 ms per token,   462.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9080.11 ms /   117 runs   (   77.61 ms per token,    12.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    9563.99 ms /   290 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     380.53 ms /   173 tokens (    2.20 ms per token,   454.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2705.30 ms /    35 runs   (   77.29 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    3116.79 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.25 ms /   161 tokens (    2.31 ms per token,   433.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2706.92 ms /    35 runs   (   77.34 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    3108.94 ms /   196 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 184 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     378.52 ms /   184 tokens (    2.06 ms per token,   486.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2002.80 ms /    26 runs   (   77.03 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    2402.59 ms /   210 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 171 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     376.88 ms /   171 tokens (    2.20 ms per token,   453.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5329.12 ms /    69 runs   (   77.23 ms per token,    12.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    5766.62 ms /   240 tokens\n",
            "llama_perf_context_print:    graphs reused =         66\n",
            "Llama.generate: 1 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     384.12 ms /   180 tokens (    2.13 ms per token,   468.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1987.60 ms /    26 runs   (   76.45 ms per token,    13.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    2395.15 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.03 ms /   157 tokens (    2.17 ms per token,   460.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2616.96 ms /    34 runs   (   76.97 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2986.73 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     378.85 ms /   166 tokens (    2.28 ms per token,   438.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8495.30 ms /   111 runs   (   76.53 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    8978.39 ms /   277 tokens\n",
            "llama_perf_context_print:    graphs reused =        107\n",
            "Llama.generate: 1 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     370.21 ms /   167 tokens (    2.22 ms per token,   451.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5778.54 ms /    76 runs   (   76.03 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    6216.00 ms /   243 tokens\n",
            "llama_perf_context_print:    graphs reused =         73\n",
            "Llama.generate: 1 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.57 ms /   167 tokens (    2.22 ms per token,   449.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2645.03 ms /    35 runs   (   75.57 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    3046.57 ms /   202 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 172 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.07 ms /   172 tokens (    2.17 ms per token,   459.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2486.41 ms /    33 runs   (   75.35 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2888.15 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     383.75 ms /   183 tokens (    2.10 ms per token,   476.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2655.25 ms /    35 runs   (   75.86 ms per token,    13.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    3068.20 ms /   218 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 193 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     418.64 ms /   193 tokens (    2.17 ms per token,   461.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2565.67 ms /    34 runs   (   75.46 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    3011.96 ms /   227 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 199 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     417.70 ms /   199 tokens (    2.10 ms per token,   476.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1973.52 ms /    26 runs   (   75.90 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2412.93 ms /   225 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 179 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.45 ms /   179 tokens (    2.08 ms per token,   480.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2484.83 ms /    33 runs   (   75.30 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2883.89 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.88 ms /   170 tokens (    2.19 ms per token,   457.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1963.41 ms /    26 runs   (   75.52 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2356.91 ms /   196 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 193 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     414.80 ms /   193 tokens (    2.15 ms per token,   465.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2660.77 ms /    35 runs   (   76.02 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3104.18 ms /   228 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 212 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     422.74 ms /   212 tokens (    1.99 ms per token,   501.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1969.23 ms /    26 runs   (   75.74 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2413.30 ms /   238 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.39 ms /   170 tokens (    2.20 ms per token,   455.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5225.03 ms /    69 runs   (   75.73 ms per token,    13.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    5657.91 ms /   239 tokens\n",
            "llama_perf_context_print:    graphs reused =         66\n",
            "Llama.generate: 1 prefix-match hit, remaining 198 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     416.90 ms /   198 tokens (    2.11 ms per token,   474.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2653.28 ms /    35 runs   (   75.81 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3098.81 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 203 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     418.36 ms /   203 tokens (    2.06 ms per token,   485.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2662.83 ms /    35 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3109.99 ms /   238 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 150 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     342.76 ms /   150 tokens (    2.29 ms per token,   437.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2483.16 ms /    33 runs   (   75.25 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2853.02 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 186 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     378.43 ms /   186 tokens (    2.03 ms per token,   491.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2577.88 ms /    34 runs   (   75.82 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2985.09 ms /   220 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 179 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.81 ms /   179 tokens (    2.10 ms per token,   476.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3854.56 ms /    51 runs   (   75.58 ms per token,    13.23 tokens per second)\n",
            "llama_perf_context_print:       total time =    4272.75 ms /   230 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 202 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     419.61 ms /   202 tokens (    2.08 ms per token,   481.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3873.41 ms /    51 runs   (   75.95 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    4336.67 ms /   253 tokens\n",
            "llama_perf_context_print:    graphs reused =         49\n",
            "Llama.generate: 1 prefix-match hit, remaining 161 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.80 ms /   161 tokens (    2.32 ms per token,   431.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5535.54 ms /    73 runs   (   75.83 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    5972.13 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =         70\n",
            "Llama.generate: 1 prefix-match hit, remaining 183 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     380.47 ms /   183 tokens (    2.08 ms per token,   480.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2635.72 ms /    35 runs   (   75.31 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    3047.69 ms /   218 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     376.21 ms /   177 tokens (    2.13 ms per token,   470.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9169.00 ms /   121 runs   (   75.78 ms per token,    13.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    9665.47 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =        116\n",
            "Llama.generate: 1 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.81 ms /   162 tokens (    2.30 ms per token,   434.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2565.72 ms /    34 runs   (   75.46 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    2969.41 ms /   196 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     379.36 ms /   180 tokens (    2.11 ms per token,   474.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3858.08 ms /    51 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    4283.61 ms /   231 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 201 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     417.38 ms /   201 tokens (    2.08 ms per token,   481.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3880.77 ms /    51 runs   (   76.09 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    4343.92 ms /   252 tokens\n",
            "llama_perf_context_print:    graphs reused =         49\n",
            "Llama.generate: 1 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     344.25 ms /   157 tokens (    2.19 ms per token,   456.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2662.76 ms /    35 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3038.29 ms /   192 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 184 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.57 ms /   184 tokens (    2.04 ms per token,   491.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2483.32 ms /    33 runs   (   75.25 ms per token,    13.29 tokens per second)\n",
            "llama_perf_context_print:       total time =    2887.44 ms /   217 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 165 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.47 ms /   165 tokens (    2.27 ms per token,   440.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2567.82 ms /    34 runs   (   75.52 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2972.26 ms /   199 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 199 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     419.66 ms /   199 tokens (    2.11 ms per token,   474.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2566.82 ms /    34 runs   (   75.49 ms per token,    13.25 tokens per second)\n",
            "llama_perf_context_print:       total time =    3017.76 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.63 ms /   162 tokens (    2.31 ms per token,   432.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3782.35 ms /    50 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    4201.78 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     340.92 ms /   159 tokens (    2.14 ms per token,   466.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2653.29 ms /    35 runs   (   75.81 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3024.32 ms /   194 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 162 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.54 ms /   162 tokens (    2.29 ms per token,   436.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2653.14 ms /    35 runs   (   75.80 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3055.76 ms /   197 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 190 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     382.61 ms /   190 tokens (    2.01 ms per token,   496.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3866.29 ms /    51 runs   (   75.81 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    4295.02 ms /   241 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 168 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.22 ms /   168 tokens (    2.22 ms per token,   450.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1960.03 ms /    26 runs   (   75.39 ms per token,    13.27 tokens per second)\n",
            "llama_perf_context_print:       total time =    2355.12 ms /   194 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n",
            "Llama.generate: 1 prefix-match hit, remaining 259 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     566.06 ms /   259 tokens (    2.19 ms per token,   457.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2544.53 ms /    33 runs   (   77.11 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    3137.91 ms /   292 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 166 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     373.95 ms /   166 tokens (    2.25 ms per token,   443.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2662.52 ms /    35 runs   (   76.07 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3065.88 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     371.06 ms /   163 tokens (    2.28 ms per token,   439.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2654.12 ms /    35 runs   (   75.83 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    3054.60 ms /   198 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 184 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     380.08 ms /   184 tokens (    2.07 ms per token,   484.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3883.11 ms /    51 runs   (   76.14 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    4306.61 ms /   235 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     376.69 ms /   174 tokens (    2.16 ms per token,   461.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2819.91 ms /    37 runs   (   76.21 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    3227.38 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 191 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     381.69 ms /   191 tokens (    2.00 ms per token,   500.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5958.76 ms /    78 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    6408.37 ms /   269 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            "Llama.generate: 1 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     370.37 ms /   164 tokens (    2.26 ms per token,   442.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2814.86 ms /    37 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3215.40 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     345.13 ms /   159 tokens (    2.17 ms per token,   460.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2899.17 ms /    38 runs   (   76.29 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    3277.24 ms /   197 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.52 ms /   180 tokens (    2.07 ms per token,   483.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2678.90 ms /    35 runs   (   76.54 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    3080.18 ms /   215 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 176 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.25 ms /   176 tokens (    2.13 ms per token,   469.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4336.87 ms /    57 runs   (   76.09 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    4759.09 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         54\n",
            "Llama.generate: 1 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     418.16 ms /   197 tokens (    2.12 ms per token,   471.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3824.91 ms /    50 runs   (   76.50 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    4286.48 ms /   247 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 176 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     379.12 ms /   176 tokens (    2.15 ms per token,   464.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7973.49 ms /   104 runs   (   76.67 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    8445.63 ms /   280 tokens\n",
            "llama_perf_context_print:    graphs reused =        100\n",
            "Llama.generate: 1 prefix-match hit, remaining 203 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     421.71 ms /   203 tokens (    2.08 ms per token,   481.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4357.50 ms /    57 runs   (   76.45 ms per token,    13.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    4828.27 ms /   260 tokens\n",
            "llama_perf_context_print:    graphs reused =         54\n",
            "Llama.generate: 1 prefix-match hit, remaining 147 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     342.09 ms /   147 tokens (    2.33 ms per token,   429.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2501.66 ms /    33 runs   (   75.81 ms per token,    13.19 tokens per second)\n",
            "llama_perf_context_print:       total time =    2870.97 ms /   180 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     378.08 ms /   180 tokens (    2.10 ms per token,   476.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2901.43 ms /    38 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3311.98 ms /   218 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 197 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     422.85 ms /   197 tokens (    2.15 ms per token,   465.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5828.04 ms /    76 runs   (   76.68 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    6316.35 ms /   273 tokens\n",
            "llama_perf_context_print:    graphs reused =         73\n",
            "Llama.generate: 1 prefix-match hit, remaining 170 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     375.00 ms /   170 tokens (    2.21 ms per token,   453.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2508.80 ms /    33 runs   (   76.02 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2911.10 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 195 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     417.14 ms /   195 tokens (    2.14 ms per token,   467.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2908.06 ms /    38 runs   (   76.53 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    3355.99 ms /   233 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 200 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     423.56 ms /   200 tokens (    2.12 ms per token,   472.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6057.40 ms /    79 runs   (   76.68 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    6548.72 ms /   279 tokens\n",
            "llama_perf_context_print:    graphs reused =         76\n",
            "Llama.generate: 1 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     372.91 ms /   174 tokens (    2.14 ms per token,   466.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8967.12 ms /   117 runs   (   76.64 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    9449.51 ms /   291 tokens\n",
            "llama_perf_context_print:    graphs reused =        112\n",
            "Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.03 ms /   159 tokens (    2.14 ms per token,   466.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2442.78 ms /    32 runs   (   76.34 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2810.31 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     382.79 ms /   177 tokens (    2.16 ms per token,   462.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6723.14 ms /    88 runs   (   76.40 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    7184.06 ms /   265 tokens\n",
            "llama_perf_context_print:    graphs reused =         84\n",
            "Llama.generate: 1 prefix-match hit, remaining 158 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     343.33 ms /   158 tokens (    2.17 ms per token,   460.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2682.96 ms /    35 runs   (   76.66 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    3054.16 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 173 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.41 ms /   173 tokens (    2.16 ms per token,   462.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2680.46 ms /    35 runs   (   76.58 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    3083.23 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     369.99 ms /   164 tokens (    2.26 ms per token,   443.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2194.02 ms /    29 runs   (   75.66 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2588.57 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =         27\n",
            "Llama.generate: 1 prefix-match hit, remaining 172 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     378.05 ms /   172 tokens (    2.20 ms per token,   454.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2839.60 ms /    37 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3247.75 ms /   209 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 174 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.80 ms /   174 tokens (    2.15 ms per token,   464.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2444.57 ms /    32 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2845.30 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     374.59 ms /   177 tokens (    2.12 ms per token,   472.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8903.41 ms /   116 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    9383.72 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =        111\n",
            "Llama.generate: 1 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     341.40 ms /   157 tokens (    2.17 ms per token,   459.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1981.12 ms /    26 runs   (   76.20 ms per token,    13.12 tokens per second)\n",
            "llama_perf_context_print:       total time =    2343.52 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =         24\n"
          ]
        }
      ],
      "source": [
        "data_2['model_response_2'] = data_2['Article'].apply(lambda x: generate_llama_response(instruction_2, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kbgcUiZ2fey5",
        "outputId": "0047a828-6427-4369-959e-5fea810e7b20"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Sure, I can help you with that! Based on the content of the article provided, I would tag it as follows:\\n\\n{\"label\": \"Business\"}'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Printing the model's response.\n",
        "data_2[\"model_response_2\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "TdKFyul6VFtB"
      },
      "outputs": [],
      "source": [
        "# defining a function to parse the JSON output from the model\n",
        "def extract_json_data(json_str):\n",
        "    try:\n",
        "        # Find the indices of the opening and closing curly braces\n",
        "        json_start = json_str.find('{')\n",
        "        json_end = json_str.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            extracted_json = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
        "            \n",
        "            # Fix common escape issues in LLM-generated JSON\n",
        "            # Replace invalid backslash escapes (but preserve valid ones like \\n, \\t, \\\\, \\\")\n",
        "            import re\n",
        "            # Fix invalid escape sequences by escaping lone backslashes\n",
        "            extracted_json = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', extracted_json)\n",
        "            \n",
        "            data_dict = json.loads(extracted_json)\n",
        "            return data_dict\n",
        "        else:\n",
        "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
        "            return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "pYOXZuGXQB7q"
      },
      "outputs": [],
      "source": [
        "#Applying the function.\n",
        "data_2[\"model_response_2_parsed\"] = data_2[\"model_response_2\"].apply(extract_json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_My5r4hQP0t",
        "outputId": "d8ce1f66-a425-41aa-e0a2-68c9941d903e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model_response_2_parsed",
                  "rawType": "object",
                  "type": "unknown"
                }
              ],
              "ref": "df546455-d7cd-47c9-9bf4-e98cc1042d76",
              "rows": [
                [
                  "0",
                  "{'label': 'Business'}"
                ],
                [
                  "1",
                  "{'label': 'Business'}"
                ],
                [
                  "2",
                  "{'label': 'World'}"
                ],
                [
                  "3",
                  "{'label': 'World'}"
                ],
                [
                  "4",
                  "{'label': 'World'}"
                ],
                [
                  "5",
                  "{'label': 'World'}"
                ],
                [
                  "6",
                  "{'label': 'World'}"
                ],
                [
                  "7",
                  "{'label': 'Business'}"
                ],
                [
                  "8",
                  "{'label': 'World'}"
                ],
                [
                  "9",
                  "{'label': 'World'}"
                ],
                [
                  "10",
                  "{'label': 'Politics'}"
                ],
                [
                  "11",
                  "{'label': 'Sci/Tech'}"
                ],
                [
                  "12",
                  "{'label': 'World'}"
                ],
                [
                  "13",
                  "{'label': 'World'}"
                ],
                [
                  "14",
                  "{'label': 'World'}"
                ],
                [
                  "15",
                  "{'label': 'Business'}"
                ],
                [
                  "16",
                  "{'label': 'World'}"
                ],
                [
                  "17",
                  "{'label': 'Politics'}"
                ],
                [
                  "18",
                  "{'label': 'World'}"
                ],
                [
                  "19",
                  "{'label': 'World'}"
                ],
                [
                  "20",
                  "{'label': 'World'}"
                ],
                [
                  "21",
                  "{'label': 'World'}"
                ],
                [
                  "22",
                  "{'label': 'Sci/Tech'}"
                ],
                [
                  "23",
                  "{'label': 'World'}"
                ],
                [
                  "24",
                  "{'label': 'World'}"
                ],
                [
                  "25",
                  "{'label': 'Sports'}"
                ],
                [
                  "26",
                  "{'label': 'Business'}"
                ],
                [
                  "27",
                  "{'label': 'Sports'}"
                ],
                [
                  "28",
                  "{'label': 'Sports'}"
                ],
                [
                  "29",
                  "{'label': 'Sports'}"
                ],
                [
                  "30",
                  "{'label': 'Sports'}"
                ],
                [
                  "31",
                  "{'label': 'Sports'}"
                ],
                [
                  "32",
                  "{'label': 'Sports'}"
                ],
                [
                  "33",
                  "{'label': 'Sports'}"
                ],
                [
                  "34",
                  "{'label': 'Sports'}"
                ],
                [
                  "35",
                  "{'label': 'Sports'}"
                ],
                [
                  "36",
                  "{'label': 'Sports'}"
                ],
                [
                  "37",
                  "{'label': 'Sports'}"
                ],
                [
                  "38",
                  "{'label': 'Sports'}"
                ],
                [
                  "39",
                  "{'label': 'Sports'}"
                ],
                [
                  "40",
                  "{'label': 'Sports'}"
                ],
                [
                  "41",
                  "{'label': 'Sports'}"
                ],
                [
                  "42",
                  "{'label': 'Sports'}"
                ],
                [
                  "43",
                  "{'label': 'Sports'}"
                ],
                [
                  "44",
                  "{'label': 'Sports'}"
                ],
                [
                  "45",
                  "{'label': 'Sports'}"
                ],
                [
                  "46",
                  "{'label': 'Sports'}"
                ],
                [
                  "47",
                  "{'label': 'Sports'}"
                ],
                [
                  "48",
                  "{'label': 'Sports'}"
                ],
                [
                  "49",
                  "{'label': 'Sports'}"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 100
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_response_2_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'label': 'World'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'label': 'World'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'label': 'World'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>{'label': 'Sci/Tech'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>{'label': 'Sci/Tech'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>{'label': 'Sci/Tech'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0     {'label': 'Business'}\n",
              "1     {'label': 'Business'}\n",
              "2        {'label': 'World'}\n",
              "3        {'label': 'World'}\n",
              "4        {'label': 'World'}\n",
              "              ...          \n",
              "95    {'label': 'Sci/Tech'}\n",
              "96    {'label': 'Sci/Tech'}\n",
              "97    {'label': 'Sci/Tech'}\n",
              "98    {'label': 'Business'}\n",
              "99    {'label': 'Business'}\n",
              "Name: model_response_2_parsed, Length: 100, dtype: object"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Printing the labels predicted by the model.\n",
        "data_2[\"model_response_2_parsed\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "AK77fOZY5_II",
        "outputId": "5a408367-2552-403b-8a0a-acf879a47b8a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_2",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_2_parsed",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "863fa8af-98d7-4293-b10b-31a7e1f4bd91",
              "rows": [],
              "shape": {
                "columns": 3,
                "rows": 0
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-728b0bd0-e3b3-4892-91b3-5fe014bde6c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_2</th>\n",
              "      <th>model_response_2_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-728b0bd0-e3b3-4892-91b3-5fe014bde6c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-728b0bd0-e3b3-4892-91b3-5fe014bde6c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-728b0bd0-e3b3-4892-91b3-5fe014bde6c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Article, model_response_2, model_response_2_parsed]\n",
              "Index: []"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_2[data_2[\"model_response_2_parsed\"]=={}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rVjzgozMQUPc",
        "outputId": "43388d3f-4f96-4449-e879-ef3051b6401a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "3591d76b-6ec8-46fa-a372-f0e0dbc61a06",
              "rows": [
                [
                  "0",
                  "Business"
                ],
                [
                  "1",
                  "Business"
                ],
                [
                  "2",
                  "World"
                ],
                [
                  "3",
                  "World"
                ],
                [
                  "4",
                  "World"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a8cfbba3-da75-4fd2-bfb6-0d8e6930f85f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8cfbba3-da75-4fd2-bfb6-0d8e6930f85f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8cfbba3-da75-4fd2-bfb6-0d8e6930f85f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8cfbba3-da75-4fd2-bfb6-0d8e6930f85f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      label\n",
              "0  Business\n",
              "1  Business\n",
              "2     World\n",
              "3     World\n",
              "4     World"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_response_parsed_df_2 = pd.json_normalize(data_2['model_response_2_parsed'])\n",
        "model_response_parsed_df_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Unt5z3WGQbob",
        "outputId": "2413d937-2eff-44fa-8725-13f4eacb56ac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_2",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_2_parsed",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "aac7fd4f-923c-49d1-8488-c52853d554e6",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it as follows:\n\n{\"label\": \"Business\"}",
                  "{'label': 'Business'}",
                  "Business"
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  " Sure! Here's the article you provided, tagged as \"Business\":\n\n{\"label\": \"Business\"}",
                  "{'label': 'Business'}",
                  "Business"
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it as:\n\n{\"label\": \"World\"}",
                  "{'label': 'World'}",
                  "World"
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  " Sure, I can help you with that! Based on the content of the article you provided, I would tag it as follows:\n\n{\"label\": \"World\"}",
                  "{'label': 'World'}",
                  "World"
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  " Sure, I can help you with that! Based on the content of the article provided, I would tag it as follows:\n\n{\"label\": \"World\"}",
                  "{'label': 'World'}",
                  "World"
                ]
              ],
              "shape": {
                "columns": 4,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-148c8b16-2123-440c-8816-bd7609ba2dd9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_2</th>\n",
              "      <th>model_response_2_parsed</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>Sure, I can help you with that! Based on the ...</td>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>Sure! Here's the article you provided, tagged...</td>\n",
              "      <td>{'label': 'Business'}</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>Sure, I can help you with that! Based on the ...</td>\n",
              "      <td>{'label': 'World'}</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>Sure, I can help you with that! Based on the ...</td>\n",
              "      <td>{'label': 'World'}</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>Sure, I can help you with that! Based on the ...</td>\n",
              "      <td>{'label': 'World'}</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-148c8b16-2123-440c-8816-bd7609ba2dd9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-148c8b16-2123-440c-8816-bd7609ba2dd9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-148c8b16-2123-440c-8816-bd7609ba2dd9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article  \\\n",
              "0  A New Push to Loosen New York's Divorce Law La...   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...   \n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...   \n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                    model_response_2 model_response_2_parsed  \\\n",
              "0   Sure, I can help you with that! Based on the ...   {'label': 'Business'}   \n",
              "1   Sure! Here's the article you provided, tagged...   {'label': 'Business'}   \n",
              "2   Sure, I can help you with that! Based on the ...      {'label': 'World'}   \n",
              "3   Sure, I can help you with that! Based on the ...      {'label': 'World'}   \n",
              "4   Sure, I can help you with that! Based on the ...      {'label': 'World'}   \n",
              "\n",
              "      label  \n",
              "0  Business  \n",
              "1  Business  \n",
              "2     World  \n",
              "3     World  \n",
              "4     World  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_with_parsed_model_output_2 = pd.concat([data_2, model_response_parsed_df_2], axis=1)\n",
        "data_with_parsed_model_output_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mwQf2TFOuqCO",
        "outputId": "09820f44-2fdb-47e0-ccfc-e46512d78a67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "1b970c7e-a42c-4716-a101-671a6521e5a8",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "Business"
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "Business"
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "World"
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "World"
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "World"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-647b8961-3756-47b3-ac95-7ecac0e182b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-647b8961-3756-47b3-ac95-7ecac0e182b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-647b8961-3756-47b3-ac95-7ecac0e182b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-647b8961-3756-47b3-ac95-7ecac0e182b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article     label\n",
              "0  A New Push to Loosen New York's Divorce Law La...  Business\n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...  Business\n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...     World\n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...     World\n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...     World"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data_2 = data_with_parsed_model_output_2.drop(['model_response_2','model_response_2_parsed'], axis=1)\n",
        "final_data_2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEz-HcaObq9"
      },
      "source": [
        "## Classifying the news articles and generating a headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "D2oJrGt3err1"
      },
      "outputs": [],
      "source": [
        "data_3 = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "voim8RLhNtny"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_3 = \"\"\"\n",
        "    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only depending upon the content of the article:\n",
        "    - World\n",
        "    - Sports\n",
        "    - Business\n",
        "    - Sci/Tech\n",
        "\n",
        "    Once done,craft a compelling headline summarizing the key insights and significance of the article.Ensure the headline is attention-grabbing and concise while accurately reflecting the content and output it.\n",
        "\n",
        "    Provide the output in a JSON format with the following keys:\n",
        "    {\n",
        "        \"Label\": \"your_label_prediction\",\n",
        "        \"Headline\": \"your_headline_prediction\",\n",
        "    }\n",
        "\n",
        "    Only return the JSON, do not return any other information and remove the extra spaces.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEPsF7mcOivX",
        "outputId": "107a56af-38fb-47d1-d4b4-85dbc6f65138"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     526.39 ms /   248 tokens (    2.12 ms per token,   471.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2876.64 ms /    37 runs   (   77.75 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3433.71 ms /   285 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 236 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     474.06 ms /   236 tokens (    2.01 ms per token,   497.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3456.55 ms /    43 runs   (   80.38 ms per token,    12.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    3966.46 ms /   279 tokens\n",
            "llama_perf_context_print:    graphs reused =         41\n",
            "Llama.generate: 1 prefix-match hit, remaining 274 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     587.05 ms /   274 tokens (    2.14 ms per token,   466.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2964.00 ms /    36 runs   (   82.33 ms per token,    12.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3582.64 ms /   310 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     481.86 ms /   248 tokens (    1.94 ms per token,   514.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3025.46 ms /    36 runs   (   84.04 ms per token,    11.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    3536.77 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     611.24 ms /   282 tokens (    2.17 ms per token,   461.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3255.28 ms /    38 runs   (   85.67 ms per token,    11.67 tokens per second)\n",
            "llama_perf_context_print:       total time =    3897.91 ms /   320 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 283 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     600.46 ms /   283 tokens (    2.12 ms per token,   471.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3145.12 ms /    37 runs   (   85.00 ms per token,    11.76 tokens per second)\n",
            "llama_perf_context_print:       total time =    3777.09 ms /   320 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 280 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     596.94 ms /   280 tokens (    2.13 ms per token,   469.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3735.20 ms /    45 runs   (   83.00 ms per token,    12.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    4369.26 ms /   325 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 226 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.33 ms /   226 tokens (    2.09 ms per token,   478.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4156.57 ms /    52 runs   (   79.93 ms per token,    12.51 tokens per second)\n",
            "llama_perf_context_print:       total time =    4673.84 ms /   278 tokens\n",
            "llama_perf_context_print:    graphs reused =         50\n",
            "Llama.generate: 1 prefix-match hit, remaining 293 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.13 ms /   293 tokens (    2.17 ms per token,   461.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3886.43 ms /    49 runs   (   79.31 ms per token,    12.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    4561.64 ms /   342 tokens\n",
            "llama_perf_context_print:    graphs reused =         47\n",
            "Llama.generate: 1 prefix-match hit, remaining 272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     577.60 ms /   272 tokens (    2.12 ms per token,   470.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2566.16 ms /    33 runs   (   77.76 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3169.87 ms /   305 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 234 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.85 ms /   234 tokens (    2.00 ms per token,   499.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2595.83 ms /    34 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3092.71 ms /   268 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.22 ms /   248 tokens (    1.88 ms per token,   530.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4373.68 ms /    58 runs   (   75.41 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    4891.45 ms /   306 tokens\n",
            "llama_perf_context_print:    graphs reused =         55\n",
            "Llama.generate: 1 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     465.92 ms /   252 tokens (    1.85 ms per token,   540.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2720.05 ms /    36 runs   (   75.56 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    3214.91 ms /   288 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.56 ms /   247 tokens (    1.90 ms per token,   527.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2921.63 ms /    39 runs   (   74.91 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    3423.82 ms /   286 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 271 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     561.10 ms /   271 tokens (    2.07 ms per token,   482.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4039.99 ms /    54 runs   (   74.81 ms per token,    13.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    4646.54 ms /   325 tokens\n",
            "llama_perf_context_print:    graphs reused =         51\n",
            "Llama.generate: 1 prefix-match hit, remaining 318 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.21 ms /   318 tokens (    1.98 ms per token,   504.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3882.46 ms /    52 runs   (   74.66 ms per token,    13.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    4556.06 ms /   370 tokens\n",
            "llama_perf_context_print:    graphs reused =         49\n",
            "Llama.generate: 1 prefix-match hit, remaining 231 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     462.04 ms /   231 tokens (    2.00 ms per token,   499.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2067.94 ms /    28 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2553.25 ms /   259 tokens\n",
            "llama_perf_context_print:    graphs reused =         26\n",
            "Llama.generate: 1 prefix-match hit, remaining 241 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     461.57 ms /   241 tokens (    1.92 ms per token,   522.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3203.16 ms /    43 runs   (   74.49 ms per token,    13.42 tokens per second)\n",
            "llama_perf_context_print:       total time =    3701.04 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =         41\n",
            "Llama.generate: 1 prefix-match hit, remaining 272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     564.06 ms /   272 tokens (    2.07 ms per token,   482.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2845.44 ms /    38 runs   (   74.88 ms per token,    13.35 tokens per second)\n",
            "llama_perf_context_print:       total time =    3440.62 ms /   310 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     476.15 ms /   256 tokens (    1.86 ms per token,   537.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2497.10 ms /    33 runs   (   75.67 ms per token,    13.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    3000.73 ms /   289 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 251 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.99 ms /   251 tokens (    1.87 ms per token,   535.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3017.18 ms /    40 runs   (   75.43 ms per token,    13.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    3521.22 ms /   291 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 255 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.68 ms /   255 tokens (    1.85 ms per token,   541.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3197.21 ms /    42 runs   (   76.12 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3701.26 ms /   297 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 249 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.61 ms /   249 tokens (    1.88 ms per token,   531.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3134.19 ms /    41 runs   (   76.44 ms per token,    13.08 tokens per second)\n",
            "llama_perf_context_print:       total time =    3636.23 ms /   290 tokens\n",
            "llama_perf_context_print:    graphs reused =         38\n",
            "Llama.generate: 1 prefix-match hit, remaining 263 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     576.75 ms /   263 tokens (    2.19 ms per token,   456.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2613.78 ms /    34 runs   (   76.88 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    3219.26 ms /   297 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 279 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     570.47 ms /   279 tokens (    2.04 ms per token,   489.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3675.35 ms /    47 runs   (   78.20 ms per token,    12.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    4285.09 ms /   326 tokens\n",
            "llama_perf_context_print:    graphs reused =         44\n",
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.64 ms /   248 tokens (    1.90 ms per token,   525.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3343.03 ms /    43 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3849.60 ms /   291 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 257 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     575.34 ms /   257 tokens (    2.24 ms per token,   446.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2802.86 ms /    36 runs   (   77.86 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    3410.46 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 251 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.45 ms /   251 tokens (    1.88 ms per token,   531.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3199.97 ms /    41 runs   (   78.05 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    3707.50 ms /   292 tokens\n",
            "llama_perf_context_print:    graphs reused =         38\n",
            "Llama.generate: 1 prefix-match hit, remaining 239 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     473.41 ms /   239 tokens (    1.98 ms per token,   504.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2733.48 ms /    35 runs   (   78.10 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    3235.33 ms /   274 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 292 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.53 ms /   292 tokens (    2.16 ms per token,   463.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2607.67 ms /    33 runs   (   79.02 ms per token,    12.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    3264.91 ms /   325 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 249 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.57 ms /   249 tokens (    1.89 ms per token,   528.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3503.66 ms /    45 runs   (   77.86 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    4013.55 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 255 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     473.72 ms /   255 tokens (    1.86 ms per token,   538.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2567.34 ms /    33 runs   (   77.80 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    3067.91 ms /   288 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 255 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     473.40 ms /   255 tokens (    1.86 ms per token,   538.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2945.08 ms /    38 runs   (   77.50 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    3450.34 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 243 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.13 ms /   243 tokens (    1.94 ms per token,   514.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2710.54 ms /    35 runs   (   77.44 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    3212.30 ms /   278 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 266 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     566.56 ms /   266 tokens (    2.13 ms per token,   469.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2488.04 ms /    32 runs   (   77.75 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    3081.09 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 253 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.56 ms /   253 tokens (    1.86 ms per token,   536.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3472.95 ms /    45 runs   (   77.18 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    3981.35 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 262 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     567.31 ms /   262 tokens (    2.17 ms per token,   461.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2991.87 ms /    39 runs   (   76.71 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    3592.47 ms /   301 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 239 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.29 ms /   239 tokens (    1.98 ms per token,   506.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2764.31 ms /    36 runs   (   76.79 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3265.60 ms /   275 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.11 ms /   248 tokens (    1.90 ms per token,   525.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3063.15 ms /    40 runs   (   76.58 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    3567.68 ms /   288 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 249 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.93 ms /   249 tokens (    1.90 ms per token,   527.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2681.50 ms /    35 runs   (   76.61 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    3183.26 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 249 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.29 ms /   249 tokens (    1.89 ms per token,   528.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2894.14 ms /    38 runs   (   76.16 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    3397.79 ms /   287 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 254 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     469.19 ms /   254 tokens (    1.85 ms per token,   541.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3294.06 ms /    43 runs   (   76.61 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    3798.30 ms /   297 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 265 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     570.14 ms /   265 tokens (    2.15 ms per token,   464.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3827.20 ms /    50 runs   (   76.54 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    4439.16 ms /   315 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 275 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     566.69 ms /   275 tokens (    2.06 ms per token,   485.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2975.76 ms /    39 runs   (   76.30 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    3575.73 ms /   314 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 281 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     580.20 ms /   281 tokens (    2.06 ms per token,   484.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3285.32 ms /    43 runs   (   76.40 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3901.48 ms /   324 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 261 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     563.87 ms /   261 tokens (    2.16 ms per token,   462.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2979.19 ms /    39 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3575.56 ms /   300 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.59 ms /   252 tokens (    1.87 ms per token,   534.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3130.64 ms /    41 runs   (   76.36 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3638.02 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =         38\n",
            "Llama.generate: 1 prefix-match hit, remaining 275 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     575.53 ms /   275 tokens (    2.09 ms per token,   477.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3141.98 ms /    41 runs   (   76.63 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    3751.52 ms /   316 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 294 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.42 ms /   294 tokens (    2.14 ms per token,   466.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3136.76 ms /    41 runs   (   76.51 ms per token,    13.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    3801.96 ms /   335 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     471.00 ms /   252 tokens (    1.87 ms per token,   535.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2520.75 ms /    33 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3019.76 ms /   285 tokens\n",
            "llama_perf_context_print:    graphs reused =         31\n",
            "Llama.generate: 1 prefix-match hit, remaining 280 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     572.09 ms /   280 tokens (    2.04 ms per token,   489.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3622.59 ms /    47 runs   (   77.08 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    4233.90 ms /   327 tokens\n",
            "llama_perf_context_print:    graphs reused =         44\n",
            "Llama.generate: 1 prefix-match hit, remaining 285 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     574.31 ms /   285 tokens (    2.02 ms per token,   496.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3303.10 ms /    43 runs   (   76.82 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3915.22 ms /   328 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 232 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     464.17 ms /   232 tokens (    2.00 ms per token,   499.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3193.85 ms /    42 runs   (   76.04 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    3696.90 ms /   274 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 268 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     573.83 ms /   268 tokens (    2.14 ms per token,   467.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3514.22 ms /    46 runs   (   76.40 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    4128.07 ms /   314 tokens\n",
            "llama_perf_context_print:    graphs reused =         44\n",
            "Llama.generate: 1 prefix-match hit, remaining 261 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     564.19 ms /   261 tokens (    2.16 ms per token,   462.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4059.31 ms /    53 runs   (   76.59 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    4670.53 ms /   314 tokens\n",
            "llama_perf_context_print:    graphs reused =         51\n",
            "Llama.generate: 1 prefix-match hit, remaining 284 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     581.56 ms /   284 tokens (    2.05 ms per token,   488.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3283.27 ms /    43 runs   (   76.36 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3903.88 ms /   327 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 243 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     466.45 ms /   243 tokens (    1.92 ms per token,   520.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3598.32 ms /    47 runs   (   76.56 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    4107.17 ms /   290 tokens\n",
            "llama_perf_context_print:    graphs reused =         44\n",
            "Llama.generate: 1 prefix-match hit, remaining 265 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     565.21 ms /   265 tokens (    2.13 ms per token,   468.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3362.26 ms /    44 runs   (   76.41 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3967.71 ms /   309 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 259 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     564.69 ms /   259 tokens (    2.18 ms per token,   458.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2911.44 ms /    38 runs   (   76.62 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    3510.56 ms /   297 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 244 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.52 ms /   244 tokens (    1.92 ms per token,   521.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2443.85 ms /    32 runs   (   76.37 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2939.76 ms /   276 tokens\n",
            "llama_perf_context_print:    graphs reused =         30\n",
            "Llama.generate: 1 prefix-match hit, remaining 262 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     566.03 ms /   262 tokens (    2.16 ms per token,   462.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3607.23 ms /    47 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    4215.72 ms /   309 tokens\n",
            "llama_perf_context_print:    graphs reused =         45\n",
            "Llama.generate: 1 prefix-match hit, remaining 283 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     570.85 ms /   283 tokens (    2.02 ms per token,   495.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3224.24 ms /    42 runs   (   76.77 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3832.76 ms /   325 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 239 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     466.50 ms /   239 tokens (    1.95 ms per token,   512.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3130.15 ms /    41 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3631.92 ms /   280 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 266 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     573.36 ms /   266 tokens (    2.16 ms per token,   463.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3054.86 ms /    40 runs   (   76.37 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    3661.38 ms /   306 tokens\n",
            "llama_perf_context_print:    graphs reused =         38\n",
            "Llama.generate: 1 prefix-match hit, remaining 247 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     472.28 ms /   247 tokens (    1.91 ms per token,   522.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2915.71 ms /    38 runs   (   76.73 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3420.98 ms /   285 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 281 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     576.14 ms /   281 tokens (    2.05 ms per token,   487.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2627.45 ms /    34 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    3231.69 ms /   315 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 244 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.10 ms /   244 tokens (    1.93 ms per token,   519.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3189.99 ms /    42 runs   (   75.95 ms per token,    13.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    3694.27 ms /   286 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 241 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.86 ms /   241 tokens (    1.94 ms per token,   515.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3819.26 ms /    50 runs   (   76.39 ms per token,    13.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    4329.70 ms /   291 tokens\n",
            "llama_perf_context_print:    graphs reused =         47\n",
            "Llama.generate: 1 prefix-match hit, remaining 244 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.84 ms /   244 tokens (    1.93 ms per token,   518.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2825.07 ms /    37 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    3326.16 ms /   281 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 272 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     570.86 ms /   272 tokens (    2.10 ms per token,   476.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2841.93 ms /    37 runs   (   76.81 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3442.83 ms /   309 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "Llama.generate: 1 prefix-match hit, remaining 250 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.20 ms /   250 tokens (    1.87 ms per token,   535.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2618.20 ms /    34 runs   (   77.01 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    3113.46 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =         32\n",
            "Llama.generate: 1 prefix-match hit, remaining 341 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     697.69 ms /   341 tokens (    2.05 ms per token,   488.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2794.19 ms /    36 runs   (   77.62 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    3523.07 ms /   377 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n",
            "Llama.generate: 1 prefix-match hit, remaining 248 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.18 ms /   248 tokens (    1.89 ms per token,   529.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4135.20 ms /    54 runs   (   76.58 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    4649.12 ms /   302 tokens\n",
            "llama_perf_context_print:    graphs reused =         51\n",
            "Llama.generate: 1 prefix-match hit, remaining 245 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.45 ms /   245 tokens (    1.92 ms per token,   520.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3126.63 ms /    41 runs   (   76.26 ms per token,    13.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    3631.56 ms /   286 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 266 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     575.63 ms /   266 tokens (    2.16 ms per token,   462.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2688.09 ms /    35 runs   (   76.80 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3292.79 ms /   301 tokens\n",
            "llama_perf_context_print:    graphs reused =         33\n",
            "Llama.generate: 1 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     480.46 ms /   256 tokens (    1.88 ms per token,   532.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2925.03 ms /    38 runs   (   76.97 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    3436.66 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 273 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     573.08 ms /   273 tokens (    2.10 ms per token,   476.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3457.10 ms /    45 runs   (   76.82 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    4067.79 ms /   318 tokens\n",
            "llama_perf_context_print:    graphs reused =         43\n",
            "Llama.generate: 1 prefix-match hit, remaining 246 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.33 ms /   246 tokens (    1.90 ms per token,   526.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3681.63 ms /    48 runs   (   76.70 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    4189.35 ms /   294 tokens\n",
            "llama_perf_context_print:    graphs reused =         45\n",
            "Llama.generate: 1 prefix-match hit, remaining 241 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     465.90 ms /   241 tokens (    1.93 ms per token,   517.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3349.65 ms /    44 runs   (   76.13 ms per token,    13.14 tokens per second)\n",
            "llama_perf_context_print:       total time =    3851.23 ms /   285 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 262 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     567.51 ms /   262 tokens (    2.17 ms per token,   461.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4364.53 ms /    57 runs   (   76.57 ms per token,    13.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    4982.13 ms /   319 tokens\n",
            "llama_perf_context_print:    graphs reused =         55\n",
            "Llama.generate: 1 prefix-match hit, remaining 258 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     564.86 ms /   258 tokens (    2.19 ms per token,   456.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4464.49 ms /    58 runs   (   76.97 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    5082.55 ms /   316 tokens\n",
            "llama_perf_context_print:    graphs reused =         56\n",
            "Llama.generate: 1 prefix-match hit, remaining 279 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     569.47 ms /   279 tokens (    2.04 ms per token,   489.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3237.07 ms /    42 runs   (   77.07 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    3842.93 ms /   321 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 258 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     570.25 ms /   258 tokens (    2.21 ms per token,   452.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3448.22 ms /    45 runs   (   76.63 ms per token,    13.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    4059.61 ms /   303 tokens\n",
            "llama_perf_context_print:    graphs reused =         43\n",
            "Llama.generate: 1 prefix-match hit, remaining 285 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     573.54 ms /   285 tokens (    2.01 ms per token,   496.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14150.09 ms /   183 runs   (   77.32 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =   14910.90 ms /   468 tokens\n",
            "llama_perf_context_print:    graphs reused =        176\n",
            "Llama.generate: 1 prefix-match hit, remaining 229 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     462.34 ms /   229 tokens (    2.02 ms per token,   495.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3604.25 ms /    47 runs   (   76.69 ms per token,    13.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    4107.08 ms /   276 tokens\n",
            "llama_perf_context_print:    graphs reused =         45\n",
            "Llama.generate: 1 prefix-match hit, remaining 262 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     565.11 ms /   262 tokens (    2.16 ms per token,   463.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3852.96 ms /    50 runs   (   77.06 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    4462.76 ms /   312 tokens\n",
            "llama_perf_context_print:    graphs reused =         48\n",
            "Llama.generate: 1 prefix-match hit, remaining 279 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     574.88 ms /   279 tokens (    2.06 ms per token,   485.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3471.59 ms /    45 runs   (   77.15 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    4084.10 ms /   324 tokens\n",
            "llama_perf_context_print:    graphs reused =         42\n",
            "Llama.generate: 1 prefix-match hit, remaining 252 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.38 ms /   252 tokens (    1.87 ms per token,   535.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11808.25 ms /   153 runs   (   77.18 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =   12426.13 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =        147\n",
            "Llama.generate: 1 prefix-match hit, remaining 277 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     571.02 ms /   277 tokens (    2.06 ms per token,   485.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4859.22 ms /    63 runs   (   77.13 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    5485.50 ms /   340 tokens\n",
            "llama_perf_context_print:    graphs reused =         60\n",
            "Llama.generate: 1 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     574.82 ms /   282 tokens (    2.04 ms per token,   490.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3400.06 ms /    44 runs   (   77.27 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    4010.61 ms /   326 tokens\n",
            "llama_perf_context_print:    graphs reused =         41\n",
            "Llama.generate: 1 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     479.03 ms /   256 tokens (    1.87 ms per token,   534.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3932.04 ms /    51 runs   (   77.10 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    4453.26 ms /   307 tokens\n",
            "llama_perf_context_print:    graphs reused =         49\n",
            "Llama.generate: 1 prefix-match hit, remaining 241 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     467.49 ms /   241 tokens (    1.94 ms per token,   515.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3299.36 ms /    43 runs   (   76.73 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3804.37 ms /   284 tokens\n",
            "llama_perf_context_print:    graphs reused =         41\n",
            "Llama.generate: 1 prefix-match hit, remaining 259 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     574.09 ms /   259 tokens (    2.22 ms per token,   451.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3147.67 ms /    41 runs   (   76.77 ms per token,    13.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    3755.37 ms /   300 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n",
            "Llama.generate: 1 prefix-match hit, remaining 240 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     468.81 ms /   240 tokens (    1.95 ms per token,   511.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3226.70 ms /    42 runs   (   76.83 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    3730.05 ms /   282 tokens\n",
            "llama_perf_context_print:    graphs reused =         40\n",
            "Llama.generate: 1 prefix-match hit, remaining 255 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     477.19 ms /   255 tokens (    1.87 ms per token,   534.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4173.27 ms /    54 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    4697.44 ms /   309 tokens\n",
            "llama_perf_context_print:    graphs reused =         52\n",
            "Llama.generate: 1 prefix-match hit, remaining 246 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     474.72 ms /   246 tokens (    1.93 ms per token,   518.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3864.20 ms /    50 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    4379.99 ms /   296 tokens\n",
            "llama_perf_context_print:    graphs reused =         47\n",
            "Llama.generate: 1 prefix-match hit, remaining 254 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     476.63 ms /   254 tokens (    1.88 ms per token,   532.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3003.67 ms /    39 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    3513.63 ms /   293 tokens\n",
            "llama_perf_context_print:    graphs reused =         36\n",
            "Llama.generate: 1 prefix-match hit, remaining 256 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     478.22 ms /   256 tokens (    1.87 ms per token,   535.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5335.56 ms /    69 runs   (   77.33 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    5873.90 ms /   325 tokens\n",
            "llama_perf_context_print:    graphs reused =         66\n",
            "Llama.generate: 1 prefix-match hit, remaining 259 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     568.59 ms /   259 tokens (    2.20 ms per token,   455.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4724.77 ms /    61 runs   (   77.46 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    5345.24 ms /   320 tokens\n",
            "llama_perf_context_print:    graphs reused =         58\n",
            "Llama.generate: 1 prefix-match hit, remaining 239 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     470.06 ms /   239 tokens (    1.97 ms per token,   508.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2774.31 ms /    36 runs   (   77.06 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    3276.45 ms /   275 tokens\n",
            "llama_perf_context_print:    graphs reused =         34\n"
          ]
        }
      ],
      "source": [
        "data_3['model_response_3'] = data_3['Article'].apply(lambda x: generate_llama_response(instruction_3, x).replace('\\n', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH1tRnpZVmRV",
        "outputId": "7cd9a859-bbb3-4323-c1d8-4c7ae476d985"
      },
      "outputs": [],
      "source": [
        "data_3[\"model_response_3_parsed\"] = data_3[\"model_response_3\"].apply(extract_json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYjS9ZTIXHCA",
        "outputId": "8674267d-576d-4821-e68a-d0edc0f241c6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model_response_3_parsed",
                  "rawType": "object",
                  "type": "unknown"
                }
              ],
              "ref": "d3b103d6-59fa-4d6f-86f8-9d3861f37175",
              "rows": [
                [
                  "0",
                  "{'Label': 'Business', 'Headline': \"New York's Divorce Law Under Scrutiny: A Push for Change\"}"
                ],
                [
                  "1",
                  "{'Label': 'Business', 'Headline': 'Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look'}"
                ],
                [
                  "2",
                  "{'Label': 'World', 'Headline': 'New Chechen Leader Vows Peace, Poll Criticized'}"
                ],
                [
                  "3",
                  "{'Label': 'World', 'Headline': 'Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon'}"
                ],
                [
                  "4",
                  "{'Label': 'World', 'Headline': 'U.S. State Dept Finishes Review of Iraq Aid Plan'}"
                ],
                [
                  "5",
                  "{'Label': 'World', 'Headline': 'Iraq Elections to Proceed Despite Violence and Grisly Hostage Video'}"
                ],
                [
                  "6",
                  "{'Label': 'World', 'Headline': 'Indonesian Cleric Abu Bakar Bashir on Trial for Al Qaeda Links and Terror Plots'}"
                ],
                [
                  "7",
                  "{'Label': None, 'Headline': None}"
                ],
                [
                  "8",
                  "{'Label': 'World', 'Headline': 'U.S. Helicopters Rescue Injured Refugees in Aceh, Orphan Trafficking Reports Surface'}"
                ],
                [
                  "9",
                  "{'Label': 'World', 'Headline': 'Northern Ireland Peace Talks End Without Breakthrough'}"
                ],
                [
                  "10",
                  "{'Label': 'World', 'Headline': 'Security Measures Fail to Spot Protesters at Labour Conference'}"
                ],
                [
                  "11",
                  "{'Label': 'World', 'Headline': 'Galapagos Park Rangers Go on Strike Over Firing of Director'}"
                ],
                [
                  "12",
                  "{'Label': 'World', 'Headline': 'US troops targeted in Iraq attacks, four civilians killed and two soldiers wounded'}"
                ],
                [
                  "13",
                  "{'Label': 'World', 'Headline': 'Afghan Election: Karzai Expected to Win, Early Results Show'}"
                ],
                [
                  "14",
                  "{'Label': 'World', 'Headline': 'Ukraine Seeks to Control Yushchenko Probe'}"
                ],
                [
                  "15",
                  "{'Label': 'Business', 'Headline': 'L.A., Washington Hotel Workers Strike: Thousands of Housekeepers, Bellmen and Other Workers Authorize Walkout'}"
                ],
                [
                  "16",
                  "{'Label': 'World', 'Headline': 'Nigeria lifts Plateau emergency'}"
                ],
                [
                  "17",
                  "{'Label': 'Politics', 'Headline': 'Voters Begin Casting Early Ballots in Florida, Memories of 2000 Still Fresh'}"
                ],
                [
                  "18",
                  "{'Label': 'World', 'Headline': 'Tamil Tigers Make Unexpected Offer to Revive Peace Talks in Sri Lanka'}"
                ],
                [
                  "19",
                  "{'Label': 'World', 'Headline': 'Israeli Lions Symbols of Peace at Palestinian Zoo'}"
                ],
                [
                  "20",
                  "{'Label': 'World', 'Headline': \"North Korea's Mysterious Blast: Britain's Ambassador Investigates\"}"
                ],
                [
                  "21",
                  "{'Label': 'World', 'Headline': \"Iraq Voter Registration Begins Amidst Latest Violence and Deputy Governor's Death\"}"
                ],
                [
                  "22",
                  "{'Label': 'World', 'Headline': \"Rubbish Art Sparks Controversy at London's Tate Gallery\"}"
                ],
                [
                  "23",
                  "{'Label': 'World', 'Headline': 'Insurgents Attack Iraq National Assembly Opening'}"
                ],
                [
                  "24",
                  "{'Label': 'World', 'Headline': 'Hurricane Ivan Heads for Jamaica: A Life-Threatening Storm Set to Hit the Island Nation'}"
                ],
                [
                  "25",
                  "{'Label': 'Sports', 'Headline': 'Five things to watch: Can LaDainian Tomlinson cure his groin injury against the Raiders?'}"
                ],
                [
                  "26",
                  "{'Label': 'Business', 'Headline': 'Bank Mulls Rival Proposal for Manchester United'}"
                ],
                [
                  "27",
                  "{'Label': 'Sports', 'Headline': \"Vijay Singh's Win Topples Tiger Woods from Top Spot\"}"
                ],
                [
                  "28",
                  "{'Label': 'Sports', 'Headline': 'Chiefs Crush Rams: A Dominant Performance on Monday Night Football'}"
                ],
                [
                  "29",
                  "{'Label': 'Sports', 'Headline': 'Big Boi Concert Hopes to Keep College Football Team in Division'}"
                ],
                [
                  "30",
                  "{'Label': 'Sports', 'Headline': \"Georgetown Prep's Football Team Banned from IAC Due to Excessive Dominance\"}"
                ],
                [
                  "31",
                  "{'Label': 'Sports', 'Headline': \"Clijsters' Wrist Injury Threatens Career\"}"
                ],
                [
                  "32",
                  "{'Label': 'Sports', 'Headline': 'Underdog Danny Williams Makes a Strong Case for the Heavyweight Title Fight'}"
                ],
                [
                  "33",
                  "{'Label': 'Sports', 'Headline': \"Gerrard: We'll Get It Right, Claims Gerrard\"}"
                ],
                [
                  "34",
                  "{'Label': 'Sports', 'Headline': 'Angry Ferrero Has Not Given Up Hope'}"
                ],
                [
                  "35",
                  "{'Label': 'Sports', 'Headline': 'Yao Ming and the Houston Rockets Make History in Shanghai: A Game-Changing Moment for NBA in China'}"
                ],
                [
                  "36",
                  "{'Label': 'Sports', 'Headline': 'Globetrotters Challenge Argentina to a $1 Million Winner-Take-All Game'}"
                ],
                [
                  "37",
                  "{'Label': 'Sports', 'Headline': \"Auburn Tigers' Offensive Balance Key to Success\"}"
                ],
                [
                  "38",
                  "{'Label': 'Sports', 'Headline': \"Dwyane Wade's New Role as a Back-Seat Driver\"}"
                ],
                [
                  "39",
                  "{'Label': 'Sports', 'Headline': 'Pacers image hurting after brawl, charges'}"
                ],
                [
                  "40",
                  "{'Label': 'Sports', 'Headline': 'Rangers Pitcher Suspended for Rest of Season After Arrest'}"
                ],
                [
                  "41",
                  "{'Label': 'Sports', 'Headline': \"John Daly's Wife Pleads Guilty to Federal Money Laundering Charge\"}"
                ],
                [
                  "42",
                  "{'Label': 'Sports', 'Headline': 'Coleman Insists Fulham Must quot;Wake Up and Smell the Coffeequot; to End Dismal Start'}"
                ],
                [
                  "43",
                  "{'Label': 'Sports', 'Headline': 'Hewitt Wins Washington Open, Fine-Tunes Game for US Open'}"
                ],
                [
                  "44",
                  "{'Label': 'Sports', 'Headline': \"Beltran's Record-Tying Home Run Leads Astros to Comeback Win in NLCS\"}"
                ],
                [
                  "45",
                  "{'Label': 'Sports', 'Headline': 'Mark Kreidler: An ocean away, an experience to savor Bee Sports Columnist'}"
                ],
                [
                  "46",
                  "{'Label': 'Sports', 'Headline': \"Belichick Highlights Miami's Strong Defense Despite Woeful Season\"}"
                ],
                [
                  "47",
                  "{'Label': 'World', 'Headline': \"Greece's Security Preparations for Olympics Successful, Says Defense Minister\"}"
                ],
                [
                  "48",
                  "{'Label': 'Sports', 'Headline': 'LeBron James Leads Cavaliers to Victory Over Raptors with 27 Points'}"
                ],
                [
                  "49",
                  "{'Label': 'Sports', 'Headline': 'Pampling clings to lead at Australian Open'}"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 100
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_response_3_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'New York's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Memos Warne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'New Chechen Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'Two UN Workers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'U.S. State Dep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>{'Label': 'Sci/Tech', 'Headline': 'Atomic Visi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>{'Label': 'Sci/Tech', 'Headline': 'Ninth-grade...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>{'Label': 'Sci/Tech', 'Headline': 'Congress Wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'IBM Jumps i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Clearing Th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0     {'Label': 'Business', 'Headline': 'New York's ...\n",
              "1     {'Label': 'Business', 'Headline': 'Memos Warne...\n",
              "2     {'Label': 'World', 'Headline': 'New Chechen Le...\n",
              "3     {'Label': 'World', 'Headline': 'Two UN Workers...\n",
              "4     {'Label': 'World', 'Headline': 'U.S. State Dep...\n",
              "                            ...                        \n",
              "95    {'Label': 'Sci/Tech', 'Headline': 'Atomic Visi...\n",
              "96    {'Label': 'Sci/Tech', 'Headline': 'Ninth-grade...\n",
              "97    {'Label': 'Sci/Tech', 'Headline': 'Congress Wa...\n",
              "98    {'Label': 'Business', 'Headline': 'IBM Jumps i...\n",
              "99    {'Label': 'Business', 'Headline': 'Clearing Th...\n",
              "Name: model_response_3_parsed, Length: 100, dtype: object"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_3[\"model_response_3_parsed\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "MonQLBfk6X5Q",
        "outputId": "b2de942a-eecd-4057-a3dc-0241aa64646e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_3",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_3_parsed",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "b44840ac-114e-439e-b5d8-516cd6da639b",
              "rows": [],
              "shape": {
                "columns": 3,
                "rows": 0
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-74a0740c-d7e7-4191-8f9d-ddb145abea63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_3</th>\n",
              "      <th>model_response_3_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74a0740c-d7e7-4191-8f9d-ddb145abea63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74a0740c-d7e7-4191-8f9d-ddb145abea63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74a0740c-d7e7-4191-8f9d-ddb145abea63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Article, model_response_3, model_response_3_parsed]\n",
              "Index: []"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_3[data_3[\"model_response_3_parsed\"]=={}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mkEkZcqPRk5v",
        "outputId": "d1fbb6d2-bf4c-48e6-f05f-ea48e55f5d1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    {        \"Label\": \"Sports\",        \"Headline\": \"Hewitt Wins Washington Open, Fine-Tunes Game for US Open\"    }'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_3[\"model_response_3\"][43]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "9Om2vLNs7a7_"
      },
      "outputs": [],
      "source": [
        "data_3[\"model_response_3_parsed\"][43] = {\"label\":\"Sports\",\"Headline\":\"Hewitt wins Washington Open: Australia's Lleyton Hewitt Captures $US500,000 Tournament\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "h2WbEVaVRqnU",
        "outputId": "d17c8fa8-959b-4453-b85d-29f97475ac4e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "b8246750-5018-421e-86a1-52eaf7acff14",
              "rows": [
                [
                  "0",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change"
                ],
                [
                  "1",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look"
                ],
                [
                  "2",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized"
                ],
                [
                  "3",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon"
                ],
                [
                  "4",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3b5c7162-1ff0-4ab3-9370-9400428d1fc5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Plan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b5c7162-1ff0-4ab3-9370-9400428d1fc5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3b5c7162-1ff0-4ab3-9370-9400428d1fc5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3b5c7162-1ff0-4ab3-9370-9400428d1fc5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Label                                           Headline\n",
              "0  Business  New York's Divorce Law Under Scrutiny: A Push ...\n",
              "1  Business  Memos Warned of Billing Fraud by Firm in Iraq:...\n",
              "2     World     New Chechen Leader Vows Peace, Poll Criticized\n",
              "3     World  Two UN Workers Kidnapped in Afghanistan Call H...\n",
              "4     World   U.S. State Dept Finishes Review of Iraq Aid Plan"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_response_parsed_df_3 = pd.json_normalize(data_3['model_response_3_parsed']).drop([\"label\"],axis=1)\n",
        "model_response_parsed_df_3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "xvl64RLh8gpE",
        "outputId": "3d2c0e63-b139-4950-af98-dadf3f683066"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_3",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_3_parsed",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "7b1180c6-2f17-477a-b5e2-4e47580ee3c0",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "{\"Label\": \"Business\",\"Headline\": \"New York's Divorce Law Under Scrutiny: A Push for Change\"}",
                  "{'Label': 'Business', 'Headline': \"New York's Divorce Law Under Scrutiny: A Push for Change\"}",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change"
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "    {        \"Label\": \"Business\",        \"Headline\": \"Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look\"    }",
                  "{'Label': 'Business', 'Headline': 'Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look'}",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look"
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "    {        \"Label\": \"World\",        \"Headline\": \"New Chechen Leader Vows Peace, Poll Criticized\"    }",
                  "{'Label': 'World', 'Headline': 'New Chechen Leader Vows Peace, Poll Criticized'}",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized"
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "{\"Label\": \"World\",\"Headline\": \"Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon\"}",
                  "{'Label': 'World', 'Headline': 'Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon'}",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon"
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "    {        \"Label\": \"World\",        \"Headline\": \"U.S. State Dept Finishes Review of Iraq Aid Plan\"    }",
                  "{'Label': 'World', 'Headline': 'U.S. State Dept Finishes Review of Iraq Aid Plan'}",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e4f8ad5d-cb26-405f-b17d-2d6d6cc27249\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_3</th>\n",
              "      <th>model_response_3_parsed</th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>{\"Label\": \"Business\",\"Headline\": \"New York's D...</td>\n",
              "      <td>{'Label': 'Business', 'Headline': 'New York's ...</td>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>{        \"Label\": \"Business\",        \"Head...</td>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Memos Warne...</td>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>{        \"Label\": \"World\",        \"Headlin...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'New Chechen Le...</td>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>{\"Label\": \"World\",\"Headline\": \"Two UN Workers ...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'Two UN Workers...</td>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>{        \"Label\": \"World\",        \"Headlin...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'U.S. State Dep...</td>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Plan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4f8ad5d-cb26-405f-b17d-2d6d6cc27249')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4f8ad5d-cb26-405f-b17d-2d6d6cc27249 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4f8ad5d-cb26-405f-b17d-2d6d6cc27249');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article  \\\n",
              "0  A New Push to Loosen New York's Divorce Law La...   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...   \n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...   \n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                    model_response_3  \\\n",
              "0  {\"Label\": \"Business\",\"Headline\": \"New York's D...   \n",
              "1      {        \"Label\": \"Business\",        \"Head...   \n",
              "2      {        \"Label\": \"World\",        \"Headlin...   \n",
              "3  {\"Label\": \"World\",\"Headline\": \"Two UN Workers ...   \n",
              "4      {        \"Label\": \"World\",        \"Headlin...   \n",
              "\n",
              "                             model_response_3_parsed     Label  \\\n",
              "0  {'Label': 'Business', 'Headline': 'New York's ...  Business   \n",
              "1  {'Label': 'Business', 'Headline': 'Memos Warne...  Business   \n",
              "2  {'Label': 'World', 'Headline': 'New Chechen Le...     World   \n",
              "3  {'Label': 'World', 'Headline': 'Two UN Workers...     World   \n",
              "4  {'Label': 'World', 'Headline': 'U.S. State Dep...     World   \n",
              "\n",
              "                                            Headline  \n",
              "0  New York's Divorce Law Under Scrutiny: A Push ...  \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq:...  \n",
              "2     New Chechen Leader Vows Peace, Poll Criticized  \n",
              "3  Two UN Workers Kidnapped in Afghanistan Call H...  \n",
              "4   U.S. State Dept Finishes Review of Iraq Aid Plan  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_with_parsed_model_output_3 = pd.concat([data_3, model_response_parsed_df_3], axis=1)\n",
        "data_with_parsed_model_output_3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "actLzum1R2Ue",
        "outputId": "3b661362-abe7-46a7-b154-1877897ceace"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "bb072aed-b340-42b3-b720-ec012265eafd",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change"
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq: An Inside Look"
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized"
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon"
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8531435a-ad6c-40f5-bf27-ae8505857f29\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Plan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8531435a-ad6c-40f5-bf27-ae8505857f29')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8531435a-ad6c-40f5-bf27-ae8505857f29 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8531435a-ad6c-40f5-bf27-ae8505857f29');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article     Label  \\\n",
              "0  A New Push to Loosen New York's Divorce Law La...  Business   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...  Business   \n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...     World   \n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...     World   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...     World   \n",
              "\n",
              "                                            Headline  \n",
              "0  New York's Divorce Law Under Scrutiny: A Push ...  \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq:...  \n",
              "2     New Chechen Leader Vows Peace, Poll Criticized  \n",
              "3  Two UN Workers Kidnapped in Afghanistan Call H...  \n",
              "4   U.S. State Dept Finishes Review of Iraq Aid Plan  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data_3 = data_with_parsed_model_output_3.drop(['model_response_3','model_response_3_parsed'], axis=1)\n",
        "final_data_3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyqSugwPOfej"
      },
      "source": [
        "## Classifying the news articles, generating a headline, and generating a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ighVyQmme7IR"
      },
      "outputs": [],
      "source": [
        "data_4 = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zzSL4XGOOhnX"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_4 = \"\"\"\n",
        "    You are an AI analyzing news articles. Tag the given article using one or more of the below mentioned categories only depending upon the content of the article:\n",
        "    - World\n",
        "    - Sports\n",
        "    - Business\n",
        "    - Sci/Tech\n",
        "\n",
        "    Once done,craft a compelling headline summarizing the key insights and significance of the article.Ensure the headline is attention-grabbing and concise while accurately reflecting the content and output it.\n",
        "\n",
        "    Further,\"Summarize the key points and main arguments presented in the news article, while providing a concise yet comprehensive overview of the information and its implications.\"\n",
        "    Provide the output in a JSON format with the following keys:\n",
        "    {\n",
        "        \"Label\": \"your_label_prediction\",\n",
        "        \"Headline\": \"your_headline_prediction\",\n",
        "        \"Summary\": \"your_summary_prediction\"\n",
        "    }\n",
        "\n",
        "    Only return the JSON, do not return any other information and remove the extra spaces.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ3-Bl3cWKEz",
        "outputId": "a2874831-1c87-48b2-c3aa-ce51107b3c3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     714.48 ms /   298 tokens (    2.40 ms per token,   417.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7161.40 ms /    87 runs   (   82.31 ms per token,    12.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    7954.54 ms /   385 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 286 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     605.22 ms /   286 tokens (    2.12 ms per token,   472.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10450.70 ms /   123 runs   (   84.97 ms per token,    11.77 tokens per second)\n",
            "llama_perf_context_print:       total time =   11173.07 ms /   409 tokens\n",
            "llama_perf_context_print:    graphs reused =        118\n",
            "Llama.generate: 1 prefix-match hit, remaining 324 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     653.68 ms /   324 tokens (    2.02 ms per token,   495.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6637.74 ms /    81 runs   (   81.95 ms per token,    12.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    7363.49 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =         78\n",
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.61 ms /   298 tokens (    2.13 ms per token,   468.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5863.17 ms /    74 runs   (   79.23 ms per token,    12.62 tokens per second)\n",
            "llama_perf_context_print:       total time =    6566.46 ms /   372 tokens\n",
            "llama_perf_context_print:    graphs reused =         71\n",
            "Llama.generate: 1 prefix-match hit, remaining 332 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     647.08 ms /   332 tokens (    1.95 ms per token,   513.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7282.29 ms /    94 runs   (   77.47 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    8012.57 ms /   426 tokens\n",
            "llama_perf_context_print:    graphs reused =         90\n",
            "Llama.generate: 1 prefix-match hit, remaining 333 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     637.78 ms /   333 tokens (    1.92 ms per token,   522.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7146.24 ms /    94 runs   (   76.02 ms per token,    13.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    7866.81 ms /   427 tokens\n",
            "llama_perf_context_print:    graphs reused =         90\n",
            "Llama.generate: 1 prefix-match hit, remaining 330 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.71 ms /   330 tokens (    1.92 ms per token,   521.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6925.12 ms /    92 runs   (   75.27 ms per token,    13.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    7639.62 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 276 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     563.07 ms /   276 tokens (    2.04 ms per token,   490.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5144.45 ms /    69 runs   (   74.56 ms per token,    13.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    5766.32 ms /   345 tokens\n",
            "llama_perf_context_print:    graphs reused =         66\n",
            "Llama.generate: 1 prefix-match hit, remaining 343 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     695.82 ms /   343 tokens (    2.03 ms per token,   492.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6870.78 ms /    91 runs   (   75.50 ms per token,    13.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    7648.03 ms /   434 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 322 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.62 ms /   322 tokens (    1.96 ms per token,   510.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6396.25 ms /    84 runs   (   76.15 ms per token,    13.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    7102.52 ms /   406 tokens\n",
            "llama_perf_context_print:    graphs reused =         81\n",
            "Llama.generate: 1 prefix-match hit, remaining 284 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     568.31 ms /   284 tokens (    2.00 ms per token,   499.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5840.65 ms /    76 runs   (   76.85 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    6476.05 ms /   360 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.82 ms /   298 tokens (    2.11 ms per token,   473.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6444.17 ms /    83 runs   (   77.64 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    7147.41 ms /   381 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.59 ms /   302 tokens (    2.08 ms per token,   479.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5807.90 ms /    74 runs   (   78.49 ms per token,    12.74 tokens per second)\n",
            "llama_perf_context_print:       total time =    6501.60 ms /   376 tokens\n",
            "llama_perf_context_print:    graphs reused =         71\n",
            "Llama.generate: 1 prefix-match hit, remaining 297 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.24 ms /   297 tokens (    2.14 ms per token,   467.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5423.14 ms /    69 runs   (   78.60 ms per token,    12.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    6118.65 ms /   366 tokens\n",
            "llama_perf_context_print:    graphs reused =         66\n",
            "Llama.generate: 1 prefix-match hit, remaining 321 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     640.58 ms /   321 tokens (    2.00 ms per token,   501.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6396.04 ms /    81 runs   (   78.96 ms per token,    12.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    7106.58 ms /   402 tokens\n",
            "llama_perf_context_print:    graphs reused =         78\n",
            "Llama.generate: 1 prefix-match hit, remaining 368 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     712.06 ms /   368 tokens (    1.93 ms per token,   516.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6150.71 ms /    78 runs   (   78.86 ms per token,    12.68 tokens per second)\n",
            "llama_perf_context_print:       total time =    6931.30 ms /   446 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            "Llama.generate: 1 prefix-match hit, remaining 281 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     578.40 ms /   281 tokens (    2.06 ms per token,   485.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5918.80 ms /    76 runs   (   77.88 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    6563.38 ms /   357 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.02 ms /   291 tokens (    2.16 ms per token,   463.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7230.32 ms /    93 runs   (   77.75 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    7940.48 ms /   384 tokens\n",
            "llama_perf_context_print:    graphs reused =         89\n",
            "Llama.generate: 1 prefix-match hit, remaining 322 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     638.24 ms /   322 tokens (    1.98 ms per token,   504.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7980.09 ms /   103 runs   (   77.48 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    8710.56 ms /   425 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.74 ms /   306 tokens (    2.06 ms per token,   484.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4697.97 ms /    61 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    5381.11 ms /   367 tokens\n",
            "llama_perf_context_print:    graphs reused =         58\n",
            "Llama.generate: 1 prefix-match hit, remaining 301 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.83 ms /   301 tokens (    2.09 ms per token,   478.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5916.18 ms /    77 runs   (   76.83 ms per token,    13.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    6613.06 ms /   378 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 305 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.40 ms /   305 tokens (    2.07 ms per token,   482.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7689.70 ms /   100 runs   (   76.90 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    8415.26 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =         96\n",
            "Llama.generate: 1 prefix-match hit, remaining 299 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     627.76 ms /   299 tokens (    2.10 ms per token,   476.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6715.48 ms /    87 runs   (   77.19 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    7419.20 ms /   386 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 313 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.11 ms /   313 tokens (    2.03 ms per token,   492.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6495.71 ms /    84 runs   (   77.33 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    7205.35 ms /   397 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 329 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     642.26 ms /   329 tokens (    1.95 ms per token,   512.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6359.44 ms /    82 runs   (   77.55 ms per token,    12.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    7075.15 ms /   411 tokens\n",
            "llama_perf_context_print:    graphs reused =         79\n",
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.51 ms /   298 tokens (    2.11 ms per token,   474.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6833.41 ms /    88 runs   (   77.65 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    7538.48 ms /   386 tokens\n",
            "llama_perf_context_print:    graphs reused =         84\n",
            "Llama.generate: 1 prefix-match hit, remaining 307 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.24 ms /   307 tokens (    2.06 ms per token,   486.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6976.28 ms /    90 runs   (   77.51 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    7688.64 ms /   397 tokens\n",
            "llama_perf_context_print:    graphs reused =         86\n",
            "Llama.generate: 1 prefix-match hit, remaining 301 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.35 ms /   301 tokens (    2.09 ms per token,   478.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8005.29 ms /   103 runs   (   77.72 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    8727.00 ms /   404 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 289 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     626.92 ms /   289 tokens (    2.17 ms per token,   460.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7376.25 ms /    95 runs   (   77.64 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    8087.41 ms /   384 tokens\n",
            "llama_perf_context_print:    graphs reused =         91\n",
            "Llama.generate: 1 prefix-match hit, remaining 342 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     702.14 ms /   342 tokens (    2.05 ms per token,   487.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4760.87 ms /    61 runs   (   78.05 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    5515.10 ms /   403 tokens\n",
            "llama_perf_context_print:    graphs reused =         58\n",
            "Llama.generate: 1 prefix-match hit, remaining 299 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.41 ms /   299 tokens (    2.12 ms per token,   472.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8335.15 ms /   107 runs   (   77.90 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    9065.02 ms /   406 tokens\n",
            "llama_perf_context_print:    graphs reused =        103\n",
            "Llama.generate: 1 prefix-match hit, remaining 305 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.38 ms /   305 tokens (    2.07 ms per token,   482.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6708.63 ms /    86 runs   (   78.01 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    7415.94 ms /   391 tokens\n",
            "llama_perf_context_print:    graphs reused =         82\n",
            "Llama.generate: 1 prefix-match hit, remaining 305 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.49 ms /   305 tokens (    2.08 ms per token,   481.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5850.97 ms /    75 runs   (   78.01 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    6550.44 ms /   380 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 293 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.48 ms /   293 tokens (    2.15 ms per token,   465.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5558.88 ms /    71 runs   (   78.29 ms per token,    12.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    6248.73 ms /   364 tokens\n",
            "llama_perf_context_print:    graphs reused =         68\n",
            "Llama.generate: 1 prefix-match hit, remaining 316 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.11 ms /   316 tokens (    2.00 ms per token,   499.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5633.09 ms /    72 runs   (   78.24 ms per token,    12.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    6329.58 ms /   388 tokens\n",
            "llama_perf_context_print:    graphs reused =         68\n",
            "Llama.generate: 1 prefix-match hit, remaining 303 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.12 ms /   303 tokens (    2.08 ms per token,   480.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6729.11 ms /    86 runs   (   78.25 ms per token,    12.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    7436.75 ms /   389 tokens\n",
            "llama_perf_context_print:    graphs reused =         82\n",
            "Llama.generate: 1 prefix-match hit, remaining 312 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     636.39 ms /   312 tokens (    2.04 ms per token,   490.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6785.70 ms /    87 runs   (   78.00 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    7497.55 ms /   399 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 289 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.46 ms /   289 tokens (    2.18 ms per token,   457.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6063.92 ms /    78 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    6762.56 ms /   367 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.75 ms /   298 tokens (    2.12 ms per token,   472.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6690.36 ms /    86 runs   (   77.79 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    7395.22 ms /   384 tokens\n",
            "llama_perf_context_print:    graphs reused =         82\n",
            "Llama.generate: 1 prefix-match hit, remaining 299 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.43 ms /   299 tokens (    2.12 ms per token,   472.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5577.32 ms /    72 runs   (   77.46 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    6272.14 ms /   371 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            "Llama.generate: 1 prefix-match hit, remaining 299 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.69 ms /   299 tokens (    2.10 ms per token,   475.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7455.08 ms /    96 runs   (   77.66 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    8168.73 ms /   395 tokens\n",
            "llama_perf_context_print:    graphs reused =         92\n",
            "Llama.generate: 1 prefix-match hit, remaining 304 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     627.91 ms /   304 tokens (    2.07 ms per token,   484.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7363.26 ms /    95 runs   (   77.51 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    8074.45 ms /   399 tokens\n",
            "llama_perf_context_print:    graphs reused =         91\n",
            "Llama.generate: 1 prefix-match hit, remaining 315 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.60 ms /   315 tokens (    2.01 ms per token,   497.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6278.66 ms /    81 runs   (   77.51 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    6981.80 ms /   396 tokens\n",
            "llama_perf_context_print:    graphs reused =         77\n",
            "Llama.generate: 1 prefix-match hit, remaining 325 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.91 ms /   325 tokens (    1.96 ms per token,   511.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8923.52 ms /   115 runs   (   77.60 ms per token,    12.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    9663.08 ms /   440 tokens\n",
            "llama_perf_context_print:    graphs reused =        111\n",
            "Llama.generate: 1 prefix-match hit, remaining 331 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     639.86 ms /   331 tokens (    1.93 ms per token,   517.30 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8457.47 ms /   109 runs   (   77.59 ms per token,    12.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    9198.90 ms /   440 tokens\n",
            "llama_perf_context_print:    graphs reused =        105\n",
            "Llama.generate: 1 prefix-match hit, remaining 311 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.04 ms /   311 tokens (    2.03 ms per token,   493.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6475.37 ms /    84 runs   (   77.09 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    7180.15 ms /   395 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.14 ms /   302 tokens (    2.08 ms per token,   480.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5381.38 ms /    70 runs   (   76.88 ms per token,    13.01 tokens per second)\n",
            "llama_perf_context_print:       total time =    6070.32 ms /   372 tokens\n",
            "llama_perf_context_print:    graphs reused =         67\n",
            "Llama.generate: 1 prefix-match hit, remaining 325 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.00 ms /   325 tokens (    1.95 ms per token,   511.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6260.33 ms /    81 runs   (   77.29 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    6964.71 ms /   406 tokens\n",
            "llama_perf_context_print:    graphs reused =         78\n",
            "Llama.generate: 1 prefix-match hit, remaining 344 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     701.38 ms /   344 tokens (    2.04 ms per token,   490.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10211.20 ms /   132 runs   (   77.36 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =   11036.80 ms /   476 tokens\n",
            "llama_perf_context_print:    graphs reused =        127\n",
            "Llama.generate: 1 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.38 ms /   302 tokens (    2.08 ms per token,   480.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5545.84 ms /    72 runs   (   77.03 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    6236.89 ms /   374 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            "Llama.generate: 1 prefix-match hit, remaining 330 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     636.39 ms /   330 tokens (    1.93 ms per token,   518.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7004.56 ms /    91 runs   (   76.97 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    7722.41 ms /   421 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 335 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     642.82 ms /   335 tokens (    1.92 ms per token,   521.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8327.77 ms /   108 runs   (   77.11 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    9069.34 ms /   443 tokens\n",
            "llama_perf_context_print:    graphs reused =        104\n",
            "Llama.generate: 1 prefix-match hit, remaining 282 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     571.34 ms /   282 tokens (    2.03 ms per token,   493.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6843.65 ms /    89 runs   (   76.89 ms per token,    13.00 tokens per second)\n",
            "llama_perf_context_print:       total time =    7496.07 ms /   371 tokens\n",
            "llama_perf_context_print:    graphs reused =         85\n",
            "Llama.generate: 1 prefix-match hit, remaining 318 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     634.34 ms /   318 tokens (    1.99 ms per token,   501.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6744.65 ms /    87 runs   (   77.52 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    7456.49 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =         83\n",
            "Llama.generate: 1 prefix-match hit, remaining 311 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.26 ms /   311 tokens (    2.04 ms per token,   489.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6525.89 ms /    84 runs   (   77.69 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    7236.31 ms /   395 tokens\n",
            "llama_perf_context_print:    graphs reused =         80\n",
            "Llama.generate: 1 prefix-match hit, remaining 334 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     644.44 ms /   334 tokens (    1.93 ms per token,   518.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8037.33 ms /   103 runs   (   78.03 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    8775.95 ms /   437 tokens\n",
            "llama_perf_context_print:    graphs reused =         99\n",
            "Llama.generate: 1 prefix-match hit, remaining 293 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     627.75 ms /   293 tokens (    2.14 ms per token,   466.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7644.84 ms /    98 runs   (   78.01 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    8359.99 ms /   391 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 315 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     639.37 ms /   315 tokens (    2.03 ms per token,   492.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6940.79 ms /    89 runs   (   77.99 ms per token,    12.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    7657.50 ms /   404 tokens\n",
            "llama_perf_context_print:    graphs reused =         85\n",
            "Llama.generate: 1 prefix-match hit, remaining 309 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     634.83 ms /   309 tokens (    2.05 ms per token,   486.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7658.23 ms /    98 runs   (   78.15 ms per token,    12.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    8381.60 ms /   407 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 294 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.37 ms /   294 tokens (    2.15 ms per token,   465.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4987.41 ms /    64 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    5672.96 ms /   358 tokens\n",
            "llama_perf_context_print:    graphs reused =         61\n",
            "Llama.generate: 1 prefix-match hit, remaining 312 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     636.66 ms /   312 tokens (    2.04 ms per token,   490.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8602.12 ms /   110 runs   (   78.20 ms per token,    12.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    9337.26 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =        105\n",
            "Llama.generate: 1 prefix-match hit, remaining 333 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     646.09 ms /   333 tokens (    1.94 ms per token,   515.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7652.06 ms /    98 runs   (   78.08 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    8387.10 ms /   431 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 289 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.87 ms /   289 tokens (    2.18 ms per token,   458.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6556.55 ms /    84 runs   (   78.05 ms per token,    12.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    7260.43 ms /   373 tokens\n",
            "llama_perf_context_print:    graphs reused =         81\n",
            "Llama.generate: 1 prefix-match hit, remaining 316 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     639.19 ms /   316 tokens (    2.02 ms per token,   494.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5670.47 ms /    73 runs   (   77.68 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    6372.01 ms /   389 tokens\n",
            "llama_perf_context_print:    graphs reused =         69\n",
            "Llama.generate: 1 prefix-match hit, remaining 297 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.88 ms /   297 tokens (    2.13 ms per token,   470.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5418.78 ms /    70 runs   (   77.41 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    6112.55 ms /   367 tokens\n",
            "llama_perf_context_print:    graphs reused =         67\n",
            "Llama.generate: 1 prefix-match hit, remaining 331 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     642.56 ms /   331 tokens (    1.94 ms per token,   515.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7080.55 ms /    91 runs   (   77.81 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    7802.00 ms /   422 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 294 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.72 ms /   294 tokens (    2.14 ms per token,   466.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7044.93 ms /    91 runs   (   77.42 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    7755.71 ms /   385 tokens\n",
            "llama_perf_context_print:    graphs reused =         87\n",
            "Llama.generate: 1 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     626.07 ms /   291 tokens (    2.15 ms per token,   464.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7035.50 ms /    91 runs   (   77.31 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    7742.81 ms /   382 tokens\n",
            "llama_perf_context_print:    graphs reused =         88\n",
            "Llama.generate: 1 prefix-match hit, remaining 294 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     627.53 ms /   294 tokens (    2.13 ms per token,   468.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6100.66 ms /    79 runs   (   77.22 ms per token,    12.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    6794.58 ms /   373 tokens\n",
            "llama_perf_context_print:    graphs reused =         76\n",
            "Llama.generate: 1 prefix-match hit, remaining 322 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     637.66 ms /   322 tokens (    1.98 ms per token,   504.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8439.38 ms /   109 runs   (   77.43 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    9175.96 ms /   431 tokens\n",
            "llama_perf_context_print:    graphs reused =        105\n",
            "Llama.generate: 1 prefix-match hit, remaining 300 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.32 ms /   300 tokens (    2.10 ms per token,   475.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7796.50 ms /   101 runs   (   77.19 ms per token,    12.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    8519.61 ms /   401 tokens\n",
            "llama_perf_context_print:    graphs reused =         97\n",
            "Llama.generate: 1 prefix-match hit, remaining 391 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     838.11 ms /   391 tokens (    2.14 ms per token,   466.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6065.26 ms /    78 runs   (   77.76 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    6971.43 ms /   469 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            "Llama.generate: 1 prefix-match hit, remaining 298 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     627.91 ms /   298 tokens (    2.11 ms per token,   474.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7665.98 ms /    99 runs   (   77.43 ms per token,    12.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    8381.28 ms /   397 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            "Llama.generate: 1 prefix-match hit, remaining 295 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     625.86 ms /   295 tokens (    2.12 ms per token,   471.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5795.68 ms /    75 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    6487.36 ms /   370 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 316 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.46 ms /   316 tokens (    2.00 ms per token,   498.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6185.07 ms /    80 runs   (   77.31 ms per token,    12.93 tokens per second)\n",
            "llama_perf_context_print:       total time =    6887.80 ms /   396 tokens\n",
            "llama_perf_context_print:    graphs reused =         76\n",
            "Llama.generate: 1 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     634.38 ms /   306 tokens (    2.07 ms per token,   482.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5166.51 ms /    67 runs   (   77.11 ms per token,    12.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    5860.10 ms /   373 tokens\n",
            "llama_perf_context_print:    graphs reused =         64\n",
            "Llama.generate: 1 prefix-match hit, remaining 323 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     636.89 ms /   323 tokens (    1.97 ms per token,   507.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8035.31 ms /   104 runs   (   77.26 ms per token,    12.94 tokens per second)\n",
            "llama_perf_context_print:       total time =    8767.51 ms /   427 tokens\n",
            "llama_perf_context_print:    graphs reused =        100\n",
            "Llama.generate: 1 prefix-match hit, remaining 296 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     626.83 ms /   296 tokens (    2.12 ms per token,   472.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8407.79 ms /   109 runs   (   77.14 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    9131.53 ms /   405 tokens\n",
            "llama_perf_context_print:    graphs reused =        105\n",
            "Llama.generate: 1 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     623.25 ms /   291 tokens (    2.14 ms per token,   466.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6467.24 ms /    84 runs   (   76.99 ms per token,    12.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    7165.85 ms /   375 tokens\n",
            "llama_perf_context_print:    graphs reused =         81\n",
            "Llama.generate: 1 prefix-match hit, remaining 312 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.34 ms /   312 tokens (    2.02 ms per token,   494.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5470.20 ms /    71 runs   (   77.05 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    6163.11 ms /   383 tokens\n",
            "llama_perf_context_print:    graphs reused =         68\n",
            "Llama.generate: 1 prefix-match hit, remaining 308 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     629.39 ms /   308 tokens (    2.04 ms per token,   489.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5932.86 ms /    77 runs   (   77.05 ms per token,    12.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    6627.75 ms /   385 tokens\n",
            "llama_perf_context_print:    graphs reused =         73\n",
            "Llama.generate: 1 prefix-match hit, remaining 329 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     641.54 ms /   329 tokens (    1.95 ms per token,   512.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8591.65 ms /   111 runs   (   77.40 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    9333.59 ms /   440 tokens\n",
            "llama_perf_context_print:    graphs reused =        107\n",
            "Llama.generate: 1 prefix-match hit, remaining 308 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.73 ms /   308 tokens (    2.05 ms per token,   487.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7564.13 ms /    98 runs   (   77.19 ms per token,    12.96 tokens per second)\n",
            "llama_perf_context_print:       total time =    8285.26 ms /   406 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 335 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     644.75 ms /   335 tokens (    1.92 ms per token,   519.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8935.88 ms /   115 runs   (   77.70 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    9686.54 ms /   450 tokens\n",
            "llama_perf_context_print:    graphs reused =        110\n",
            "Llama.generate: 1 prefix-match hit, remaining 279 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     573.75 ms /   279 tokens (    2.06 ms per token,   486.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7584.73 ms /    98 runs   (   77.40 ms per token,    12.92 tokens per second)\n",
            "llama_perf_context_print:       total time =    8245.32 ms /   377 tokens\n",
            "llama_perf_context_print:    graphs reused =         94\n",
            "Llama.generate: 1 prefix-match hit, remaining 312 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.23 ms /   312 tokens (    2.03 ms per token,   492.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7517.56 ms /    97 runs   (   77.50 ms per token,    12.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    8237.64 ms /   409 tokens\n",
            "llama_perf_context_print:    graphs reused =         93\n",
            "Llama.generate: 1 prefix-match hit, remaining 329 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     642.47 ms /   329 tokens (    1.95 ms per token,   512.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4906.51 ms /    63 runs   (   77.88 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    5600.77 ms /   392 tokens\n",
            "llama_perf_context_print:    graphs reused =         60\n",
            "Llama.generate: 1 prefix-match hit, remaining 302 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.63 ms /   302 tokens (    2.09 ms per token,   478.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7531.92 ms /    97 runs   (   77.65 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    8251.08 ms /   399 tokens\n",
            "llama_perf_context_print:    graphs reused =         93\n",
            "Llama.generate: 1 prefix-match hit, remaining 327 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     640.45 ms /   327 tokens (    1.96 ms per token,   510.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7232.28 ms /    93 runs   (   77.77 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    7955.75 ms /   420 tokens\n",
            "llama_perf_context_print:    graphs reused =         89\n",
            "Llama.generate: 1 prefix-match hit, remaining 332 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     642.80 ms /   332 tokens (    1.94 ms per token,   516.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6625.75 ms /    85 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
            "llama_perf_context_print:       total time =    7344.13 ms /   417 tokens\n",
            "llama_perf_context_print:    graphs reused =         81\n",
            "Llama.generate: 1 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     635.00 ms /   306 tokens (    2.08 ms per token,   481.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6057.61 ms /    78 runs   (   77.66 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    6759.80 ms /   384 tokens\n",
            "llama_perf_context_print:    graphs reused =         74\n",
            "Llama.generate: 1 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     630.31 ms /   291 tokens (    2.17 ms per token,   461.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7695.49 ms /    99 runs   (   77.73 ms per token,    12.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    8416.22 ms /   390 tokens\n",
            "llama_perf_context_print:    graphs reused =         95\n",
            "Llama.generate: 1 prefix-match hit, remaining 309 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.97 ms /   309 tokens (    2.05 ms per token,   488.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5903.68 ms /    76 runs   (   77.68 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    6601.69 ms /   385 tokens\n",
            "llama_perf_context_print:    graphs reused =         72\n",
            "Llama.generate: 1 prefix-match hit, remaining 290 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.05 ms /   290 tokens (    2.17 ms per token,   461.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7392.19 ms /    95 runs   (   77.81 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    8104.97 ms /   385 tokens\n",
            "llama_perf_context_print:    graphs reused =         91\n",
            "Llama.generate: 1 prefix-match hit, remaining 305 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.07 ms /   305 tokens (    2.08 ms per token,   481.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5906.37 ms /    76 runs   (   77.72 ms per token,    12.87 tokens per second)\n",
            "llama_perf_context_print:       total time =    6606.36 ms /   381 tokens\n",
            "llama_perf_context_print:    graphs reused =         73\n",
            "Llama.generate: 1 prefix-match hit, remaining 296 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     628.18 ms /   296 tokens (    2.12 ms per token,   471.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7237.73 ms /    93 runs   (   77.83 ms per token,    12.85 tokens per second)\n",
            "llama_perf_context_print:       total time =    7946.99 ms /   389 tokens\n",
            "llama_perf_context_print:    graphs reused =         89\n",
            "Llama.generate: 1 prefix-match hit, remaining 304 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     633.24 ms /   304 tokens (    2.08 ms per token,   480.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6076.81 ms /    78 runs   (   77.91 ms per token,    12.84 tokens per second)\n",
            "llama_perf_context_print:       total time =    6778.57 ms /   382 tokens\n",
            "llama_perf_context_print:    graphs reused =         75\n",
            "Llama.generate: 1 prefix-match hit, remaining 306 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     631.60 ms /   306 tokens (    2.06 ms per token,   484.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6830.76 ms /    88 runs   (   77.62 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    7540.61 ms /   394 tokens\n",
            "llama_perf_context_print:    graphs reused =         84\n",
            "Llama.generate: 1 prefix-match hit, remaining 309 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     632.14 ms /   309 tokens (    2.05 ms per token,   488.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    5435.07 ms /    70 runs   (   77.64 ms per token,    12.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    6125.76 ms /   379 tokens\n",
            "llama_perf_context_print:    graphs reused =         67\n",
            "Llama.generate: 1 prefix-match hit, remaining 289 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     669.03 ms\n",
            "llama_perf_context_print: prompt eval time =     622.98 ms /   289 tokens (    2.16 ms per token,   463.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    6591.99 ms /    85 runs   (   77.55 ms per token,    12.89 tokens per second)\n",
            "llama_perf_context_print:       total time =    7290.19 ms /   374 tokens\n",
            "llama_perf_context_print:    graphs reused =         82\n"
          ]
        }
      ],
      "source": [
        "data_4['model_response_4'] = data_4['Article'].apply(lambda x: generate_llama_response(instruction_4, x).replace('\\n', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwsx9q0kWOHv",
        "outputId": "621979db-c00c-4ec1-e1d8-0ce359f62e71"
      },
      "outputs": [],
      "source": [
        "data_4[\"model_response_4_parsed\"] = data_4[\"model_response_4\"].apply(extract_json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV5BEHwnYdZ9",
        "outputId": "39104533-1da0-4512-bafe-4696e28bbfc8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model_response_4_parsed",
                  "rawType": "object",
                  "type": "unknown"
                }
              ],
              "ref": "057e0dd0-7c86-4b95-846c-49505c85e8ae",
              "rows": [
                [
                  "0",
                  "{'Label': 'Business', 'Headline': \"New York's Divorce Law Under Scrutiny: A Push for Change\", 'Summary': \"Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses.\"}"
                ],
                [
                  "1",
                  "{'Label': 'Business', 'Headline': 'Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts', 'Summary': 'The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project.'}"
                ],
                [
                  "2",
                  "{'Label': 'World', 'Headline': 'New Chechen Leader Vows Peace, Poll Criticized', 'Summary': \"Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow.\"}"
                ],
                [
                  "3",
                  "{'Label': 'World', 'Headline': 'Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon', 'Summary': 'Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives.'}"
                ],
                [
                  "4",
                  "{'Label': 'World', 'Headline': 'U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible', 'Summary': 'The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects.'}"
                ],
                [
                  "5",
                  "{'Label': 'World', 'Headline': 'Violence Continues in Iraq as Elections Approach, PM Vows to Proceed', 'Summary': 'Iraqi Prime Minister vows to proceed with January elections despite ongoing violence and grisly militant attacks. The article highlights the challenges facing Iraq amidst an increasingly bold insurgency.'}"
                ],
                [
                  "6",
                  "{'Label': 'World', 'Headline': 'Indonesian Cleric Abu Bakar Bashir on Trial for Terrorism', 'Summary': \"Abu Bakar Bashir, an Indonesian Islamic cleric, has gone on trial accused of leading an al Qaeda-linked militant network and planning or inciting attacks in the world's most populous Muslim nation.\"}"
                ],
                [
                  "7",
                  "{'Label': 'World', 'Headline': 'US and China agree to phase one trade deal, but tensions remain', 'Summary': 'The US and China have reached a tentative agreement on the first phase of a trade deal, but significant challenges still lie ahead.'}"
                ],
                [
                  "8",
                  "{'Label': 'World', 'Headline': 'Helicopters Rescue Injured Refugees in Aceh, Orphan Trafficking Reports Surface', 'Summary': 'U.S. helicopters evacuate injured refugees from tsunami-hit Aceh, while reports of orphan trafficking emerge.'}"
                ],
                [
                  "9",
                  "{'Label': 'World', 'Headline': 'Northern Ireland Peace Talks End Without Breakthrough', 'Summary': 'Talks aimed at reviving home rule in Northern Ireland and disbanding IRA guerrillas ended without a final deal, according to British and Irish Prime Ministers Tony Blair and Bertie Ahern.'}"
                ],
                [
                  "10",
                  "{'Label': 'World', 'Headline': 'Security Measures Fail to Spot Protesters at Labour Conference', 'Summary': \"A multi-million pound security operation failed to prevent dramatic protests during Tony Blair's Labour conference speech, raising questions about the effectiveness of the measures.\"}"
                ],
                [
                  "11",
                  "{'Label': 'World', 'Headline': 'Galapagos Park Rangers Go on Strike, Protest Firing of Director and Replacement Plans', 'Summary': 'Some 300 Galapagos Islands rangers peacefully occupied park research stations to protest the reported firing of the park director and his probable replacement.'}"
                ],
                [
                  "12",
                  "{'Label': 'World', 'Headline': 'US troops targeted in Iraq attacks', 'Summary': 'A car bomb exploded near a US military patrol in Baiji, killing four civilians and wounding two soldiers. The attack highlights the ongoing security challenges facing US forces in Iraq.'}"
                ],
                [
                  "13",
                  "{'Label': 'World', 'Headline': 'Afghan Election Underway, Karzai Expected to Win', 'Summary': \"Early results show interim Afghan president Hamid Karzai leading in the country's first free presidential election.\"}"
                ],
                [
                  "14",
                  "{'Label': 'World', 'Headline': 'Ukraine Seeks to Control Yushchenko Probe', 'Summary': \"Ukraine's outgoing government seeks control of the inquiry into Viktor Yushchenko's poisoning, with officials close to the government taking charge of both investigations.\"}"
                ],
                [
                  "15",
                  "{'Label': 'Business', 'Headline': 'L.A., Washington Hotel Workers Strike: Union Authorizes Walkout in Three Cities', 'Summary': 'Thousands of hotel workers in Los Angeles voted to authorize a strike, joining an effort that could lead to walkouts in three cities.'}"
                ],
                [
                  "16",
                  "{'Label': 'World', 'Headline': 'Nigeria Lifts Six-Month State of Emergency in Plateau State', 'Summary': 'Nigeria has lifted the six-month state of emergency in Plateau State, central highland region, after a significant improvement in security.'}"
                ],
                [
                  "17",
                  "{'Label': 'Politics', 'Headline': 'Voters Begin Casting Early Ballots in Florida, Reviving Memories of 2000 Election Controversy', 'Summary': 'The article discusses the start of early voting in Florida, with a focus on the potential for controversy and the importance of the state in the upcoming presidential election.'}"
                ],
                [
                  "18",
                  "{'Label': 'World', 'Headline': 'Tamil Tigers Make Unexpected Offer to Revive Peace Talks in Sri Lanka', 'Summary': \"The Tamil Tigers, a rebel group in Sri Lanka, have sent an unexpected message to the country's president through Norwegian peace brokers, offering ways to revive stalled peace talks aimed at ending a two-decade civil war.\"}"
                ],
                [
                  "19",
                  "{'Label': 'World', 'Headline': 'Israeli Lions Symbols of Peace at Palestinian Zoo', 'Summary': 'A convoy of Israelis visited a Palestinian zoo, bringing symbols of peace and joy to the area.'}"
                ],
                [
                  "20",
                  "{'Label': 'World', 'Headline': \"North Korea's Mysterious Blast: Britain's Ambassador Investigates\", 'Summary': \"Britain's ambassador to North Korea travelled to the site of a massive explosion to verify claims that it was not a nuclear test.\"}"
                ],
                [
                  "21",
                  "{'Label': 'World', 'Headline': 'Iraq Voter Registration Begins Amidst Violence and Uncertainty', 'Summary': \"The article reports on the commencement of voter registration in Iraq, despite recent violence that claimed the life of Baghdad's deputy governor. The article highlights the challenges facing the electoral process and the significance of these elections for the future of Iraq.\"}"
                ],
                [
                  "22",
                  "{'Label': 'World', 'Headline': \"Rubbish Art Sparks Controversy at London's Tate Gallery\", 'Summary': 'A bag of rubbish thrown away at the Tate gallery in London sparked controversy, with some assuming it was a piece of art and others calling it an insult to the institution.'}"
                ],
                [
                  "23",
                  "{'Label': 'World', 'Headline': \"Insurgent Attack on Iraq's National Assembly During First Session\", 'Summary': \"Mortars and rockets were fired at the venue of Iraq's interim national assembly during its first session, highlighting the country's risky road to elections in January.\"}"
                ],
                [
                  "24",
                  "{'Label': 'World', 'Headline': 'Hurricane Ivan Heads for Jamaica, Kingston in Its Path', 'Summary': \"Jamaican freelance journalist Knolly Moses discusses Hurricane Ivan's expected impact on the island nation, with the capital city of Kingston directly in its path.\"}"
                ],
                [
                  "25",
                  "{'Label': 'Sports', 'Headline': \"LaDainian Tomlinson's Groin Injury: A Sight for Sore Eyes?\", 'Summary': \"An analysis of the article about LaDainian Tomlinson's groin injury and how the sight of the Raiders across the line of scrimmage might be the cure.\"}"
                ],
                [
                  "26",
                  "{'Label': 'Business', 'Headline': 'Bank Mulls Rival Proposal for Manchester United Takeover', 'Summary': 'Seymour Pierce, a British boutique investment bank, is considering a financial restructuring of Manchester United as a possible rival proposal to a planned takeover by US sports tycoon Malcolm Glazer.'}"
                ],
                [
                  "27",
                  "{'Label': 'Sports', 'Headline': 'Vijay Singh Wins Deutsche Bank Championship, Toppling Tiger Woods from the Top', 'Summary': \"In a stunning upset, Vijay Singh of Fiji won the Deutsche Bank Championship, dethroning Tiger Woods from the top spot. This marks Singh's first victory in over 8 years and a significant turnaround in his career.\"}"
                ],
                [
                  "28",
                  "{'Label': 'Sports', 'Headline': \"Chiefs Crush Rams 24-7: A Dominant Performance by Kansas City's High-Octane Offense\", 'Summary': 'The Kansas City Chiefs asserted their dominance in the NFL with a convincing 24-7 victory over the St. Louis Rams, showcasing their potent offense and solid defense.'}"
                ],
                [
                  "29",
                  "{'Label': 'Sports', 'Headline': 'Big Boi Concert Hopes to Keep Football Team in Division', 'Summary': 'Middle Tennessee State football team hopes a Big Boi concert will help keep them in their division.'}"
                ],
                [
                  "30",
                  "{'Label': 'Sports', 'Headline': \"Georgetown Prep's Football Team Banned from IAC Due to Excessive Dominance\", 'Summary': \"Georgetown Preparatory School's football team has been banned from the Interstate Athletic Conference due to their overwhelming dominance on the field, marking a controversial decision that raises questions about the balance of power in high school sports.\"}"
                ],
                [
                  "31",
                  "{'Label': 'Sports', 'Headline': \"Clijsters' Wrist Injury Raises Concerns About Career\", 'Summary': \"Former No. 1 Kim Clijsters' latest left wrist problem is a new injury that may end her season but probably won't require surgery, according to a tournament doctor.\"}"
                ],
                [
                  "32",
                  "{'Label': 'Sports', 'Headline': 'Underdog Danny Williams Looking to Upset the Heavyweight Champion', 'Summary': \"Danny Williams, a sizable underdog in Saturday night's heavyweight title fight, is taking a major step up in class but looking to upset the champion.\"}"
                ],
                [
                  "33",
                  "{'Label': 'Sports', 'Headline': \"Gerrard: We'll Get It Right, Claims Gerrard\", 'Summary': 'Steven Gerrard is confident that Liverpool will get it right in the second leg of their Champions League tie against Real Madrid.'}"
                ],
                [
                  "34",
                  "{'Label': 'Sports', 'Headline': 'Angry Ferrero Has Not Given Up Hope', 'Summary': 'Juan Carlos Ferrero, a Spanish tennis player, is disappointed at being used as a bit-part doubles player in the Davis Cup final against the United States.'}"
                ],
                [
                  "35",
                  "{'Label': 'Sports', 'Headline': 'Yao Ming and the Houston Rockets Make History in Shanghai', 'Summary': \"The Houston Rockets, led by hometown hero Yao Ming, have arrived in Shanghai for the NBA's first games in China. The team is on a 'business trip' to promote basketball and cultural exchange.\"}"
                ],
                [
                  "36",
                  "{'Label': 'Sports', 'Headline': 'Globetrotters Challenge Argentina to a $1 Million Game', 'Summary': 'The Harlem Globetrotters have challenged Argentina, the gold medalist in basketball at the Olympics, to a winner-take-all game for $1 million. The game would be shown on pay-per-view TV.'}"
                ],
                [
                  "37",
                  "{'Label': 'Sports', 'Headline': \"Auburn's Offensive Balance Key to Success\", 'Summary': \"An analysis of a news article about Auburn's football team, highlighting the importance of offensive balance and the potential for the Tigers to be a balanced team.\"}"
                ],
                [
                  "38",
                  "{'Label': 'Sports', 'Headline': \"Dwyane Wade's New Role as a Back-Up Driver for Shaquille O'Neal\", 'Summary': \"Despite being a high-flying player, Dwyane Wade is comfortable playing in Shaquille O'Neal's shadow this season.\"}"
                ],
                [
                  "39",
                  "{'Label': 'Sports', 'Headline': 'Pacers image hurting after brawl, charges', 'Summary': \"The Indiana Pacers' image has been damaged following a brawl with Detroit fans, leading to questions about the team's future and reputation.\"}"
                ],
                [
                  "40",
                  "{'Label': 'Sports', 'Headline': 'Rangers Pitcher Suspended for Rest of Season After Arrest', 'Summary': \"Frank Francisco, a pitcher for the Texas Rangers, was arrested and suspended for the rest of the season after throwing a chair at Oakland Athletics fans. The incident has raised concerns about player behavior and the impact on the team's performance.\"}"
                ],
                [
                  "41",
                  "{'Label': 'Sports', 'Headline': \"John Daly's Wife Pleads Guilty to Federal Money Laundering Charge\", 'Summary': \"John Daly's wife, Sherrie Miller Daly, pleaded guilty to a federal money laundering charge and will await sentencing. The golf pro #39;s personal life continues to make headlines.\"}"
                ],
                [
                  "42",
                  "{'Label': 'Sports', 'Headline': 'Coleman Insists Fulham Must quot;Wake Up and Smell the Coffeequot;', 'Summary': 'Fulham manager Chris Coleman urges his team to improve their dismal start to the season during the upcoming World Cup qualifying break.'}"
                ],
                [
                  "43",
                  "{'Label': 'Sports', 'Headline': 'Hewitt Wins Washington Open, Fine-Tunes Game for US Open', 'Summary': \"Lleyton Hewitt wins the \\\\$US500,000 (\\\\$A692,377) Washington Open, defeating Gilles Muller in a convincing 6-3 6-4 victory. The win fine-tunes Hewitt's game ahead of the US Open.\"}"
                ],
                [
                  "44",
                  "{'Label': 'Sports', 'Headline': \"Beltran's Record-Tying Home Run Leads Astros to Comeback Win\", 'Summary': \"Carlos Beltran's record-tying eighth postseason home run helped the Houston Astros come back from a 5-0 deficit to defeat the St. Louis Cardinals 6-5, tying the National League Championship Series at two games apiece.\"}"
                ],
                [
                  "45",
                  "{'Label': 'Sports', 'Headline': 'Mark Kreidler: An ocean away, an experience to savor', 'Summary': 'Beijing - The world probably gets smaller only a little bit at a time. When Peja Stojakovic watched Yao Ming moving gracefully through a sea of people here late Sunday'}"
                ],
                [
                  "46",
                  "{'Label': 'Sports', 'Headline': \"Belichick Highlights Dolphins' Strong Defense Despite Woeful Season\", 'Summary': \"Bill Belichick praises Miami Dolphins' defense despite their poor offensive performance this season.\"}"
                ],
                [
                  "47",
                  "{'Label': 'World', 'Headline': \"Greece's Security Preparations for Olympics Successful, Says Defense Minister\", 'Summary': \"Greek Defense Minister says that the country's security preparations for the Athens Olympics have been successful and can advise China on how to avoid terrorist attacks.\"}"
                ],
                [
                  "48",
                  "{'Label': 'Sports', 'Headline': 'Cavs Outlast Raptors in High-Scoring Thriller, LeBron James Shines with 27 Points', 'Summary': 'In a thrilling matchup between the Cleveland Cavaliers and Toronto Raptors, the Cavs emerged victorious with a score of 105-97. LeBron James led the way with 27 points, while Drew Gooden and Zydrunas Ilgauskas also contributed heavily for the Cavs.'}"
                ],
                [
                  "49",
                  "{'Label': 'Sports', 'Headline': 'Pampling clings to lead at Australian Open', 'Summary': \"Australia's Rod Pampling recovered from a shaky start to retain the outright lead going into the final day of the centenary Australian Open.\"}"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 100
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_response_4_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'New York's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Memos Warne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'New Chechen Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'Two UN Workers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'U.S. State Dep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>{'Label': 'Sci/Tech', 'Headline': 'World's Sha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>{'Label': 'Sci/Tech', 'Headline': 'Ninth-Grade...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>{'Label': 'World', 'Headline': 'Congress Wants...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'IBM Jumps i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Clearing Th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0     {'Label': 'Business', 'Headline': 'New York's ...\n",
              "1     {'Label': 'Business', 'Headline': 'Memos Warne...\n",
              "2     {'Label': 'World', 'Headline': 'New Chechen Le...\n",
              "3     {'Label': 'World', 'Headline': 'Two UN Workers...\n",
              "4     {'Label': 'World', 'Headline': 'U.S. State Dep...\n",
              "                            ...                        \n",
              "95    {'Label': 'Sci/Tech', 'Headline': 'World's Sha...\n",
              "96    {'Label': 'Sci/Tech', 'Headline': 'Ninth-Grade...\n",
              "97    {'Label': 'World', 'Headline': 'Congress Wants...\n",
              "98    {'Label': 'Business', 'Headline': 'IBM Jumps i...\n",
              "99    {'Label': 'Business', 'Headline': 'Clearing Th...\n",
              "Name: model_response_4_parsed, Length: 100, dtype: object"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_4[\"model_response_4_parsed\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Sq00Tste6ugN",
        "outputId": "85b83428-2196-4a68-94c4-0f304d3cb648"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_4",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_4_parsed",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "d6b2bf78-62c3-4060-a826-b1576c524afc",
              "rows": [],
              "shape": {
                "columns": 3,
                "rows": 0
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a72bba7c-17e3-41ff-826e-ae4f7372d38a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_4</th>\n",
              "      <th>model_response_4_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a72bba7c-17e3-41ff-826e-ae4f7372d38a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a72bba7c-17e3-41ff-826e-ae4f7372d38a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a72bba7c-17e3-41ff-826e-ae4f7372d38a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Article, model_response_4, model_response_4_parsed]\n",
              "Index: []"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_4[data_4[\"model_response_4_parsed\"]=={}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Jpdeew-tft9P",
        "outputId": "b4ce791e-be36-4f6a-ce77-a014ce440933"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    {        \"Label\": \"Sports\",        \"Headline\": \"Hewitt Wins Washington Open, Fine-Tunes Game for US Open\",        \"Summary\": \"Lleyton Hewitt wins the \\\\$US500,000 (\\\\$A692,377) Washington Open, defeating Gilles Muller in a convincing 6-3 6-4 victory. The win fine-tunes Hewitt\\'s game ahead of the US Open.\"    }'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_4[\"model_response_4\"][43]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "n-jmYc6JfxRP"
      },
      "outputs": [],
      "source": [
        "data_4[\"model_response_4_parsed\"][43] = {\"Label\": \"Sports\",\"Headline\": \"Hewitt Wins Washington Open, Fine-Tunes Game for US Open\",\"Summary\": \"Lleyton Hewitt won the $500,000 Washington Open, defeating Gilles Muller in straight sets.The victory fine-tuned his game ahead of the US Open.\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SC6m-EdCuvp5",
        "outputId": "d4608e9a-53fe-4598-83be-8f399ad10a80"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Summary",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "76f98be2-397f-4ae0-a98c-d4ba502b45ae",
              "rows": [
                [
                  "0",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change",
                  "Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses."
                ],
                [
                  "1",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts",
                  "The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project."
                ],
                [
                  "2",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized",
                  "Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow."
                ],
                [
                  "3",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon",
                  "Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives."
                ],
                [
                  "4",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible",
                  "The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects."
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8a331ed6-9c71-4403-84ae-3c4b779f1ddf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "      <td>Lawyers and judges are advocating for a change...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq,...</td>\n",
              "      <td>The article reveals memos that suggest Custer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "      <td>Chechnya's new leader vowed to rebuild the reg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "      <td>Two of three United Nations workers kidnapped ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>The U.S. State Department has finished an inte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a331ed6-9c71-4403-84ae-3c4b779f1ddf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a331ed6-9c71-4403-84ae-3c4b779f1ddf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a331ed6-9c71-4403-84ae-3c4b779f1ddf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Label                                           Headline  \\\n",
              "0  Business  New York's Divorce Law Under Scrutiny: A Push ...   \n",
              "1  Business  Memos Warned of Billing Fraud by Firm in Iraq,...   \n",
              "2     World     New Chechen Leader Vows Peace, Poll Criticized   \n",
              "3     World  Two UN Workers Kidnapped in Afghanistan Call H...   \n",
              "4     World  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                             Summary  \n",
              "0  Lawyers and judges are advocating for a change...  \n",
              "1  The article reveals memos that suggest Custer ...  \n",
              "2  Chechnya's new leader vowed to rebuild the reg...  \n",
              "3  Two of three United Nations workers kidnapped ...  \n",
              "4  The U.S. State Department has finished an inte...  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_response_parsed_df_4 = pd.json_normalize(data_4['model_response_4_parsed'])\n",
        "model_response_parsed_df_4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "3Yz7sY06SN6W",
        "outputId": "c8af36a3-5290-47e5-d88f-5f69703a9446"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_4",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "model_response_4_parsed",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Summary",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "441fccce-1715-45e6-842a-4cc688f13baf",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "    {        \"Label\": \"Business\",        \"Headline\": \"New York's Divorce Law Under Scrutiny: A Push for Change\",        \"Summary\": \"Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses.\"    }",
                  "{'Label': 'Business', 'Headline': \"New York's Divorce Law Under Scrutiny: A Push for Change\", 'Summary': \"Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses.\"}",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change",
                  "Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses."
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "    {        \"Label\": \"Business\",        \"Headline\": \"Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts\",        \"Summary\": \"The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project.\"    }",
                  "{'Label': 'Business', 'Headline': 'Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts', 'Summary': 'The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project.'}",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts",
                  "The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project."
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "    {        \"Label\": \"World\",        \"Headline\": \"New Chechen Leader Vows Peace, Poll Criticized\",        \"Summary\": \"Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow.\"    }",
                  "{'Label': 'World', 'Headline': 'New Chechen Leader Vows Peace, Poll Criticized', 'Summary': \"Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow.\"}",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized",
                  "Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow."
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "{\"Label\": \"World\",\"Headline\": \"Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon\",\"Summary\": \"Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives.\"}",
                  "{'Label': 'World', 'Headline': 'Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon', 'Summary': 'Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives.'}",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon",
                  "Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives."
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "    {        \"Label\": \"World\",        \"Headline\": \"U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible\",        \"Summary\": \"The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects.\"    }",
                  "{'Label': 'World', 'Headline': 'U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible', 'Summary': 'The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects.'}",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible",
                  "The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects."
                ]
              ],
              "shape": {
                "columns": 6,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-58c4300c-0f5d-42fa-9951-15793676b745\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>model_response_4</th>\n",
              "      <th>model_response_4_parsed</th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>{        \"Label\": \"Business\",        \"Head...</td>\n",
              "      <td>{'Label': 'Business', 'Headline': 'New York's ...</td>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "      <td>Lawyers and judges are advocating for a change...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>{        \"Label\": \"Business\",        \"Head...</td>\n",
              "      <td>{'Label': 'Business', 'Headline': 'Memos Warne...</td>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq,...</td>\n",
              "      <td>The article reveals memos that suggest Custer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>{        \"Label\": \"World\",        \"Headlin...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'New Chechen Le...</td>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "      <td>Chechnya's new leader vowed to rebuild the reg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>{\"Label\": \"World\",\"Headline\": \"Two UN Workers ...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'Two UN Workers...</td>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "      <td>Two of three United Nations workers kidnapped ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>{        \"Label\": \"World\",        \"Headlin...</td>\n",
              "      <td>{'Label': 'World', 'Headline': 'U.S. State Dep...</td>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>The U.S. State Department has finished an inte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58c4300c-0f5d-42fa-9951-15793676b745')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58c4300c-0f5d-42fa-9951-15793676b745 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58c4300c-0f5d-42fa-9951-15793676b745');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article  \\\n",
              "0  A New Push to Loosen New York's Divorce Law La...   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...   \n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...   \n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                    model_response_4  \\\n",
              "0      {        \"Label\": \"Business\",        \"Head...   \n",
              "1      {        \"Label\": \"Business\",        \"Head...   \n",
              "2      {        \"Label\": \"World\",        \"Headlin...   \n",
              "3  {\"Label\": \"World\",\"Headline\": \"Two UN Workers ...   \n",
              "4      {        \"Label\": \"World\",        \"Headlin...   \n",
              "\n",
              "                             model_response_4_parsed     Label  \\\n",
              "0  {'Label': 'Business', 'Headline': 'New York's ...  Business   \n",
              "1  {'Label': 'Business', 'Headline': 'Memos Warne...  Business   \n",
              "2  {'Label': 'World', 'Headline': 'New Chechen Le...     World   \n",
              "3  {'Label': 'World', 'Headline': 'Two UN Workers...     World   \n",
              "4  {'Label': 'World', 'Headline': 'U.S. State Dep...     World   \n",
              "\n",
              "                                            Headline  \\\n",
              "0  New York's Divorce Law Under Scrutiny: A Push ...   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq,...   \n",
              "2     New Chechen Leader Vows Peace, Poll Criticized   \n",
              "3  Two UN Workers Kidnapped in Afghanistan Call H...   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                             Summary  \n",
              "0  Lawyers and judges are advocating for a change...  \n",
              "1  The article reveals memos that suggest Custer ...  \n",
              "2  Chechnya's new leader vowed to rebuild the reg...  \n",
              "3  Two of three United Nations workers kidnapped ...  \n",
              "4  The U.S. State Department has finished an inte...  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_with_parsed_model_output_4 = pd.concat([data_4, model_response_parsed_df_4], axis=1)\n",
        "data_with_parsed_model_output_4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "NOzYO3K1STzi",
        "outputId": "c94ad77d-af79-47fb-e5a5-e87b9a9851c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Article",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Headline",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Summary",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "bb5b9f4e-d5c9-435f-aee0-91aa67c5ef71",
              "rows": [
                [
                  "0",
                  "A New Push to Loosen New York's Divorce Law Lawyers and judges are pushing to have New York's divorce law changed, saying it is archaic and heightens hostilities between spouses.",
                  "Business",
                  "New York's Divorce Law Under Scrutiny: A Push for Change",
                  "Lawyers and judges are advocating for a change in New York's divorce law, citing its archaic nature and potential to heighten hostilities between spouses."
                ],
                [
                  "1",
                  "Memos Warned of Billing Fraud by Firm in Iraq The memorandums charge that Custer Battles repeatedly billed occupation authorities for nonexistent services.",
                  "Business",
                  "Memos Warned of Billing Fraud by Firm in Iraq, Raising Questions on U.S. Reconstruction Efforts",
                  "The article reveals memos that suggest Custer Battles, a firm involved in the U.S.-led reconstruction efforts in Iraq, may have committed billing fraud, highlighting concerns about the transparency and accountability of the multibillion-dollar project."
                ],
                [
                  "2",
                  "New Chechen Leader Vows Peace, Poll Criticized  GROZNY, Russia (Reuters) - Chechnya's new leader vowed on  Monday to rebuild the shattered region and crush extremists  after winning an election condemned by a rights group as a show  stage-managed by Moscow.",
                  "World",
                  "New Chechen Leader Vows Peace, Poll Criticized",
                  "Chechnya's new leader vowed to rebuild the region and crush extremists after winning an election criticized by a rights group as a show stage-managed by Moscow."
                ],
                [
                  "3",
                  "WWW KOTV.com _ Two of three UN workers kidnapped in Afghanistan have called home to say they are OK, officials and relatives said Tuesday, and one hostage said she expected to be released soon.",
                  "World",
                  "Two UN Workers Kidnapped in Afghanistan Call Home, Expect Release Soon",
                  "Two of three United Nations workers kidnapped in Afghanistan have called home to say they are okay and expect to be released soon, according to officials and relatives."
                ],
                [
                  "4",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan (Reuters) Reuters - The State Department finished an\\intensive review this week on how best to spend  #36;18.4 billion\\in U.S. aid to Iraq and may shift focus to smaller-scale\\projects, U.S. officials said on Wednesday.",
                  "World",
                  "U.S. State Dept Finishes Review of Iraq Aid Plan, Shift to Smaller-Scale Projects Possible",
                  "The U.S. State Department has finished an intensive review of how best to spend $18.4 billion in aid to Iraq and may shift focus to smaller-scale projects."
                ]
              ],
              "shape": {
                "columns": 4,
                "rows": 5
              }
            },
            "text/html": [
              "\n",
              "  <div id=\"df-de76df2e-c352-44d7-93d3-6041c7b91c8b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Label</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A New Push to Loosen New York's Divorce Law La...</td>\n",
              "      <td>Business</td>\n",
              "      <td>New York's Divorce Law Under Scrutiny: A Push ...</td>\n",
              "      <td>Lawyers and judges are advocating for a change...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq ...</td>\n",
              "      <td>Business</td>\n",
              "      <td>Memos Warned of Billing Fraud by Firm in Iraq,...</td>\n",
              "      <td>The article reveals memos that suggest Custer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized...</td>\n",
              "      <td>World</td>\n",
              "      <td>New Chechen Leader Vows Peace, Poll Criticized</td>\n",
              "      <td>Chechnya's new leader vowed to rebuild the reg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWW KOTV.com _ Two of three UN workers kidnapp...</td>\n",
              "      <td>World</td>\n",
              "      <td>Two UN Workers Kidnapped in Afghanistan Call H...</td>\n",
              "      <td>Two of three United Nations workers kidnapped ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>World</td>\n",
              "      <td>U.S. State Dept Finishes Review of Iraq Aid Pl...</td>\n",
              "      <td>The U.S. State Department has finished an inte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de76df2e-c352-44d7-93d3-6041c7b91c8b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de76df2e-c352-44d7-93d3-6041c7b91c8b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de76df2e-c352-44d7-93d3-6041c7b91c8b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Article     Label  \\\n",
              "0  A New Push to Loosen New York's Divorce Law La...  Business   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq ...  Business   \n",
              "2  New Chechen Leader Vows Peace, Poll Criticized...     World   \n",
              "3  WWW KOTV.com _ Two of three UN workers kidnapp...     World   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...     World   \n",
              "\n",
              "                                            Headline  \\\n",
              "0  New York's Divorce Law Under Scrutiny: A Push ...   \n",
              "1  Memos Warned of Billing Fraud by Firm in Iraq,...   \n",
              "2     New Chechen Leader Vows Peace, Poll Criticized   \n",
              "3  Two UN Workers Kidnapped in Afghanistan Call H...   \n",
              "4  U.S. State Dept Finishes Review of Iraq Aid Pl...   \n",
              "\n",
              "                                             Summary  \n",
              "0  Lawyers and judges are advocating for a change...  \n",
              "1  The article reveals memos that suggest Custer ...  \n",
              "2  Chechnya's new leader vowed to rebuild the reg...  \n",
              "3  Two of three United Nations workers kidnapped ...  \n",
              "4  The U.S. State Department has finished an inte...  "
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data_4 = data_with_parsed_model_output_4.drop(['model_response_4','model_response_4_parsed'], axis=1)\n",
        "final_data_4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTeJElYtpBKV"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H5tOCcipDlx"
      },
      "source": [
        "- We used an LLM to do multiple tasks, one stage at a time\n",
        "    1. We first identified the category of the news article.\n",
        "    2. Next, in addition to identifying the category, we generated a headline for it.\n",
        "    3. Finally, in addition to identifying the category and generating a headline, we also generated a summary for the article.\n",
        "\n",
        "- To try and improve the model output, one can try the following:\n",
        "    1. Update the prompt\n",
        "    2. Update the model parameters (`temparature`, `top_p`, ...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJypTp8-YMuj"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MjncuDf2qugI",
        "x9VSf2D_F5iU",
        "MzSKXh2LsOvd",
        "saFx1pbT_zTP",
        "kNdcQy4-GSaG",
        "ttQK4FHqHTar",
        "mY7NrV60HeJS",
        "iT3Vzwl1G8VZ",
        "ZSt6RorEHC3_",
        "vp9rd_qBHKXg",
        "T0T86JLANI5b",
        "2ODMLGbBfFJ1",
        "GzEz-HcaObq9",
        "tyqSugwPOfej",
        "uTeJElYtpBKV"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2776eb66d0404149b1e0cac94a83d014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b042fc0a45b48c3909406a751542461": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ce15578f764c31b30e1ca603a48a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "674582a985be4560a0aaf05d243a0d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc4ea4ed529a4822b978d9b5b0182425",
            "placeholder": "​",
            "style": "IPY_MODEL_2776eb66d0404149b1e0cac94a83d014",
            "value": " 9.23G/9.23G [01:33&lt;00:00, 107MB/s]"
          }
        },
        "94a1117cfd4747d38b6547e073061da1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd033b9252f4428a9cb4852c802ba9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4ea4ed529a4822b978d9b5b0182425": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b91c25e6954f32ad2544a65f0329ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b042fc0a45b48c3909406a751542461",
            "placeholder": "​",
            "style": "IPY_MODEL_acd033b9252f4428a9cb4852c802ba9d",
            "value": "llama-2-13b-chat.Q5_K_M.gguf: 100%"
          }
        },
        "ddaf5ffd281d451aa4b670747fbdb394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5b91c25e6954f32ad2544a65f0329ba",
              "IPY_MODEL_f6cd5e994f4947cbb52d97c74f4ef2f6",
              "IPY_MODEL_674582a985be4560a0aaf05d243a0d73"
            ],
            "layout": "IPY_MODEL_e0de0dbf405a447e80208d19c9c5a3c0"
          }
        },
        "e0de0dbf405a447e80208d19c9c5a3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6cd5e994f4947cbb52d97c74f4ef2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a1117cfd4747d38b6547e073061da1",
            "max": 9229924224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ce15578f764c31b30e1ca603a48a5c",
            "value": 9229924224
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
