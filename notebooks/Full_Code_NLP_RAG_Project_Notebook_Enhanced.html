<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Full_Code_NLP_RAG_Project_Notebook_</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        /* Table of Contents Sidebar */
        .toc-sidebar {
            position: fixed;
            left: 0;
            top: 0;
            width: 280px;
            height: 100vh;
            background: #2c3e50;
            color: white;
            padding: 20px;
            overflow-y: auto;
            box-shadow: 2px 0 5px rgba(0,0,0,0.1);
            z-index: 1000;
        }

        .toc-sidebar h2 {
            font-size: 1.5rem;
            margin-bottom: 20px;
            color: #3498db;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }

        .toc-list {
            list-style: none;
        }

        .toc-list li {
            margin: 8px 0;
        }

        .toc-list a {
            color: #ecf0f1;
            text-decoration: none;
            display: block;
            padding: 5px 10px;
            border-radius: 4px;
            transition: all 0.3s;
        }

        .toc-list a:hover {
            background: #34495e;
            color: #3498db;
            padding-left: 15px;
        }

        .toc-level-1 { font-weight: bold; font-size: 1.1em; }
        .toc-level-2 { padding-left: 15px; font-size: 1em; }
        .toc-level-3 { padding-left: 30px; font-size: 0.95em; }
        .toc-level-4 { padding-left: 45px; font-size: 0.9em; }
        .toc-level-5 { padding-left: 60px; font-size: 0.85em; }
        .toc-level-6 { padding-left: 75px; font-size: 0.8em; }

        /* Main Content */
        .main-content {
            margin-left: 280px;
            flex: 1;
            padding: 40px;
            background: white;
        }

        .notebook-title {
            font-size: 2.5rem;
            margin-bottom: 30px;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
        }

        /* Cell Styles */
        .cell {
            margin-bottom: 30px;
            border-radius: 8px;
            overflow: hidden;
        }

        .markdown-cell {
            padding: 20px;
            background: #fff;
            line-height: 1.8;
        }

        .markdown-cell h1 {
            font-size: 2rem;
            color: #2c3e50;
            margin: 20px 0 15px 0;
            padding-top: 20px;
            border-bottom: 2px solid #e0e0e0;
        }

        .markdown-cell h2 {
            font-size: 1.75rem;
            color: #34495e;
            margin: 18px 0 12px 0;
            padding-top: 15px;
        }

        .markdown-cell h3 {
            font-size: 1.5rem;
            color: #546e7a;
            margin: 15px 0 10px 0;
        }

        .markdown-cell h4 {
            font-size: 1.25rem;
            color: #607d8b;
            margin: 12px 0 8px 0;
        }

        .markdown-cell p {
            margin: 10px 0;
        }

        .markdown-cell ul, .markdown-cell ol {
            margin: 10px 0 10px 30px;
        }

        .markdown-cell li {
            margin: 5px 0;
        }

        .markdown-cell code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        .markdown-cell pre {
            background: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 10px 0;
        }

        .markdown-cell pre code {
            background: none;
            padding: 0;
        }

        .code-cell {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
        }

        .code-header {
            background: #263238;
            color: #aed581;
            padding: 8px 15px;
            font-family: 'Courier New', monospace;
            font-size: 0.85rem;
            font-weight: bold;
        }

        .code-content {
            padding: 15px;
            background: #263238;
            color: #aed581;
            overflow-x: auto;
        }

        .code-content pre {
            margin: 0;
            font-family: 'Courier New', Consolas, monospace;
            font-size: 0.9rem;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Output Styles - No scrolling */
        .cell-output {
            background: #f8f9fa;
            border-top: 1px solid #dee2e6;
            padding: 15px;
        }

        .output-label {
            font-weight: bold;
            color: #666;
            margin-bottom: 10px;
            font-size: 0.9rem;
        }

        .output-content {
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            padding: 15px;
            font-family: 'Courier New', Consolas, monospace;
            font-size: 0.85rem;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
        }

        .anchor-target {
            scroll-margin-top: 20px;
        }

        @media (max-width: 768px) {
            .toc-sidebar {
                width: 100%;
                height: auto;
                position: relative;
            }
            .main-content {
                margin-left: 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Table of Contents Sidebar -->
        <div class="toc-sidebar">
            <h2>ğŸ“‘ Table of Contents</h2>
            <ul class="toc-list">
                <li class="toc-level-2">
                    <a href="#section-0-problem-statement">Problem Statement</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-1-business-context">Business Context</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-4-objective">Objective</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-6-data-description">Data Description</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-8-installing-and-importing-necessary-libraries-and-dependencies">Installing and Importing Necessary Libraries and Dependencies</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-15-question-answering-using-llm">Question Answering using LLM</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-16-downloading-and-loading-the-model">Downloading and Loading the model</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-21-response">Response</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-23-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit">Query 1: What is the protocol for managing sepsis in a critical care unit?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-26-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it">Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-29-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it">Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-32-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function">Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-35-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery">Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-38-question-answering-using-llm-with-prompt-engineering">Question Answering using LLM with Prompt Engineering</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-40-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit">Query 1: What is the protocol for managing sepsis in a critical care unit?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-42-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it">Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-44-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it">Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-46-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function">Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-48-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery">Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-50-parameter-tuning-experiments">Parameter Tuning Experiments</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-51-combination-1-high-temperature-creative-response">Combination 1: High Temperature (Creative Response)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-53-combination-2-low-temperature-deterministic-response">Combination 2: Low Temperature (Deterministic Response)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-55-combination-3-high-top_k-diverse-vocabulary">Combination 3: High top_k (Diverse Vocabulary)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-57-combination-4-balanced-parameters-recommended-for-medical">Combination 4: Balanced Parameters (Recommended for Medical)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-59-combination-5-default-deterministic">Combination 5: Default (Deterministic)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-61-parameter-tuning-summary">Parameter Tuning Summary</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-62-data-preparation-for-rag">Data Preparation for RAG</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-63-loading-the-data">Loading the Data</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-67-data-overview">Data Overview</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-68-checking-the-first-5-pages">Checking the first 5 pages</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-70-checking-the-number-of-pages">Checking the number of pages</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-72-data-chunking">Data Chunking</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-80-embedding">Embedding</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-85-vector-database">Vector Database</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-91-retriever">Retriever</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-93-system-and-user-prompt-template">System and User Prompt Template</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-95-response-function">Response Function</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-97-question-answering-using-rag">Question Answering using RAG</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-98-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit">Query 1: What is the protocol for managing sepsis in a critical care unit?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-101-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it">Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-104-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it">Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-107-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function">Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-110-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery">Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-113-fine-tuning">Fine-tuning</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-119-rag-parameter-tuning-analysis">RAG Parameter Tuning Analysis</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-120-rag-combination-1-low-temperature-deterministic">RAG Combination 1: Low Temperature (Deterministic)</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-122-rag-combination-2-higher-temperature-with-constrained-top_p">RAG Combination 2: Higher Temperature with Constrained top_p</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-124-rag-combination-3-extended-max_tokens-for-detailed-responses">RAG Combination 3: Extended max_tokens for Detailed Responses</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-126-rag-combination-4-restricted-top_k-sampling">RAG Combination 4: Restricted top_k Sampling</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-128-rag-combination-5-increased-retrieval-k-with-balanced-generation-recommended-for-production">RAG Combination 5: Increased Retrieval k with Balanced Generation (Recommended for Production)</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-130-rag-fine-tuning-summary">RAG Fine-Tuning Summary</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-131-output-evaluation">Output Evaluation</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-137-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit">Query 1: What is the protocol for managing sepsis in a critical care unit?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-139-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it">Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-141-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it">Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-143-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function">Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-145-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery">Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-147-actionable-insights-and-business-recommendations">Actionable Insights and Business Recommendations</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-key-findings-from-the-rag-implementation">Key Findings from the RAG Implementation</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-1-performance-comparison-base-llm-vs-rag-enhanced-llm">1. **Performance Comparison: Base LLM vs. RAG-Enhanced LLM**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-2-evaluation-results-summary">2. **Evaluation Results Summary**</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-alignment-with-business-objectives">Alignment with Business Objectives</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-quantifiable-impact-on-information-overload">Quantifiable Impact on Information Overload</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-recommended-model-parameters-for-production">Recommended Model Parameters for Production</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-actionable-insights">Actionable Insights</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-insight-1-information-retrieval-quality-is-critical">**Insight 1: Information Retrieval Quality is Critical**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-insight-2-context-window-constraints-require-optimization">**Insight 2: Context Window Constraints Require Optimization**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-insight-3-medical-terminology-handling">**Insight 3: Medical Terminology Handling**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-insight-4-response-structure-improves-usability">**Insight 4: Response Structure Improves Usability**</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-business-recommendations">Business Recommendations</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-1-for-healthcare-implementation">**1. For Healthcare Implementation**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-2-technical-enhancements">**2. Technical Enhancements**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-3-risk-mitigation">**3. Risk Mitigation**</a>
                </li>
                <li class="toc-level-4">
                    <a href="#section-148-4-roi-considerations">**4. ROI Considerations**</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-future-development-roadmap">Future Development Roadmap</a>
                </li>
                <li class="toc-level-3">
                    <a href="#section-148-conclusion">Conclusion</a>
                </li>
                <li class="toc-level-2">
                    <a href="#section-150-export-to-custom-html">Export to Custom HTML</a>
                </li>
            </ul>
        </div>

        <!-- Main Content -->
        <div class="main-content">
            <h1 class="notebook-title">Full Code NLP RAG Project Notebook </h1>

            <div class="cell markdown-cell">
                <h2><span id="section-0-problem-statement" class="anchor-target"></span>Problem Statement</h2>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-1-business-context" class="anchor-target"></span>Business Context</h3>
            </div>
            <div class="cell markdown-cell">
                The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.</p><p>Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.</p><p>To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness.
            </div>
            <div class="cell markdown-cell">
                <strong>Common Questions to Answer</strong></p><p><strong>1. Diagnostic Assistance</strong>: "What are the common symptoms and treatments for pulmonary embolism?"</p><p><strong>2. Drug Information</strong>: "Can you provide the trade names of medications used for treating hypertension?"</p><p><strong>3. Treatment Plans</strong>: "What are the first-line options and alternatives for managing rheumatoid arthritis?"</p><p><strong>4. Specialty Knowledge</strong>: "What are the diagnostic steps for suspected endocrine disorders?"</p><p><strong>5. Critical Care Protocols</strong>: "What is the protocol for managing sepsis in a critical care unit?"
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-4-objective" class="anchor-target"></span>Objective</h3>
            </div>
            <div class="cell markdown-cell">
                As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to <strong>understand</strong> issues like information overload, <strong>apply</strong> AI techniques to streamline decision-making, <strong>analyze</strong> its impact on diagnostics and patient outcomes, <strong>evaluate</strong> its potential to standardize care practices, and <strong>create</strong> a functional prototype demonstrating its feasibility and effectiveness.
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-6-data-description" class="anchor-target"></span>Data Description</h3>
            </div>
            <div class="cell markdown-cell">
                The <strong>Merck Manuals</strong> are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.</p><p>The manual is provided as a PDF with over 4,000 pages divided into 23 sections.
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-8-installing-and-importing-necessary-libraries-and-dependencies" class="anchor-target"></span>Installing and Importing Necessary Libraries and Dependencies</h2>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [10]</div>
                <div class="code-content">
                    <pre>!nvidia-smi
</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Thu Jan 15 00:28:35 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |
| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [11]</div>
                <div class="code-content">
                    <pre># Installation for GPU llama-cpp-python
# uncomment and run the following code in case GPU is being used
!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 -q

# Installation for CPU llama-cpp-python
# uncomment and run the following code in case GPU is not being used
# !CMAKE_ARGS=&quot;-DLLAMA_CUBLAS=off&quot; FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --no-cache-dir -q</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m551.3/551.3 MB[0m [31m3.0 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m45.5/45.5 kB[0m [31m3.3 MB/s[0m eta [36m0:00:00[0m
[?25h</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Note</strong>:
<ul>
<li>After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.</li>
<li>On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in <strong><em>this notebook</strong></em>.</li>
</ul>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [13]</div>
                <div class="code-content">
                    <pre># For installing the libraries &amp; downloading models from HF Hub
!pip install huggingface_hub pandas tiktoken pymupdf langchain langchain-community langchain-text-splitters chromadb sentence-transformers numpy -q 2&gt;/dev/null || pip install huggingface_hub pandas tiktoken pymupdf langchain langchain-community langchain-text-splitters chromadb sentence-transformers numpy -q</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m52.0/52.0 kB[0m [31m4.2 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m24.1/24.1 MB[0m [31m102.9 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m2.5/2.5 MB[0m [31m92.6 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.1/21.1 MB[0m [31m108.5 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m278.2/278.2 kB[0m [31m27.8 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m2.0/2.0 MB[0m [31m102.2 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.0/1.0 MB[0m [31m67.1 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m17.4/17.4 MB[0m [31m116.8 MB/s[0m eta [36m0:00:00[0m00:01[0m00:01[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m72.5/72.5 kB[0m [31m8.1 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m132.6/132.6 kB[0m [31m15.3 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m66.4/66.4 kB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m220.0/220.0 kB[0m [31m24.8 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m105.4/105.4 kB[0m [31m11.6 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m71.6/71.6 kB[0m [31m8.8 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m60.6/60.6 kB[0m [31m7.0 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m64.7/64.7 kB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m51.0/51.0 kB[0m [31m5.9 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m128.4/128.4 kB[0m [31m15.7 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m46.0/46.0 kB[0m [31m5.1 MB/s[0m eta [36m0:00:00[0m
[2K   [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m86.8/86.8 kB[0m [31m10.0 MB/s[0m eta [36m0:00:00[0m
[?25h</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Note</strong>:
<ul>
<li>After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.</li>
<li>On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in <strong><em>this notebook</strong></em>.</li>
</ul>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [15]</div>
                <div class="code-content">
                    <pre>#Libraries for processing dataframes,text
import json,os
import tiktoken
import pandas as pd

#Libraries for Loading Data, Chunking, Embedding, and Vector Databases
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

#Libraries for downloading and loading the llm
from huggingface_hub import hf_hub_download
from llama_cpp import Llama</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-15-question-answering-using-llm" class="anchor-target"></span>Question Answering using LLM</h2>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-16-downloading-and-loading-the-model" class="anchor-target"></span>Downloading and Loading the model</h4>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [18]</div>
                <div class="code-content">
                    <pre>model_name_or_path = &quot;TheBloke/Mistral-7B-Instruct-v0.2-GGUF&quot;
model_basename = &quot;mistral-7b-instruct-v0.2.Q6_K.gguf&quot;</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [19]</div>
                <div class="code-content">
                    <pre># Get the Hugging Face token
HF_TOKEN = &quot;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&quot;

print(f&quot;âœ“ HF_TOKEN set successfully&quot;)
print(f&quot;  Token preview: {HF_TOKEN[:10]}...&quot;)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">âœ“ HF_TOKEN set successfully
  Token preview: hf_sMxzzru...
</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [20]</div>
                <div class="code-content">
                    <pre># Download the model from Hugging Face Hub
# For public models like TheBloke/Mistral-7B-Instruct-v0.2-GGUF, token is optional
model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename,
    token=HF_TOKEN if HF_TOKEN else None  # Only pass token if it exists
)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [21]</div>
                <div class="code-content">
                    <pre># Initialize Mistral-7B LLM with GPU acceleration
# Parameters:
#   - model_path: Path to downloaded GGUF model file
#   - n_ctx: Context window size (2300 tokens for balance of memory/context)
#   - n_gpu_layers: Number of layers offloaded to GPU (38 for efficient inference)
#   - n_batch: Batch size for prompt processing (512 for throughput optimization)
llm = Llama(
    model_path=model_path,
    n_ctx=2300,
    n_gpu_layers=38,
    n_batch=512
)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [&quot;&lt;unk&gt;&quot;, &quot;&lt;s&gt;&quot;, &quot;&lt;/s&gt;&quot;, &quot;&lt;0x00&gt;&quot;, &quot;&lt;...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q6_K
print_info: file size   = 5.53 GiB (6.56 BPW) 
init_tokenizer: initializing tokenizer for type 1
load: control token:      2 &#x27;&lt;/s&gt;&#x27; is not marked as EOG
load: control token:      1 &#x27;&lt;s&gt;&#x27; is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 (&#x27;&lt;/s&gt;&#x27;)
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = mistralai_mistral-7b-instruct-v0.2
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 &#x27;&lt;s&gt;&#x27;
print_info: EOS token        = 2 &#x27;&lt;/s&gt;&#x27;
print_info: UNK token        = 0 &#x27;&lt;unk&gt;&#x27;
print_info: PAD token        = 0 &#x27;&lt;unk&gt;&#x27;
print_info: LF token         = 13 &#x27;&lt;0x0A&gt;&#x27;
print_info: EOG token        = 2 &#x27;&lt;/s&gt;&#x27;
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0, is_swa = 0
load_tensors: layer   1 assigned to device CUDA0, is_swa = 0
load_tensors: layer   2 assigned to device CUDA0, is_swa = 0
load_tensors: layer   3 assigned to device CUDA0, is_swa = 0
load_tensors: layer   4 assigned to device CUDA0, is_swa = 0
load_tensors: layer   5 assigned to device CUDA0, is_swa = 0
load_tensors: layer   6 assigned to device CUDA0, is_swa = 0
load_tensors: layer   7 assigned to device CUDA0, is_swa = 0
load_tensors: layer   8 assigned to device CUDA0, is_swa = 0
load_tensors: layer   9 assigned to device CUDA0, is_swa = 0
load_tensors: layer  10 assigned to device CUDA0, is_swa = 0
load_tensors: layer  11 assigned to device CUDA0, is_swa = 0
load_tensors: layer  12 assigned to device CUDA0, is_swa = 0
load_tensors: layer  13 assigned to device CUDA0, is_swa = 0
load_tensors: layer  14 assigned to device CUDA0, is_swa = 0
load_tensors: layer  15 assigned to device CUDA0, is_swa = 0
load_tensors: layer  16 assigned to device CUDA0, is_swa = 0
load_tensors: layer  17 assigned to device CUDA0, is_swa = 0
load_tensors: layer  18 assigned to device CUDA0, is_swa = 0
load_tensors: layer  19 assigned to device CUDA0, is_swa = 0
load_tensors: layer  20 assigned to device CUDA0, is_swa = 0
load_tensors: layer  21 assigned to device CUDA0, is_swa = 0
load_tensors: layer  22 assigned to device CUDA0, is_swa = 0
load_tensors: layer  23 assigned to device CUDA0, is_swa = 0
load_tensors: layer  24 assigned to device CUDA0, is_swa = 0
load_tensors: layer  25 assigned to device CUDA0, is_swa = 0
load_tensors: layer  26 assigned to device CUDA0, is_swa = 0
load_tensors: layer  27 assigned to device CUDA0, is_swa = 0
load_tensors: layer  28 assigned to device CUDA0, is_swa = 0
load_tensors: layer  29 assigned to device CUDA0, is_swa = 0
load_tensors: layer  30 assigned to device CUDA0, is_swa = 0
load_tensors: layer  31 assigned to device CUDA0, is_swa = 0
load_tensors: layer  32 assigned to device CUDA0, is_swa = 0
load_tensors: tensor &#x27;token_embd.weight&#x27; (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  5563.55 MiB
load_tensors:   CPU_Mapped model buffer size =   102.54 MiB
...................................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2300
llama_context: n_ctx_per_seq = 2300
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (2300) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     0.12 MiB
create_memory: n_ctx = 2304 (padded)
llama_kv_cache_unified: layer   0: dev = CUDA0
llama_kv_cache_unified: layer   1: dev = CUDA0
llama_kv_cache_unified: layer   2: dev = CUDA0
llama_kv_cache_unified: layer   3: dev = CUDA0
llama_kv_cache_unified: layer   4: dev = CUDA0
llama_kv_cache_unified: layer   5: dev = CUDA0
llama_kv_cache_unified: layer   6: dev = CUDA0
llama_kv_cache_unified: layer   7: dev = CUDA0
llama_kv_cache_unified: layer   8: dev = CUDA0
llama_kv_cache_unified: layer   9: dev = CUDA0
llama_kv_cache_unified: layer  10: dev = CUDA0
llama_kv_cache_unified: layer  11: dev = CUDA0
llama_kv_cache_unified: layer  12: dev = CUDA0
llama_kv_cache_unified: layer  13: dev = CUDA0
llama_kv_cache_unified: layer  14: dev = CUDA0
llama_kv_cache_unified: layer  15: dev = CUDA0
llama_kv_cache_unified: layer  16: dev = CUDA0
llama_kv_cache_unified: layer  17: dev = CUDA0
llama_kv_cache_unified: layer  18: dev = CUDA0
llama_kv_cache_unified: layer  19: dev = CUDA0
llama_kv_cache_unified: layer  20: dev = CUDA0
llama_kv_cache_unified: layer  21: dev = CUDA0
llama_kv_cache_unified: layer  22: dev = CUDA0
llama_kv_cache_unified: layer  23: dev = CUDA0
llama_kv_cache_unified: layer  24: dev = CUDA0
llama_kv_cache_unified: layer  25: dev = CUDA0
llama_kv_cache_unified: layer  26: dev = CUDA0
llama_kv_cache_unified: layer  27: dev = CUDA0
llama_kv_cache_unified: layer  28: dev = CUDA0
llama_kv_cache_unified: layer  29: dev = CUDA0
llama_kv_cache_unified: layer  30: dev = CUDA0
llama_kv_cache_unified: layer  31: dev = CUDA0
llama_kv_cache_unified:      CUDA0 KV buffer size =   288.00 MiB
llama_kv_cache_unified: size =  288.00 MiB (  2304 cells,  32 layers,  1/1 seqs), K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 2
llama_context: max_nodes = 2328
llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   184.51 MiB
llama_context:  CUDA_Host compute buffer size =    16.51 MiB
llama_context: graph nodes  = 1126
llama_context: graph splits = 2
CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
Model metadata: {&#x27;tokenizer.chat_template&#x27;: &quot;{{ bos_token }}{% for message in messages %}{% if (message[&#x27;role&#x27;] == &#x27;user&#x27;) != (loop.index0 % 2 == 0) %}{{ raise_exception(&#x27;Conversation roles must alternate user/assistant/user/assistant/...&#x27;) }}{% endif %}{% if message[&#x27;role&#x27;] == &#x27;user&#x27; %}{{ &#x27;[INST] &#x27; + message[&#x27;content&#x27;] + &#x27; [/INST]&#x27; }}{% elif message[&#x27;role&#x27;] == &#x27;assistant&#x27; %}{{ message[&#x27;content&#x27;] + eos_token}}{% else %}{{ raise_exception(&#x27;Only user and assistant roles are supported!&#x27;) }}{% endif %}{% endfor %}&quot;, &#x27;tokenizer.ggml.add_eos_token&#x27;: &#x27;false&#x27;, &#x27;tokenizer.ggml.padding_token_id&#x27;: &#x27;0&#x27;, &#x27;tokenizer.ggml.unknown_token_id&#x27;: &#x27;0&#x27;, &#x27;tokenizer.ggml.eos_token_id&#x27;: &#x27;2&#x27;, &#x27;general.architecture&#x27;: &#x27;llama&#x27;, &#x27;llama.rope.freq_base&#x27;: &#x27;1000000.000000&#x27;, &#x27;llama.context_length&#x27;: &#x27;32768&#x27;, &#x27;general.name&#x27;: &#x27;mistralai_mistral-7b-instruct-v0.2&#x27;, &#x27;tokenizer.ggml.add_bos_token&#x27;: &#x27;true&#x27;, &#x27;llama.embedding_length&#x27;: &#x27;4096&#x27;, &#x27;llama.feed_forward_length&#x27;: &#x27;14336&#x27;, &#x27;llama.attention.layer_norm_rms_epsilon&#x27;: &#x27;0.000010&#x27;, &#x27;llama.rope.dimension_count&#x27;: &#x27;128&#x27;, &#x27;tokenizer.ggml.bos_token_id&#x27;: &#x27;1&#x27;, &#x27;llama.attention.head_count&#x27;: &#x27;32&#x27;, &#x27;llama.block_count&#x27;: &#x27;32&#x27;, &#x27;llama.attention.head_count_kv&#x27;: &#x27;8&#x27;, &#x27;general.quantization_version&#x27;: &#x27;2&#x27;, &#x27;tokenizer.ggml.model&#x27;: &#x27;llama&#x27;, &#x27;general.file_type&#x27;: &#x27;18&#x27;}
Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct
</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-21-response" class="anchor-target"></span>Response</h4>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [23]</div>
                <div class="code-content">
                    <pre>def response(query, max_tokens=1024, temperature=0, top_p=0.95, top_k=50):
    &quot;&quot;&quot;
    Generate a response from the LLM based on the input query.
    
    Args:
        query (str): The input prompt/question for the model
        max_tokens (int): Maximum number of tokens in the response (default: 1024)
        temperature (float): Controls randomness (0=deterministic, higher=more random)
        top_p (float): Nucleus sampling - cumulative probability threshold (default: 0.95)
        top_k (int): Top-k sampling - limits vocabulary to k most likely tokens (default: 50)
    
    Returns:
        str: The model&#x27;s generated text response
    &quot;&quot;&quot;
    model_output = llm(
      prompt=query,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      top_k=top_k
    )

    return model_output[&#x27;choices&#x27;][0][&#x27;text&#x27;]</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-23-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit" class="anchor-target"></span>Query 1: What is the protocol for managing sepsis in a critical care unit?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [25]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =     249.71 ms /    16 tokens (   15.61 ms per token,    64.08 tokens per second)
llama_perf_context_print:        eval time =   25869.96 ms /   647 runs   (   39.98 ms per token,    25.01 tokens per second)
llama_perf_context_print:       total time =   26684.42 ms /   663 tokens
llama_perf_context_print:    graphs reused =        626
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - Query 1 (Sepsis Protocol):</strong>
<ul>
<li>The base LLM provides a general response about sepsis management without access to the Merck Manual</li>
<li>The response may contain accurate general medical knowledge from training data but lacks specific protocol details</li>
<li><strong>Limitation</strong>: Without context from authoritative sources, the model relies solely on parametric knowledge, which may be outdated or incomplete</li>
<li><strong>Note</strong>: Responses should be verified against current clinical guidelines before clinical application</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-26-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it" class="anchor-target"></span>Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [28]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 2 prefix-match hit, remaining 32 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =     120.71 ms /    32 tokens (    3.77 ms per token,   265.09 tokens per second)
llama_perf_context_print:        eval time =   17004.33 ms /   398 runs   (   42.72 ms per token,    23.41 tokens per second)
llama_perf_context_print:       total time =   17378.35 ms /   430 tokens
llama_perf_context_print:    graphs reused =        385
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - Query 2 (Appendicitis):</strong>
<ul>
<li>The LLM provides information about appendicitis symptoms and treatment options</li>
<li>The model correctly identifies appendectomy as the standard surgical intervention</li>
<li><strong>Strength</strong>: General medical knowledge about common conditions is relatively accurate</li>
<li><strong>Limitation</strong>: Specific surgical techniques and timing recommendations may vary from current best practices without context grounding</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-29-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it" class="anchor-target"></span>Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [31]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 4 prefix-match hit, remaining 34 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      61.54 ms /    34 tokens (    1.81 ms per token,   552.53 tokens per second)
llama_perf_context_print:        eval time =   30441.34 ms /   658 runs   (   46.26 ms per token,    21.62 tokens per second)
llama_perf_context_print:       total time =   31075.26 ms /   692 tokens
llama_perf_context_print:    graphs reused =        637
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - Query 3 (Hair Loss - Alopecia Areata):</strong>
<ul>
<li>The model identifies the condition as alopecia areata and provides treatment options</li>
<li><strong>Strength</strong>: Covers multiple treatment modalities (corticosteroids, minoxidil, immunotherapy)</li>
<li><strong>Limitation</strong>: Without specific Merck Manual context, the response may miss nuanced treatment protocols or latest therapeutic options</li>
<li><strong>Risk</strong>: Potential for hallucination on specific drug dosages or treatment durations</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-32-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function" class="anchor-target"></span>Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [34]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 2 prefix-match hit, remaining 28 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      57.79 ms /    28 tokens (    2.06 ms per token,   484.50 tokens per second)
llama_perf_context_print:        eval time =   26030.51 ms /   508 runs   (   51.24 ms per token,    19.52 tokens per second)
llama_perf_context_print:       total time =   26470.14 ms /   536 tokens
llama_perf_context_print:    graphs reused =        491
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - Query 4 (Traumatic Brain Injury):</strong>
<ul>
<li>The LLM provides a structured response covering acute management and rehabilitation</li>
<li><strong>Strength</strong>: Addresses both immediate interventions and long-term recovery considerations</li>
<li><strong>Limitation</strong>: Critical care protocols for TBI require precise timing and thresholds (e.g., ICP monitoring) that may not be accurately represented</li>
<li><strong>Clinical Note</strong>: TBI management is highly specialized; responses should be cross-referenced with neurology guidelines</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-35-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery" class="anchor-target"></span>Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [37]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 2 prefix-match hit, remaining 35 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      64.60 ms /    35 tokens (    1.85 ms per token,   541.77 tokens per second)
llama_perf_context_print:        eval time =   26076.44 ms /   463 runs   (   56.32 ms per token,    17.76 tokens per second)
llama_perf_context_print:       total time =   26469.45 ms /   498 tokens
llama_perf_context_print:    graphs reused =        448
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - Query 5 (Leg Fracture):</strong>
<ul>
<li>The model provides comprehensive first-aid and recovery guidance</li>
<li><strong>Strength</strong>: Good coverage of immediate care (immobilization, pain management) and rehabilitation phases</li>
<li><strong>Limitation</strong>: Specific fracture types (compound, stress, etc.) require different treatment approaches not differentiated without context</li>
<li><strong>Summary for Base LLM Section</strong>: The model demonstrates broad medical knowledge but lacks the specificity and source verification needed for clinical decision support</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-38-question-answering-using-llm-with-prompt-engineering" class="anchor-target"></span>Question Answering using LLM with Prompt Engineering</h2>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [40]</div>
                <div class="code-content">
                    <pre>system_prompt = &quot;&quot;&quot;
You are a highly specialized medical information assistant with expertise in interpreting clinical references from the Merck Manual. Your role is to provide accurate, evidence-based medical information to healthcare professionals.

### Instructions:
1. **Context Source**: You will receive context from the Merck Manual, a trusted medical reference covering disorders, diagnostics, treatments, and pharmaceutical information. This context begins with the token: ###Context.

2. **Question Format**: User questions will begin with the token: ###Question.

3. **Response Guidelines**:
   - Provide precise, clinically accurate answers based ONLY on the provided context
   - Use proper medical terminology while maintaining clarity
   - Structure your response with clear sections when appropriate (e.g., Symptoms, Diagnosis, Treatment, Prognosis)
   - Include relevant dosages, procedures, or protocols when mentioned in the context
   - Distinguish between first-line and alternative treatments when applicable

4. **Accuracy Requirements**:
   - Do NOT hallucinate or infer information not present in the context
   - Do NOT provide personal medical advice or diagnoses
   - If the context contains partial information, clearly state what is available and what is missing
   - If the answer is not found in the context, respond: &quot;The provided Merck Manual excerpt does not contain sufficient information to answer this question.&quot;

5. **Medical Disclaimer**: Always remember that responses are for informational purposes and should be verified by qualified healthcare professionals before clinical application.

Respond in a clear, professional manner suitable for healthcare practitioners.
&quot;&quot;&quot;</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-40-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit" class="anchor-target"></span>Query 1: What is the protocol for managing sepsis in a critical care unit?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [42]</div>
                <div class="code-content">
                    <pre>user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1 prefix-match hit, remaining 379 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =     422.96 ms /   379 tokens (    1.12 ms per token,   896.07 tokens per second)
llama_perf_context_print:        eval time =   34551.88 ms /   661 runs   (   52.27 ms per token,    19.13 tokens per second)
llama_perf_context_print:       total time =   35556.67 ms /  1040 tokens
llama_perf_context_print:    graphs reused =        639
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-42-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it" class="anchor-target"></span>Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [44]</div>
                <div class="code-content">
                    <pre>user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 366 prefix-match hit, remaining 32 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      58.50 ms /    32 tokens (    1.83 ms per token,   547.03 tokens per second)
llama_perf_context_print:        eval time =   25539.04 ms /   471 runs   (   54.22 ms per token,    18.44 tokens per second)
llama_perf_context_print:       total time =   25934.94 ms /   503 tokens
llama_perf_context_print:    graphs reused =        455
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-44-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it" class="anchor-target"></span>Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [46]</div>
                <div class="code-content">
                    <pre>user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 368 prefix-match hit, remaining 34 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      70.59 ms /    34 tokens (    2.08 ms per token,   481.68 tokens per second)
llama_perf_context_print:        eval time =   27032.81 ms /   493 runs   (   54.83 ms per token,    18.24 tokens per second)
llama_perf_context_print:       total time =   27465.86 ms /   527 tokens
llama_perf_context_print:    graphs reused =        477
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-46-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function" class="anchor-target"></span>Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [48]</div>
                <div class="code-content">
                    <pre>user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 366 prefix-match hit, remaining 28 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      62.95 ms /    28 tokens (    2.25 ms per token,   444.78 tokens per second)
llama_perf_context_print:        eval time =   23931.17 ms /   448 runs   (   53.42 ms per token,    18.72 tokens per second)
llama_perf_context_print:       total time =   24311.53 ms /   476 tokens
llama_perf_context_print:    graphs reused =        433
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-48-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery" class="anchor-target"></span>Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [50]</div>
                <div class="code-content">
                    <pre>user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?&quot;
respstr = response(user_input)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Response:**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 366 prefix-match hit, remaining 35 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      65.05 ms /    35 tokens (    1.86 ms per token,   538.06 tokens per second)
llama_perf_context_print:        eval time =   26704.69 ms /   494 runs   (   54.06 ms per token,    18.50 tokens per second)
llama_perf_context_print:       total time =   27130.37 ms /   529 tokens
llama_perf_context_print:    graphs reused =        478
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observations - Prompt Engineering Results:</strong>
<ul>
<li>The structured system prompt significantly improves response organization</li>
<li>Medical terminology is used more appropriately with explicit instructions</li>
<li>The model acknowledges limitations when context is not provided</li>
<li>Responses follow a more clinical format suitable for healthcare professionals</li>
</ul></p><p>---</p><p><h3><span id="section-50-parameter-tuning-experiments" class="anchor-target"></span>Parameter Tuning Experiments</h3></p><p>Below we test different LLM parameter combinations to observe their effect on response quality:
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-51-combination-1-high-temperature-creative-response" class="anchor-target"></span>Combination 1: High Temperature (Creative Response)</h4>
<strong>Parameters</strong>: <code>temperature=0.7, top<em>p=0.9, top</em>k=50, max_tokens=1024</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [53]</div>
                <div class="code-content">
                    <pre># Combination 1: High temperature for more creative/varied responses
user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input, temperature=0.7, top_p=0.9, top_k=50, max_tokens=1024)

from IPython.display import display, Markdown
display(Markdown(f&quot;**Response (temp=0.7, top_p=0.9):**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 366 prefix-match hit, remaining 14 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =      48.58 ms /    14 tokens (    3.47 ms per token,   288.21 tokens per second)
llama_perf_context_print:        eval time =   35110.51 ms /   645 runs   (   54.43 ms per token,    18.37 tokens per second)
llama_perf_context_print:       total time =   35725.10 ms /   659 tokens
llama_perf_context_print:    graphs reused =        623
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-53-combination-2-low-temperature-deterministic-response" class="anchor-target"></span>Combination 2: Low Temperature (Deterministic Response)</h4>
<strong>Parameters</strong>: <code>temperature=0.1, top<em>p=0.5, top</em>k=20, max_tokens=1024</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [55]</div>
                <div class="code-content">
                    <pre># Combination 2: Low temperature for more deterministic/focused responses
user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input, temperature=0.1, top_p=0.5, top_k=20, max_tokens=1024)

from IPython.display import display, Markdown
display(Markdown(f&quot;**Response (temp=0.1, top_p=0.5, top_k=20):**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 379 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =   43384.67 ms /   809 runs   (   53.63 ms per token,    18.65 tokens per second)
llama_perf_context_print:       total time =   44187.28 ms /   810 tokens
llama_perf_context_print:    graphs reused =        782
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-55-combination-3-high-top<em>k-diverse-vocabulary" class="anchor-target"></span>Combination 3: High top</em>k (Diverse Vocabulary)</h4>
<strong>Parameters</strong>: <code>temperature=0.3, top<em>p=0.95, top</em>k=100, max_tokens=1024</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [57]</div>
                <div class="code-content">
                    <pre># Combination 3: High top_k for more diverse vocabulary selection
user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input, temperature=0.3, top_p=0.95, top_k=100, max_tokens=1024)

from IPython.display import display, Markdown
display(Markdown(f&quot;**Response (temp=0.3, top_p=0.95, top_k=100):**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 379 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =   25607.36 ms /   478 runs   (   53.57 ms per token,    18.67 tokens per second)
llama_perf_context_print:       total time =   25956.78 ms /   479 tokens
llama_perf_context_print:    graphs reused =        462
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-57-combination-4-balanced-parameters-recommended-for-medical" class="anchor-target"></span>Combination 4: Balanced Parameters (Recommended for Medical)</h4>
<strong>Parameters</strong>: <code>temperature=0.2, top<em>p=0.85, top</em>k=40, max_tokens=1024</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [59]</div>
                <div class="code-content">
                    <pre># Combination 4: Balanced parameters - recommended for medical applications
user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input, temperature=0.2, top_p=0.85, top_k=40, max_tokens=1024)

from IPython.display import display, Markdown
display(Markdown(f&quot;**Response (temp=0.2, top_p=0.85, top_k=40):**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 379 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =   36994.56 ms /   691 runs   (   53.54 ms per token,    18.68 tokens per second)
llama_perf_context_print:       total time =   37621.69 ms /   692 tokens
llama_perf_context_print:    graphs reused =        668
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-59-combination-5-default-deterministic" class="anchor-target"></span>Combination 5: Default (Deterministic)</h4>
<strong>Parameters</strong>: <code>temperature=0.0, top<em>p=0.95, top</em>k=50, max_tokens=1024</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [61]</div>
                <div class="code-content">
                    <pre># Combination 5: Default parameters for baseline comparison
user_input = system_prompt + &quot;\n\n\n&quot; + &quot;###Question: What is the protocol for managing sepsis in a critical care unit?&quot;
respstr = response(user_input, temperature=0.0, top_p=0.95, top_k=50, max_tokens=1024)

from IPython.display import display, Markdown
display(Markdown(f&quot;**Response (temp=0.2, top_p=0.85, top_k=40):**\n\n{respstr}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1 prefix-match hit, remaining 379 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =     516.16 ms /   379 tokens (    1.36 ms per token,   734.26 tokens per second)
llama_perf_context_print:        eval time =   37673.14 ms /   661 runs   (   56.99 ms per token,    17.55 tokens per second)
llama_perf_context_print:       total time =   38785.90 ms /  1040 tokens
llama_perf_context_print:    graphs reused =        639
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-61-parameter-tuning-summary" class="anchor-target"></span>Parameter Tuning Summary</h4></p><p>| Combination | Temperature | Top<em>p | Top</em>k | Use Case |
|------------|-------------|-------|-------|----------|
| <strong>1 (High Temp)</strong> | 0.7 | 0.9 | 50 | Creative brainstorming, differential diagnosis exploration |
| <strong>2 (Low Temp)</strong> | 0.1 | 0.5 | 20 | Precise protocols, drug dosages, deterministic answers |
| <strong>3 (High Top_k)</strong> | 0.3 | 0.95 | 100 | Comprehensive coverage, diverse medical terminology |
| <strong>4 (Balanced)</strong> | 0.2 | 0.85 | 40 | General medical Q&A, recommended for clinical use |
| <strong>5 (Default)</strong> | 0.0 | 0.95 | 50 | Most deterministic, baseline comparison |</p><p><strong>Key Observations:</strong>
<ul>
<li><strong>Lower temperature</strong> (0.1-0.2) produces more consistent, factual responses suitable for medical protocols</li>
<li><strong>Higher temperature</strong> (0.7+) introduces variability, useful for differential diagnosis but risks inaccuracy</li>
<li><strong>Top<em>p and top</em>k</strong> control vocabulary diversity; lower values focus responses, higher values explore alternatives</li>
<li><strong>For medical applications</strong>, Combination 2 or 4 is recommended to minimize hallucination risk</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-62-data-preparation-for-rag" class="anchor-target"></span>Data Preparation for RAG</h2>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-63-loading-the-data" class="anchor-target"></span>Loading the Data</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [65]</div>
                <div class="code-content">
                    <pre># Option 1: Download from a public URL (GitHub, S3, etc.)
# Replace the URL below with your file&#x27;s public URL
!wget -q &quot;https://raw.githubusercontent.com/visubramaniam/AI-RAG-GENAI/main/data/medical_diagnosis_manual.pdf&quot; -O medical_diagnosis_manual.pdf</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [66]</div>
                <div class="code-content">
                    <pre>pdf_loader = PyMuPDFLoader(&quot;medical_diagnosis_manual.pdf&quot;)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [67]</div>
                <div class="code-content">
                    <pre>merck = pdf_loader.load()</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-67-data-overview" class="anchor-target"></span>Data Overview</h3>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-68-checking-the-first-5-pages" class="anchor-target"></span>Checking the first 5 pages</h4>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [70]</div>
                <div class="code-content">
                    <pre>for i in range(5):
    print(f&quot;Page Number : {i+1}&quot;,end=&quot;\n&quot;)
    print(merck[i].page_content,end=&quot;\n&quot;)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Page Number : 1
vsubrama@me.com
1QVXKHA3T6
meant for personal use by vsubrama@m
shing the contents in part or full is liable
Page Number : 2
vsubrama@me.com
1QVXKHA3T6
This file is meant for personal use by vsubrama@me.com only.
Sharing or publishing the contents in part or full is liable for legal action.
Page Number : 3
Table of Contents
1
Front    ................................................................................................................................................................................................................
1
Cover    .......................................................................................................................................................................................................
2
Front Matter    ...........................................................................................................................................................................................
53
1 - Nutritional Disorders    ...............................................................................................................................................................
53
Chapter 1. Nutrition: General Considerations    .....................................................................................................................
59
Chapter 2. Undernutrition    .............................................................................................................................................................
69
Chapter 3. Nutritional Support    ...................................................................................................................................................
76
Chapter 4. Vitamin Deficiency, Dependency &amp; Toxicity    ..................................................................................................
99
Chapter 5. Mineral Deficiency &amp; Toxicity    ..............................................................................................................................
108
Chapter 6. Obesity &amp; the Metabolic Syndrome    ...............................................................................................................
120
2 - Gastrointestinal Disorders    ..............................................................................................................................................
120
Chapter 7. Approach to the Patient With Upper GI Complaints    ...............................................................................
132
Chapter 8. Approach to the Patient With Lower GI Complaints    ...............................................................................
143
Chapter 9. Diagnostic &amp; Therapeutic GI Procedures    ....................................................................................................
150
Chapter 10. GI Bleeding    ............................................................................................................................................................
158
Chapter 11. Acute Abdomen &amp; Surgical Gastroenterology    .........................................................................................
172
Chapter 12. Esophageal &amp; Swallowing Disorders    ..........................................................................................................
183
Chapter 13. Gastritis &amp; Peptic Ulcer Disease    ..................................................................................................................
196
Chapter 14. Bezoars &amp; Foreign Bodies    ..............................................................................................................................
199
Chapter 15. Pancreatitis    ............................................................................................................................................................
206
Chapter 16. Gastroenteritis    ......................................................................................................................................................
213
Chapter 17. Malabsorption Syndromes    ..............................................................................................................................
225
Chapter 18. Irritable Bowel Syndrome    ................................................................................................................................
229
Chapter 19. Inflammatory Bowel Disease    .........................................................................................................................
241
Chapter 20. Diverticular Disease    ...........................................................................................................................................
246
Chapter 21. Anorectal Disorders    ............................................................................................................................................
254
Chapter 22. Tumors of the GI Tract    ......................................................................................................................................
275
3 - Hepatic &amp; Biliary Disorders    ............................................................................................................................................
275
Chapter 23. Approach to the Patient With Liver Disease    ...........................................................................................
294
Chapter 24. Testing for Hepatic &amp; Biliary Disorders    ......................................................................................................
305
Chapter 25. Drugs &amp; the Liver    ................................................................................................................................................
308
Chapter 26. Alcoholic Liver Disease    ....................................................................................................................................
314
Chapter 27. Fibrosis &amp; Cirrhosis    ............................................................................................................................................
322
Chapter 28. Hepatitis    ..................................................................................................................................................................
333
Chapter 29. Vascular Disorders of the Liver    .....................................................................................................................
341
Chapter 30. Liver Masses &amp; Granulomas    ..........................................................................................................................
348
Chapter 31. Gallbladder &amp; Bile Duct Disorders    ...............................................................................................................
362
4 - Musculoskeletal &amp; Connective Tissue Disorders    .........................................................................................
362
Chapter 32. Approach to the Patient With Joint Disease    ............................................................................................
373
Chapter 33. Autoimmune Rheumatic Disorders    ..............................................................................................................
391
Chapter 34. Vasculitis    .................................................................................................................................................................
416
Chapter 35. Joint Disorders    .....................................................................................................................................................
435
Chapter 36. Crystal-Induced Arthritides    ..............................................................................................................................
443
Chapter 37. Osteoporosis    .........................................................................................................................................................
448
Chapter 38. Paget&#x27;s Disease of Bone    ..................................................................................................................................
451
Chapter 39. Osteonecrosis    .......................................................................................................................................................
455
Chapter 40. Infections of Joints &amp; Bones    ...........................................................................................................................
463
Chapter 41. Bursa, Muscle &amp; Tendon Disorders    .............................................................................................................
470
Chapter 42. Neck &amp; Back Pain    ...............................................................................................................................................
481
Chapter 43. Hand Disorders    ....................................................................................................................................................
vsubrama@me.com
1QVXKHA3T6
This file is meant for personal use by vsubrama@me.com only.
Sharing or publishing the contents in part or full is liable for legal action.
Page Number : 4
491
Chapter 44. Foot &amp; Ankle Disorders    .....................................................................................................................................
502
Chapter 45. Tumors of Bones &amp; Joints    ...............................................................................................................................
510
5 - Ear, Nose, Throat &amp; Dental Disorders    ..................................................................................................................
510
Chapter 46. Approach to the Patient With Ear Problems    ...........................................................................................
523
Chapter 47. Hearing Loss    .........................................................................................................................................................
535
Chapter 48. Inner Ear Disorders    ............................................................................................................................................
542
Chapter 49. Middle Ear &amp; Tympanic Membrane Disorders    ........................................................................................
550
Chapter 50. External Ear Disorders    .....................................................................................................................................
554
Chapter 51. Approach to the Patient With Nasal &amp; Pharyngeal Symptoms    .......................................................
567
Chapter 52. Oral &amp; Pharyngeal Disorders    .........................................................................................................................
578
Chapter 53. Nose &amp; Paranasal Sinus Disorders    .............................................................................................................
584
Chapter 54. Laryngeal Disorders    ...........................................................................................................................................
590
Chapter 55. Tumors of the Head &amp; Neck    ...........................................................................................................................
600
Chapter 56. Approach to Dental &amp; Oral Symptoms    .......................................................................................................
619
Chapter 57. Common Dental Disorders    .............................................................................................................................
629
Chapter 58. Dental Emergencies    ..........................................................................................................................................
635
Chapter 59. Temporomandibular Disorders    ......................................................................................................................
641
6 - Eye Disorders    ............................................................................................................................................................................
641
Chapter 60. Approach to the Ophthalmologic Patient    ..................................................................................................
669
Chapter 61. Refractive Error    ...................................................................................................................................................
674
Chapter 62. Eyelid &amp; Lacrimal Disorders    ...........................................................................................................................
680
Chapter 63. Conjunctival &amp; Scleral Disorders    .................................................................................................................
690
Chapter 64. Corneal Disorders    ...............................................................................................................................................
703
Chapter 65. Glaucoma    ...............................................................................................................................................................
710
Chapter 66. Cataract    ...................................................................................................................................................................
713
Chapter 67. Uveitis    ......................................................................................................................................................................
719
Chapter 68. Retinal Disorders    .................................................................................................................................................
731
Chapter 69. Optic Nerve Disorders    ......................................................................................................................................
737
Chapter 70. Orbital Diseases    ..................................................................................................................................................
742
7 - Dermatologic Disorders    ....................................................................................................................................................
742
Chapter 71. Approach to the Dermatologic Patient    .......................................................................................................
755
Chapter 72. Principles of Topical Dermatologic Therapy    ............................................................................................
760
Chapter 73. Acne &amp; Related Disorders    ...............................................................................................................................
766
Chapter 74. Bullous Diseases    .................................................................................................................................................
771
Chapter 75. Cornification Disorders    .....................................................................................................................................
775
Chapter 76. Dermatitis    ...............................................................................................................................................................
786
Chapter 77. Reactions to Sunlight    ........................................................................................................................................
791
Chapter 78. Psoriasis &amp; Scaling Diseases    ........................................................................................................................
799
Chapter 79. Hypersensitivity &amp; Inflammatory Disorders    .............................................................................................
808
Chapter 80. Sweating Disorders    ............................................................................................................................................
811
Chapter 81. Bacterial Skin Infections    ...................................................................................................................................
822
Chapter 82. Fungal Skin Infections    ......................................................................................................................................
831
Chapter 83. Parasitic Skin Infections    ...................................................................................................................................
836
Chapter 84. Viral Skin Diseases    ............................................................................................................................................
841
Chapter 85. Pigmentation Disorders    ....................................................................................................................................
846
Chapter 86. Hair Disorders    .......................................................................................................................................................
855
Chapter 87. Nail Disorders    .......................................................................................................................................................
861
Chapter 88. Pressure Ulcers    ...................................................................................................................................................
867
Chapter 89. Benign Tumors    .....................................................................................................................................................
874
Chapter 90. Cancers of the Skin    ............................................................................................................................................
882
8 - Endocrine &amp; Metabolic Disorders    .............................................................................................................................
882
Chapter 91. Principles of Endocrinology    ............................................................................................................................
887
Chapter 92. Pituitary Disorders    ..............................................................................................................................................
901
Chapter 93. Thyroid Disorders    ................................................................................................................................................
vsubrama@me.com
1QVXKHA3T6
This file is meant for personal use by vsubrama@me.com only.
Sharing or publishing the contents in part or full is liable for legal action.
Page Number : 5
921
Chapter 94. Adrenal Disorders    ................................................................................................................................................
936
Chapter 95. Polyglandular Deficiency Syndromes    ........................................................................................................
939
Chapter 96. Porphyrias    ..............................................................................................................................................................
949
Chapter 97. Fluid &amp; Electrolyte Metabolism    .....................................................................................................................
987
Chapter 98. Acid-Base Regulation &amp; Disorders    ..............................................................................................................
1001
Chapter 99. Diabetes Mellitus &amp; Disorders of Carbohydrate Metabolism    ........................................................
1024
Chapter 100. Lipid Disorders    ................................................................................................................................................
1034
Chapter 101. Amyloidosis    ......................................................................................................................................................
1037
Chapter 102. Carcinoid Tumors    ..........................................................................................................................................
1040
Chapter 103. Multiple Endocrine Neoplasia Syndromes    .........................................................................................
1046
9 - Hematology &amp; Oncology    ...............................................................................................................................................
1046
Chapter 104. Approach to the Patient With Anemia    ..................................................................................................
1050
Chapter 105. Anemias Caused by Deficient Erythropoiesis    ...................................................................................
1061
Chapter 106. Anemias Caused by Hemolysis    ...............................................................................................................
1078
Chapter 107. Neutropenia &amp; Lymphocytopenia    ...........................................................................................................
1086
Chapter 108. Thrombocytopenia &amp; Platelet Dysfunction    .........................................................................................
1097
Chapter 109. Hemostasis    ......................................................................................................................................................
1104
Chapter 110. Thrombotic Disorders    ...................................................................................................................................
1107
Chapter 111. Coagulation Disorders    ..................................................................................................................................
1113
Chapter 112. Bleeding Due to Abnormal Blood Vessels    ...........................................................................................
1116
Chapter 113. Spleen Disorders    ............................................................................................................................................
1120
Chapter 114. Eosinophilic Disorders    .................................................................................................................................
1126
Chapter 115. Histiocytic Syndromes    .................................................................................................................................
1131
Chapter 116. Myeloproliferative Disorders    .....................................................................................................................
1141
Chapter 117. Leukemias    .........................................................................................................................................................
1154
Chapter 118. Lymphomas    ......................................................................................................................................................
1164
Chapter 119. Plasma Cell Disorders    .................................................................................................................................
1172
Chapter 120. Iron Overload    ...................................................................................................................................................
1177
Chapter 121. Transfusion Medicine    ...................................................................................................................................
1186
Chapter 122. Overview of Cancer    ......................................................................................................................................
1198
Chapter 123. Tumor Immunology    .......................................................................................................................................
1204
Chapter 124. Principles of Cancer Therapy    ...................................................................................................................
1215
10 - Immunology; Allergic Disorders    ...........................................................................................................................
1215
Chapter 125. Biology of the Immune System    ...............................................................................................................
1227
Chapter 126. Immunodeficiency Disorders    ....................................................................................................................
1243
Chapter 127. Allergic &amp; Other Hypersensitivity Disorders    .......................................................................................
1263
Chapter 128. Transplantation    ...............................................................................................................................................
1281
11 - Infectious Diseases    ........................................................................................................................................................
1281
Chapter 129. Biology of Infectious Disease    ...................................................................................................................
1300
Chapter 130. Laboratory Diagnosis of Infectious Disease    ......................................................................................
1306
Chapter 131. Immunization    ...................................................................................................................................................
1313
Chapter 132. Bacteria &amp; Antibacterial Drugs    .................................................................................................................
1353
Chapter 133. Gram-Positive Cocci    ....................................................................................................................................
1366
Chapter 134. Gram-Positive Bacilli    ...................................................................................................................................
1376
Chapter 135. Gram-Negative Bacilli    .................................................................................................................................
1405
Chapter 136. Spirochetes    ......................................................................................................................................................
1413
Chapter 137. Neisseriaceae    .................................................................................................................................................
1419
Chapter 138. Chlamydia &amp; Mycoplasmas    ......................................................................................................................
1421
Chapter 139. Rickettsiae &amp; Related Organisms    ..........................................................................................................
1431
Chapter 140. Anaerobic Bacteria    ........................................................................................................................................
1450
Chapter 141. Mycobacteria    ...................................................................................................................................................
1470
Chapter 142. Fungi    ...................................................................................................................................................................
1493
Chapter 143. Approach to Parasitic Infections    .............................................................................................................
1496
Chapter 144. Nematodes (Roundworms)    .......................................................................................................................
vsubrama@me.com
1QVXKHA3T6
This file is meant for personal use by vsubrama@me.com only.
Sharing or publishing the contents in part or full is liable for legal action.
</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-70-checking-the-number-of-pages" class="anchor-target"></span>Checking the number of pages</h4>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [72]</div>
                <div class="code-content">
                    <pre>len(merck)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">4114</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-72-data-chunking" class="anchor-target"></span>Data Chunking</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [74]</div>
                <div class="code-content">
                    <pre>#Libraries for processing dataframes,text
import json,os
import tiktoken
import pandas as pd

#Libraries for Loading Data, Chunking, Embedding, and Vector Databases
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [75]</div>
                <div class="code-content">
                    <pre># Configure text splitter for chunking the medical PDF
# RecursiveCharacterTextSplitter uses hierarchy: paragraphs -&gt; sentences -&gt; words
# Tuning recommendations:
#   - More context: chunk_size=800, chunk_overlap=80 â€” if responses seem incomplete
#   - Higher precision: chunk_size=256, chunk_overlap=30 â€” if retrieval returns too much irrelevant info
#   - Dense retrieval: chunk_size=1024, chunk_overlap=100 â€” for complex multi-step medical procedures

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name=&#x27;cl100k_base&#x27;,  # GPT-4 tokenizer for accurate token counting
    chunk_size=512,               # ~512 tokens per chunk - good for medical content context
    chunk_overlap=50              # ~10% overlap to maintain continuity between chunks
)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [76]</div>
                <div class="code-content">
                    <pre>document_chunks = pdf_loader.load_and_split(text_splitter)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [77]</div>
                <div class="code-content">
                    <pre>len(document_chunks)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">8685</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [78]</div>
                <div class="code-content">
                    <pre>document_chunks[0].page_content</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">&#x27;vsubrama@me.com\n1QVXKHA3T6\nmeant for personal use by vsubrama@m\nshing the contents in part or full is liable&#x27;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [79]</div>
                <div class="code-content">
                    <pre>document_chunks[1].page_content</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">&#x27;vsubrama@me.com\n1QVXKHA3T6\nThis file is meant for personal use by vsubrama@me.com only.\nSharing or publishing the contents in part or full is liable for legal action.&#x27;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [80]</div>
                <div class="code-content">
                    <pre>document_chunks[2].page_content</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">&#x27;Table of Contents\n1\nFront    ................................................................................................................................................................................................................\n1\nCover    .......................................................................................................................................................................................................\n2\nFront Matter    ...........................................................................................................................................................................................\n53\n1 - Nutritional Disorders    ...............................................................................................................................................................\n53\nChapter 1. Nutrition: General Considerations    .....................................................................................................................\n59\nChapter 2. Undernutrition    .............................................................................................................................................................\n69\nChapter 3. Nutritional Support    ...................................................................................................................................................\n76\nChapter 4. Vitamin Deficiency, Dependency &amp; Toxicity    ..................................................................................................\n99\nChapter 5. Mineral Deficiency &amp; Toxicity    ..............................................................................................................................\n108\nChapter 6. Obesity &amp; the Metabolic Syndrome    ...............................................................................................................\n120\n2 - Gastrointestinal Disorders    ..............................................................................................................................................\n120\nChapter 7. Approach to the Patient With Upper GI Complaints    ...............................................................................\n132\nChapter 8. Approach to the Patient With Lower GI Complaints    ...............................................................................\n143\nChapter 9. Diagnostic &amp; Therapeutic GI Procedures    ....................................................................................................\n150\nChapter 10. GI Bleeding    ............................................................................................................................................................\n158\nChapter 11. Acute Abdomen &amp; Surgical Gastroenterology    .........................................................................................\n172\nChapter 12. Esophageal &amp; Swallowing Disorders    ..........................................................................................................\n183\nChapter 13. Gastritis &amp; Peptic Ulcer Disease    ..................................................................................................................\n196\nChapter 14. Bezoars &amp; Foreign Bodies    ..............................................................................................................................\n199\nChapter 15. Pancreatitis    ............................................................................................................................................................\n206\nChapter 16. Gastroenteritis    ......................................................................................................................................................\n213\nChapter 17. Malabsorption Syndromes    ..............................................................................................................................\n225\nChapter 18. Irritable Bowel Syndrome    ................................................................................................................................\n229\nChapter 19. Inflammatory Bowel Disease    .........................................................................................................................\n241\nChapter 20. Diverticular Disease    ...........................................................................................................................................\n246\nChapter 21. Anorectal Disorders    ............................................................................................................................................\n254\nChapter 22. Tumors of the GI Tract    ......................................................................................................................................\n275\n3 - Hepatic &amp; Biliary Disorders    ............................................................................................................................................\n275\nChapter 23. Approach to the Patient With Liver Disease    ...........................................................................................\n294\nChapter 24. Testing for Hepatic &amp; Biliary Disorders    ......................................................................................................\n305&#x27;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-80-embedding" class="anchor-target"></span>Embedding</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [82]</div>
                <div class="code-content">
                    <pre># Initialize the SentenceTransformer embedding model for semantic search
# Model: all-MiniLM-L6-v2 - A lightweight but effective model (384-dim embeddings)
# Why this model: Good balance of speed and accuracy for medical text retrieval
# Alternative options: all-mpnet-base-v2 (768-dim, more accurate but slower)
embedding_model = SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">/tmp/ipython-input-3530320947.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.
  embedding_model = SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)
</div>
                    <div class="output-content">modules.json:   0%|          | 0.00/349 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">config_sentence_transformers.json:   0%|          | 0.00/116 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">README.md: 0.00B [00:00, ?B/s]</div>
                    <div class="output-content">sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">config.json:   0%|          | 0.00/612 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">model.safetensors:   0%|          | 0.00/90.9M [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">tokenizer_config.json:   0%|          | 0.00/350 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">vocab.txt: 0.00B [00:00, ?B/s]</div>
                    <div class="output-content">tokenizer.json: 0.00B [00:00, ?B/s]</div>
                    <div class="output-content">special_tokens_map.json:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]</div>
                    <div class="output-content">config.json:   0%|          | 0.00/190 [00:00&lt;?, ?B/s]</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [83]</div>
                <div class="code-content">
                    <pre>embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)
embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [84]</div>
                <div class="code-content">
                    <pre>print(&quot;Dimension of the embedding vector &quot;,len(embedding_1))
len(embedding_1)==len(embedding_2)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Dimension of the embedding vector  384
</div>
                    <div class="output-content">True</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [85]</div>
                <div class="code-content">
                    <pre>embedding_1,embedding_2</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">([-0.06938155740499496,
  0.028927495703101158,
  -0.012014498934149742,
  -0.03177807852625847,
  0.07338891923427582,
  0.0018049993086606264,
  0.008893112652003765,
  0.09870372712612152,
  0.009720373898744583,
  -0.04362768679857254,
  0.08152824640274048,
  0.03121708519756794,
  -0.017092159017920494,
  -0.03760414198040962,
  -0.02888396754860878,
  -0.04297143593430519,
  0.033611614257097244,
  -0.018414007499814034,
  -0.014989626593887806,
  0.03954098001122475,
  -0.05121440440416336,
  0.02495284378528595,
  -0.0438384972512722,
  0.04766015335917473,
  0.026850096881389618,
  0.010950648225843906,
  -0.039429083466529846,
  0.07888826727867126,
  0.05787385255098343,
  -0.08338332176208496,
  0.01968211494386196,
  0.01839032582938671,
  0.024355372413992882,
  -0.01856439746916294,
  0.028145698830485344,
  0.0561881922185421,
  -0.029818791896104813,
  -0.09858214110136032,
  -0.08176635205745697,
  -0.06594885885715485,
  -0.05061347037553787,
  0.000738105911295861,
  -0.06974087655544281,
  0.0985378548502922,
  0.014952200464904308,
  0.027426397427916527,
  0.05217435210943222,
  -0.058213185518980026,
  0.03437662124633789,
  0.08854188024997711,
  -0.0010602145921438932,
  -0.047055985778570175,
  0.02294011414051056,
  0.1040743887424469,
  -0.05343352258205414,
  -0.10876183956861496,
  0.03315310180187225,
  -0.018908722326159477,
  -0.01028456725180149,
  0.02752072364091873,
  0.036074236035346985,
  0.03346867486834526,
  -0.05680576711893082,
  0.01174603309482336,
  0.05466807633638382,
  0.01496057491749525,
  0.02530696988105774,
  0.055888280272483826,
  -0.023632774129509926,
  -0.03268052265048027,
  -0.029120681807398796,
  -0.052232176065444946,
  -0.010187765583395958,
  0.09781087934970856,
  -0.04179907962679863,
  0.034087926149368286,
  0.0026835412718355656,
  -0.031017323955893517,
  0.01668364182114601,
  -0.009559481404721737,
  -0.02879732847213745,
  0.04245402663946152,
  0.04582039639353752,
  -0.05241798982024193,
  -0.01891079545021057,
  0.09132001549005508,
  0.05035344883799553,
  -0.0029893857426941395,
  0.058814071118831635,
  0.006765115074813366,
  0.050806738436222076,
  -0.016061237081885338,
  0.1796954721212387,
  -0.03065561130642891,
  -0.015328501351177692,
  0.006039705593138933,
  -0.05924447998404503,
  -0.0172373428940773,
  0.05135182663798332,
  0.03815602511167526,
  -0.006016930099576712,
  -0.01204448938369751,
  -0.05002930387854576,
  -0.015753239393234253,
  -0.041475992649793625,
  -0.012342286296188831,
  -0.016505872830748558,
  0.06539930403232574,
  -0.05146162584424019,
  -0.02567404881119728,
  -0.025811363011598587,
  0.008956572972238064,
  0.028204044327139854,
  -0.008619982749223709,
  0.058739371597766876,
  0.01839383691549301,
  -0.05287855118513107,
  0.02068832702934742,
  0.0822945237159729,
  -0.13200321793556213,
  0.03065214492380619,
  -0.005784079432487488,
  0.047424305230379105,
  -0.04748164862394333,
  -0.10280632227659225,
  -0.11491096764802933,
  0.03817766532301903,
  8.683715458834068e-33,
  -0.02128669060766697,
  0.058344919234514236,
  -0.029555102810263634,
  0.013391627930104733,
  -0.0471012145280838,
  0.015729520469903946,
  -0.008916385471820831,
  0.005096564535051584,
  -0.10275227576494217,
  0.0037342451978474855,
  0.007905147038400173,
  0.0232903640717268,
  -0.01329733058810234,
  0.0026533680502325296,
  -0.07119589298963547,
  -0.018567362800240517,
  0.028032217174768448,
  0.08243528753519058,
  0.04391685500741005,
  -0.05609750375151634,
  -0.01242633443325758,
  -0.04540004953742027,
  0.00969330407679081,
  0.04523403197526932,
  -0.041859764605760574,
  0.042010348290205,
  0.08086112886667252,
  -0.022271115332841873,
  -0.022548187524080276,
  0.014056091196835041,
  -0.02552281692624092,
  0.010762407444417477,
  0.002622817875817418,
  0.07681314647197723,
  0.02032536268234253,
  0.03657042607665062,
  -0.12951204180717468,
  -0.005567911546677351,
  -0.07845688611268997,
  0.039326027035713196,
  0.07837941497564316,
  -0.042687587440013885,
  0.0050939032807946205,
  -0.024028416723012924,
  -0.029299311339855194,
  -0.036571748554706573,
  0.02134479209780693,
  0.02532922476530075,
  0.03298569843173027,
  0.02124313823878765,
  -0.07507041096687317,
  0.010092752054333687,
  -0.024018822237849236,
  -0.03647588565945625,
  -0.056017499417066574,
  -0.003879885422065854,
  0.017144907265901566,
  -0.01835361123085022,
  0.04497157037258148,
  -0.029547998681664467,
  -0.039940882474184036,
  0.042082007974386215,
  -0.0038840672932565212,
  0.06386037170886993,
  -0.08386091142892838,
  -0.0551254004240036,
  -0.0021335233468562365,
  -0.11536761373281479,
  0.014976815320551395,
  -0.0954846516251564,
  -0.07060188055038452,
  -0.027591783553361893,
  0.06708051264286041,
  0.06556114554405212,
  -0.05344630032777786,
  -0.052586957812309265,
  -0.012314780615270138,
  0.04564918577671051,
  -0.020356204360723495,
  0.00343559542670846,
  -0.06512494385242462,
  -0.0877620056271553,
  0.01116140466183424,
  -0.0004962831153534353,
  -0.06511864811182022,
  0.05158589780330658,
  0.051863737404346466,
  -0.04366545006632805,
  -0.007843990810215473,
  0.04702280834317207,
  0.0097888745367527,
  -0.03607574477791786,
  -0.04862964153289795,
  0.01146004255861044,
  0.03402905911207199,
  -8.752240167331284e-33,
  -0.028330959379673004,
  -0.05361834168434143,
  -0.010838155634701252,
  -0.003181909676641226,
  0.002631282899528742,
  0.0764620304107666,
  -0.021449489519000053,
  0.03535397723317146,
  0.041308753192424774,
  0.04283304885029793,
  -0.09632259607315063,
  0.04644011706113815,
  -0.033734992146492004,
  -0.05390648543834686,
  0.017293112352490425,
  0.0064764926210045815,
  0.05001531168818474,
  -0.09758095443248749,
  -0.08042824268341064,
  -0.08988593518733978,
  0.010458789765834808,
  0.05604228377342224,
  0.0595344640314579,
  0.0004315274127293378,
  -0.019900357350707054,
  0.058884549885988235,
  0.03272596746683121,
  0.018458446487784386,
  -0.03986448794603348,
  0.09663940221071243,
  0.08986999094486237,
  -0.03374531492590904,
  -0.17080408334732056,
  -0.014139742590487003,
  -0.024312138557434082,
  -0.13722820580005646,
  0.09627188742160797,
  0.039760772138834,
  0.02365623600780964,
  -0.023297134786844254,
  0.019287338480353355,
  0.08321195095777512,
  -0.01627127267420292,
  0.02197429910302162,
  -0.05292818322777748,
  -0.0850406289100647,
  0.054100725799798965,
  -0.10590871423482895,
  0.08869623392820358,
  0.035069406032562256,
  0.011140149086713791,
  -0.07182450592517853,
  0.12413647025823593,
  0.011256270110607147,
  -0.025685882195830345,
  0.020943043753504753,
  0.04335324466228485,
  -0.0044664181768894196,
  0.03907673433423042,
  -0.06244787573814392,
  0.08710847049951553,
  0.07601151615381241,
  0.0010175580391660333,
  -0.017795054242014885,
  -0.0025868637021631002,
  -0.02104014717042446,
  0.04407466575503349,
  -0.04384297505021095,
  0.04763763025403023,
  -0.0031448998488485813,
  -0.001757834805175662,
  -0.0015693965833634138,
  0.01678275689482689,
  -0.11892435699701309,
  0.0859762504696846,
  -0.00672557158395648,
  0.017803695052862167,
  0.0023802905343472958,
  -0.04264848679304123,
  0.006180476397275925,
  0.06393403559923172,
  -0.034934669733047485,
  -0.021036341786384583,
  -0.005331642925739288,
  0.07297623157501221,
  -0.07323883473873138,
  -0.00034187413984909654,
  0.02833797223865986,
  0.00779738649725914,
  0.024150440469384193,
  -0.08832023292779922,
  0.023564748466014862,
  -0.018386751413345337,
  0.05302579700946808,
  0.04380587860941887,
  -3.736485965077918e-08,
  0.019104784354567528,
  0.012128835543990135,
  0.003615892957895994,
  0.04361339658498764,
  0.03936534747481346,
  -0.023304034024477005,
  -0.010579295456409454,
  0.05650421231985092,
  0.012702036648988724,
  0.14891187846660614,
  -0.002737545408308506,
  -0.05980059877038002,
  -0.04900607839226723,
  0.0005850937450304627,
  -0.09473977237939835,
  -0.01930968463420868,
  -0.027136826887726784,
  -7.083048694767058e-05,
  -0.091939777135849,
  -0.030452152714133263,
  0.0221723522990942,
  -0.01686128042638302,
  0.07112954556941986,
  -0.04361085593700409,
  0.02535037323832512,
  0.017394835129380226,
  0.05628858506679535,
  0.07783021032810211,
  -0.04010731726884842,
  -0.0012336334912106395,
  -0.008938671089708805,
  0.07323307543992996,
  0.0027731319423764944,
  -0.06632302701473236,
  -0.06326960772275925,
  -0.011080870404839516,
  0.022524317726492882,
  0.038910336792469025,
  -0.00046716825454495847,
  0.00404156930744648,
  -0.046134769916534424,
  -0.04705965891480446,
  0.08157062530517578,
  0.0799841433763504,
  -0.02507956698536873,
  0.004154964815825224,
  -0.080841064453125,
  -0.02620004303753376,
  -0.03868293762207031,
  -0.014562958851456642,
  0.004038764629513025,
  -0.07202131301164627,
  0.006899698171764612,
  0.09682446718215942,
  -0.05678712949156761,
  0.009384166449308395,
  0.05362551659345627,
  0.059339702129364014,
  0.060245513916015625,
  -0.020303554832935333,
  -0.005863118451088667,
  -0.06436385959386826,
  0.07633510231971741,
  -0.0930991843342781],
 [-0.045390766113996506,
  0.025610245764255524,
  -0.061122771352529526,
  -0.03858242928981781,
  0.07325825840234756,
  -0.003331416053697467,
  -0.06977903842926025,
  0.062390733510255814,
  0.023345770314335823,
  0.017702152952551842,
  0.05806479603052139,
  0.09409213811159134,
  -0.013461381196975708,
  -0.05921242758631706,
  -0.015537743456661701,
  0.012576744891703129,
  0.0002699281321838498,
  0.008181012235581875,
  0.017558403313159943,
  0.04415692389011383,
  -0.03520352020859718,
  0.06596603244543076,
  -0.014689641073346138,
  0.0272922795265913,
  0.06841056048870087,
  -0.0030623930506408215,
  -0.06365837156772614,
  0.05708375573158264,
  -0.006514011416584253,
  -0.09749914705753326,
  -0.004507990553975105,
  0.00446569686755538,
  0.023356575518846512,
  0.013532562181353569,
  0.06110154837369919,
  0.02152526192367077,
  0.02277788333594799,
  -0.0687689408659935,
  -0.055825378745794296,
  -0.029985953122377396,
  -0.044572800397872925,
  -0.006983510218560696,
  -0.05657938867807388,
  0.06956233829259872,
  -0.020863277837634087,
  0.011099779047071934,
  0.02819902077317238,
  -0.016056116670370102,
  0.04347684234380722,
  0.03991188853979111,
  -0.06993430852890015,
  -0.03655752167105675,
  0.018965115770697594,
  0.044894300401210785,
  9.670686995377764e-05,
  -0.09639690816402435,
  0.010889914818108082,
  0.026899129152297974,
  0.0073155602440238,
  0.01834765262901783,
  0.03676392510533333,
  0.043400801718235016,
  -0.04996166005730629,
  -0.010741001926362514,
  0.029221413657069206,
  0.05879862979054451,
  -0.002207298530265689,
  0.10751097649335861,
  -0.053085800260305405,
  -0.09847630560398102,
  -0.07188733667135239,
  0.01176729891449213,
  -0.011280172504484653,
  0.07666534185409546,
  -0.006615846883505583,
  -0.027772102504968643,
  -0.03127537667751312,
  0.02587234228849411,
  0.04867591708898544,
  -0.028336593881249428,
  -0.00846531055867672,
  -0.012581774964928627,
  0.0427665188908577,
  -0.06149877980351448,
  -0.0561579205095768,
  0.0740509033203125,
  0.12096919119358063,
  -0.025316212326288223,
  0.08887175470590591,
  -0.011197347193956375,
  -0.0068489015102386475,
  -0.011447414755821228,
  0.16337083280086517,
  -0.027587907388806343,
  -0.00960543379187584,
  -0.012056744657456875,
  -0.020119138062000275,
  0.01382972951978445,
  0.046302083879709244,
  0.03294322267174721,
  0.006898914463818073,
  -0.018416238948702812,
  -0.044809918850660324,
  0.0026020018849521875,
  -0.014082912355661392,
  0.007010889705270529,
  0.05963487550616264,
  0.06941689550876617,
  -0.03733189404010773,
  -0.03279459476470947,
  -0.03844120725989342,
  -0.02775953710079193,
  -0.04039795696735382,
  -0.11892275512218475,
  0.08595480769872665,
  -0.019717629998922348,
  -0.014257531613111496,
  0.04472599923610687,
  0.061899974942207336,
  -0.09234599024057388,
  0.008561530150473118,
  -0.03265830874443054,
  0.03603704646229744,
  -0.02019970305263996,
  -0.07099513709545135,
  -0.1623438149690628,
  0.04051396995782852,
  8.070177937365884e-33,
  -0.01789691485464573,
  0.04254204034805298,
  -0.026545554399490356,
  0.058167990297079086,
  0.023010239005088806,
  -0.05586458370089531,
  -0.05406578630208969,
  -0.041556183248758316,
  -0.19049984216690063,
  -0.014554863795638084,
  0.016954001039266586,
  -0.003178993007168174,
  0.042670272290706635,
  0.031852319836616516,
  -0.07789614051580429,
  -0.02546524628996849,
  0.010606869123876095,
  0.027814138680696487,
  0.02601475641131401,
  -0.08012942224740982,
  -0.005005192942917347,
  -0.034107353538274765,
  0.00574559485539794,
  0.06850279867649078,
  -0.10468808561563492,
  0.028667166829109192,
  0.11494258791208267,
  -0.03690033033490181,
  -0.0011409486178308725,
  -0.00629813876003027,
  -0.021377673372626305,
  0.0009444384486414492,
  0.015003874897956848,
  0.03237085044384003,
  0.02083616890013218,
  0.014454400166869164,
  -0.02973594143986702,
  -0.0029823665972799063,
  -0.03451576456427574,
  0.07358157634735107,
  0.055882204324007034,
  -0.02969452738761902,
  -0.024070249870419502,
  -0.04599612578749657,
  0.002433883724734187,
  0.05208277329802513,
  0.038470178842544556,
  0.0052058761939406395,
  0.06858382374048233,
  -0.024560511112213135,
  -0.02188917249441147,
  0.05510065704584122,
  -0.021032514050602913,
  -0.01596860960125923,
  0.010875040665268898,
  0.009015187621116638,
  -0.030141886323690414,
  -0.043239254504442215,
  0.02520843967795372,
  -0.009991321712732315,
  -0.010669562965631485,
  0.05406607314944267,
  0.02090565860271454,
  0.04265891760587692,
  -0.08521512150764465,
  -0.12101033329963684,
  -0.01990722492337227,
  -0.07967063039541245,
  0.07121788710355759,
  -0.04147877171635628,
  -0.08168766647577286,
  -0.04000402241945267,
  0.07475466281175613,
  -0.01805603876709938,
  -0.03510681539773941,
  -0.061446432024240494,
  -0.02707725390791893,
  0.04616319760680199,
  -0.01185093354433775,
  -0.018573304638266563,
  -0.07163164019584656,
  -0.055762507021427155,
  -0.007362050004303455,
  -0.07088180631399155,
  -0.02073369361460209,
  0.046688616275787354,
  -0.024057544767856598,
  -0.0418207049369812,
  0.014098738320171833,
  0.006537009030580521,
  0.0045331004075706005,
  -0.029548216611146927,
  -0.03753003478050232,
  0.014620587229728699,
  0.03100752644240856,
  -7.897311207501897e-33,
  -0.041239187121391296,
  -0.037009794265031815,
  -0.03354054316878319,
  -0.01091373898088932,
  0.005085071083158255,
  0.07962905615568161,
  -0.02273242548108101,
  0.04119996726512909,
  0.03400009498000145,
  0.0580805242061615,
  -0.08613166213035583,
  0.04123781993985176,
  -0.0304094348102808,
  -0.04930081591010094,
  0.028646742925047874,
  -0.012865211814641953,
  0.07649241387844086,
  -0.11138544231653214,
  -0.1360863447189331,
  -0.06145394593477249,
  -0.0410931222140789,
  -0.012597192078828812,
  0.08379538357257843,
  0.019204813987016678,
  0.030216727405786514,
  0.0525873601436615,
  0.08499903231859207,
  0.04448899254202843,
  0.0037631220184266567,
  0.028236839920282364,
  0.08957473188638687,
  -0.007887267507612705,
  -0.17670388519763947,
  -0.06508667767047882,
  -0.00019861750479321927,
  -0.12282566726207733,
  0.0690760686993599,
  0.022074691951274872,
  0.06520241498947144,
  0.003075809683650732,
  -0.008275564759969711,
  0.09122011065483093,
  -0.02689838595688343,
  -0.037415701895952225,
  -0.02013075351715088,
  -0.010989245027303696,
  0.04529924318194389,
  -0.04351390525698662,
  0.02261095866560936,
  0.02932113967835903,
  0.04333730787038803,
  -0.09696228057146072,
  0.14067043364048004,
  -0.06403093785047531,
  -0.017884613946080208,
  -0.005451171658933163,
  0.003993352875113487,
  0.043315738439559937,
  0.050570353865623474,
  0.0009393600048497319,
  0.05692450329661369,
  0.04202285408973694,
  -0.05986543744802475,
  -0.01838744804263115,
  -0.013911961577832699,
  0.0013969591818749905,
  0.010886167176067829,
  -0.027617676183581352,
  0.021768715232610703,
  0.01211602333933115,
  -0.02970893681049347,
  -0.031356871128082275,
  -0.020544417202472687,
  -0.09224671125411987,
  0.07478920370340347,
  0.0030219091568142176,
  0.017946621403098106,
  0.030104704201221466,
  -0.06413563340902328,
  0.06465313583612442,
  0.05743439495563507,
  0.03106583096086979,
  0.012853178195655346,
  0.009809971787035465,
  0.05135922506451607,
  -0.024961864575743675,
  -0.04108355939388275,
  0.05025468021631241,
  0.04825248941779137,
  0.011873170733451843,
  -0.08112500607967377,
  0.03849083557724953,
  0.008029699325561523,
  -0.012099609710276127,
  0.06784868240356445,
  -4.114734863946978e-08,
  -0.0005197043064981699,
  -0.025522751733660698,
  -0.029735229909420013,
  0.01996523141860962,
  0.0287161897867918,
  0.07179927825927734,
  -0.013859927654266357,
  -0.007747736293822527,
  0.02332877926528454,
  0.12646709382534027,
  -0.0029391597490757704,
  -0.08118908107280731,
  -0.052067652344703674,
  0.03270414099097252,
  -0.09365365654230118,
  -0.02534526400268078,
  0.05200468376278877,
  -0.043349333107471466,
  -0.02307632379233837,
  -0.027954597026109695,
  0.05260990560054779,
  0.0020230296067893505,
  0.10608148574829102,
  -0.08274929970502853,
  0.04863452538847923,
  0.029931437224149704,
  0.03209421783685684,
  0.027709312736988068,
  -0.08927533775568008,
  0.0022116615436971188,
  0.02283496968448162,
  0.09380809217691422,
  -0.013140179216861725,
  -0.06826770305633545,
  -0.06264030188322067,
  0.01608869433403015,
  0.03909052908420563,
  0.07846824824810028,
  -0.049361422657966614,
  -0.019099952653050423,
  0.030875355005264282,
  0.014319046400487423,
  0.053874190896749496,
  0.03700678423047066,
  -0.04889152571558952,
  -0.012304374948143959,
  -0.028028175234794617,
  -0.0416594073176384,
  -0.021167678758502007,
  -0.02084801159799099,
  -0.009908810257911682,
  -0.040912821888923645,
  0.04670754447579384,
  0.15557041764259338,
  -0.025054745376110077,
  0.014411967247724533,
  0.02964959666132927,
  0.03539019078016281,
  0.06891202926635742,
  -0.029410025104880333,
  0.05857248231768608,
  -0.025428304448723793,
  0.027666684240102768,
  -0.04619927704334259])</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-85-vector-database" class="anchor-target"></span>Vector Database</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [87]</div>
                <div class="code-content">
                    <pre># Define output directory for persistent ChromaDB storage
# Persisting the vector store allows reuse without re-embedding documents
out_dir = &#x27;medical_db&#x27;

if not os.path.exists(out_dir):
  os.makedirs(out_dir)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [88]</div>
                <div class="code-content">
                    <pre># Create and populate the ChromaDB vector store with document embeddings
# This step embeds all document chunks and stores them for similarity search
# Note: This operation runs once; subsequent loads use the persisted database
vectorstore = Chroma.from_documents(
    document_chunks, # Pass the document chunks
    embedding_model, # Pass the embedding model
    persist_directory=out_dir
)</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [89]</div>
                <div class="code-content">
                    <pre># Load existing vector store from persisted directory (for subsequent runs)
# This avoids re-embedding and enables fast startup
vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">/tmp/ipython-input-3021968300.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.
  vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)
</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [90]</div>
                <div class="code-content">
                    <pre>vectorstore.embeddings</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">HuggingFaceEmbeddings(client=SentenceTransformer(
  (0): Transformer({&#x27;max_seq_length&#x27;: 256, &#x27;do_lower_case&#x27;: False, &#x27;architecture&#x27;: &#x27;BertModel&#x27;})
  (1): Pooling({&#x27;word_embedding_dimension&#x27;: 384, &#x27;pooling_mode_cls_token&#x27;: False, &#x27;pooling_mode_mean_tokens&#x27;: True, &#x27;pooling_mode_max_tokens&#x27;: False, &#x27;pooling_mode_mean_sqrt_len_tokens&#x27;: False, &#x27;pooling_mode_weightedmean_tokens&#x27;: False, &#x27;pooling_mode_lasttoken&#x27;: False, &#x27;include_prompt&#x27;: True})
  (2): Normalize()
), model_name=&#x27;all-MiniLM-L6-v2&#x27;, cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [91]</div>
                <div class="code-content">
                    <pre># Test similarity search with a sample medical query
vectorstore.similarity_search(&quot;What is the protocol for managing sepsis in a critical care unit?&quot;, k=3)</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">[Document(metadata={&#x27;keywords&#x27;: &#x27;&#x27;, &#x27;producer&#x27;: &#x27;pdf-lib (https://github.com/Hopding/pdf-lib)&#x27;, &#x27;format&#x27;: &#x27;PDF 1.7&#x27;, &#x27;file_path&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;total_pages&#x27;: 4114, &#x27;subject&#x27;: &#x27;&#x27;, &#x27;title&#x27;: &#x27;The Merck Manual of Diagnosis &amp; Therapy, 19th Edition&#x27;, &#x27;moddate&#x27;: &#x27;2026-01-12T13:48:10+00:00&#x27;, &#x27;trapped&#x27;: &#x27;&#x27;, &#x27;creationDate&#x27;: &#x27;D:20120615054440Z&#x27;, &#x27;modDate&#x27;: &#x27;D:20260112134810Z&#x27;, &#x27;creator&#x27;: &#x27;Atop CHM to PDF Converter&#x27;, &#x27;creationdate&#x27;: &#x27;2012-06-15T05:44:40+00:00&#x27;, &#x27;source&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;page&#x27;: 2400, &#x27;author&#x27;: &#x27;&#x27;}, page_content=&quot;16 - Critical Care Medicine\nChapter 222. Approach to the Critically Ill Patient\nIntroduction\nCritical care medicine specializes in caring for the most seriously ill patients. These patients are best\ntreated in an ICU staffed by experienced personnel. Some hospitals maintain separate units for special\npopulations (eg, cardiac, surgical, neurologic, pediatric, or neonatal patients). ICUs have a high\nnurse:patient ratio to provide the necessary high intensity of service, including treatment and monitoring\nof physiologic parameters.\nSupportive care for the ICU patient includes provision of adequate nutrition (see p. 21) and prevention of\ninfection, stress ulcers and gastritis (see p. 131), and pulmonary embolism (see p. 1920). Because 15 to\n25% of patients admitted to ICUs die there, physicians should know how to minimize suffering and help\ndying patients maintain dignity (see p. 3480).\nPatient Monitoring and Testing\nSome monitoring is manual (ie, by direct observation and physical examination) and intermittent, with the\nfrequency depending on the patient&#x27;s illness. This monitoring usually includes measurement of vital signs\n(temperature, BP, pulse, and respiration rate), quantification of all fluid intake and output, and often daily\nweight. BP may be recorded by an automated sphygmomanometer; a transcutaneous sensor for pulse\noximetry is used as well.\nOther monitoring is ongoing and continuous, provided by complex devices that require special training\nand experience to operate. Most such devices generate an alarm if certain physiologic parameters are\nexceeded. Every ICU should strictly follow protocols for investigating alarms.\nBlood Tests\nAlthough frequent blood draws can destroy veins, cause pain, and lead to anemia, ICU patients typically\nhave routine daily blood tests to help detect problems early. Generally, patients need a daily set of\nelectrolytes and a CBC. Patients with arrhythmias should also have Mg, phosphate, and Ca levels\nmeasured. Patients receiving TPN need weekly liver enzymes and coagulation profiles. Other tests (eg,\nblood culture for fever, CBC after a bleeding episode) are done as needed.\nPoint-of-care testing uses miniaturized, highly automated devices to do certain blood tests at the patient&#x27;s\nbedside or unit (particularly ICU, emergency department, and operating room). Commonly available tests&quot;),
 Document(metadata={&#x27;creator&#x27;: &#x27;Atop CHM to PDF Converter&#x27;, &#x27;producer&#x27;: &#x27;pdf-lib (https://github.com/Hopding/pdf-lib)&#x27;, &#x27;file_path&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;moddate&#x27;: &#x27;2026-01-12T13:48:10+00:00&#x27;, &#x27;total_pages&#x27;: 4114, &#x27;format&#x27;: &#x27;PDF 1.7&#x27;, &#x27;creationdate&#x27;: &#x27;2012-06-15T05:44:40+00:00&#x27;, &#x27;page&#x27;: 1307, &#x27;subject&#x27;: &#x27;&#x27;, &#x27;modDate&#x27;: &#x27;D:20260112134810Z&#x27;, &#x27;trapped&#x27;: &#x27;&#x27;, &#x27;keywords&#x27;: &#x27;&#x27;, &#x27;source&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;title&#x27;: &#x27;The Merck Manual of Diagnosis &amp; Therapy, 19th Edition&#x27;, &#x27;author&#x27;: &#x27;&#x27;, &#x27;creationDate&#x27;: &#x27;D:20120615054440Z&#x27;}, page_content=&#x27;shaking chills, persistent fever, altered sensorium, hypotension, and GI symptoms (abdominal pain,\nnausea, vomiting, diarrhea) suggests sepsis or septic shock. Septic shock develops in 25 to 40% of\npatients with significant bacteremia.\nDiagnosis\nIf bacteremia, sepsis, or septic shock is suspected, cultures are obtained of blood and any other\nappropriate specimens (see p. 1166).\nTreatment\nâ€¢ Antibiotics\nIn patients with suspected bacteremia, empiric antibiotics are given after appropriate cultures are\nobtained. Early treatment of bacteremia with an appropriate antimicrobial regimen appears to improve\nsurvival. Continuing therapy involves adjusting antibiotics according to the results of culture and\nsusceptibility testing, surgically draining any abscesses, and usually removing any internal devices that\nare the suspected source of bacteria.\nBiological Warfare and Terrorism\nBiological warfare is the use of microbiological agents for hostile purposes. Such use is contrary to\ninternational law and has rarely taken place during formal warfare in modern history, despite the extensive\npreparations and stockpiling of biological agents carried out during the 20th century by most major\npowers. For a variety of reasons (including uncertain military efficacy and the threat of massive\nretaliation), experts consider the use of biological agents in formal warfare unlikely. The area of most\nconcern is the use of such agents by terrorist groups. Biological agents are thought by some people to be\nan ideal weapon for terrorists. These agents may be delivered clandestinely, and they have delayed\neffects, allowing the user to remain undetected.\nPotential biological agents include anthrax, botulinum toxin, brucellosis, encephalitis, viruses,\nhemorrhagic fever viruses (Ebola and Marburg), plague, tularemia, and smallpox. Each of these agents is\npotentially fatal and, except for anthrax and botulinum toxin, can be passed from person to person.\nAnthrax is of most concern; anthrax spores are relatively easy to prepare and spread through the air,\ncreating the potential for distribution by airplane. Theoretically, 1 kg of anthrax could kill 10,000 people,\nalthough technical difficulties with preparing the spores in a sufficiently fine powder would probably limit&#x27;),
 Document(metadata={&#x27;file_path&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;source&#x27;: &#x27;medical_diagnosis_manual.pdf&#x27;, &#x27;keywords&#x27;: &#x27;&#x27;, &#x27;modDate&#x27;: &#x27;D:20260112134810Z&#x27;, &#x27;subject&#x27;: &#x27;&#x27;, &#x27;creationdate&#x27;: &#x27;2012-06-15T05:44:40+00:00&#x27;, &#x27;moddate&#x27;: &#x27;2026-01-12T13:48:10+00:00&#x27;, &#x27;creator&#x27;: &#x27;Atop CHM to PDF Converter&#x27;, &#x27;title&#x27;: &#x27;The Merck Manual of Diagnosis &amp; Therapy, 19th Edition&#x27;, &#x27;total_pages&#x27;: 4114, &#x27;creationDate&#x27;: &#x27;D:20120615054440Z&#x27;, &#x27;page&#x27;: 2094, &#x27;author&#x27;: &#x27;&#x27;, &#x27;producer&#x27;: &#x27;pdf-lib (https://github.com/Hopding/pdf-lib)&#x27;, &#x27;trapped&#x27;: &#x27;&#x27;, &#x27;format&#x27;: &#x27;PDF 1.7&#x27;}, page_content=&#x27;rate is â‰¥ 125 beats/min. ICU admission is required for patients who need mechanical ventilation and for\nthose with hypotension (systolic BP &lt; 90 mm Hg) that is unresponsive to volume resuscitation. Other\ncriteria that mandate consideration for ICU admission include respiratory rate &gt; 30/min, PaO2/fraction of\ninspired O2 (FIO2) &lt; 250, multilobar pneumonia, diastolic BP &lt; 60 mm Hg, confusion, and BUN &gt; 19.6\nmg/dL.\nAppropriate treatment involves starting antibiotics as soon as possible, preferably â‰¤ 8 h after presentation.\nSupportive care includes fluids, antipyretics, analgesics, and, for patients with hypoxemia, O2.\nBecause organisms are difficult to identify, antibiotics are selected based on likely pathogens and severity\nof illness. Consensus guidelines have been developed by many professional organizations; one widely\nused set is detailed in Table 196-2. Guidelines should be adapted to local susceptibility patterns, drug\nformularies, and individual patient circumstances. Importantly, none provide recommendations for\ntreatment of viral pneumonia.\nRibavirin and RSV Ig have been used alone and in combination for RSV bronchiolitis in children, but their\neffectiveness is controversial, and neither is standard practice. Ribavirin is not used in adults with RSV\ninfection.\n[Table 196-4. Risk Stratification for Community-Acquired Pneumonia]\nOseltamivir 75 mg po bid or zanamivir 10 mg inhaled bid started within 48 h of symptom onset and given\nfor 5 days reduces the duration and severity of symptoms in patients who develop influenza infection.\nAcyclovir 5 to 10 mg/kg IV q 8 h for adults or 250 to 500 mg/m2 body surface area IV q 8 h for children is\nrecommended for varicella lung infections. Some patients with viral pneumonia, especially those with\ninfluenza, develop superimposed bacterial infections and require antibiotics directed against S.\npneumoniae, H. influenzae, and Staphylococcus aureus.\nWith empiric treatment, 90% of patients with bacterial pneumonia improve. Improvement is manifested by&#x27;)]</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-91-retriever" class="anchor-target"></span>Retriever</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [93]</div>
                <div class="code-content">
                    <pre># Create a retriever interface for the RAG pipeline
# search_type=&#x27;similarity&#x27;: Uses cosine similarity for document matching
# k=3: Returns top 3 most relevant chunks (balances context vs. noise)
# Higher k (5-7) may improve complex queries but increases context length
retriever = vectorstore.as_retriever(
    search_type=&#x27;similarity&#x27;,
    search_kwargs={&#x27;k&#x27;: 3}  # Retrieve top 3 most relevant document chunks
)</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-93-system-and-user-prompt-template" class="anchor-target"></span>System and User Prompt Template</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [95]</div>
                <div class="code-content">
                    <pre># System message describing the assistant&#x27;s role
qna_system_message = &quot;&quot;&quot;You are a highly specialized medical information assistant with expertise in clinical references from the Merck Manual. Your role is to provide accurate, evidence-based medical information to healthcare professionals.

Guidelines:
- Provide precise, clinically accurate answers based ONLY on the provided context
- Use proper medical terminology while maintaining clarity
- Structure responses with clear sections (Symptoms, Diagnosis, Treatment) when appropriate
- Include relevant dosages, procedures, or protocols when mentioned in the context
- If the answer is not found in the context, state: &quot;The provided context does not contain sufficient information to answer this question.&quot;
- Do NOT hallucinate or infer information not present in the context
- Responses are for informational purposes and should be verified by qualified healthcare professionals
&quot;&quot;&quot;

# User message template with placeholders for context and question
qna_user_message_template = &quot;&quot;&quot;###Context:
{context}

###Question:
{question}

Please provide a comprehensive answer based on the context above.&quot;&quot;&quot;</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-95-response-function" class="anchor-target"></span>Response Function</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [97]</div>
                <div class="code-content">
                    <pre>def generate_rag_response(user_input, k=3, max_tokens=128, temperature=0, top_p=0.95, top_k=50):
    &quot;&quot;&quot;
    Generate a RAG-enhanced response by retrieving relevant context and generating an answer.
    
    This function implements the full RAG pipeline:
    1. Retrieval: Fetch relevant document chunks from the vector store
    2. Augmentation: Combine retrieved context with the user query
    3. Generation: Use the LLM to generate a contextually grounded response
    
    Args:
        user_input (str): The medical question from the user
        k (int): Number of document chunks to retrieve (default: 3)
        max_tokens (int): Maximum tokens in the generated response (default: 128)
        temperature (float): Sampling temperature (0=deterministic, default: 0)
        top_p (float): Nucleus sampling threshold (default: 0.95)
        top_k (int): Top-k sampling parameter (default: 50)
    
    Returns:
        str: The generated response grounded in retrieved medical context
    &quot;&quot;&quot;
    global qna_system_message, qna_user_message_template
    
    # STEP 1: Retrieval - Fetch relevant document chunks using invoke() (new LangChain API)
    relevant_document_chunks = retriever.invoke(user_input)
    context_list = [d.page_content for d in relevant_document_chunks]

    # STEP 2: Augmentation - Combine document chunks into a single context string
    context_for_query = &quot;. &quot;.join(context_list)

    # Build the prompt by injecting context and question into the template
    user_message = qna_user_message_template.replace(&#x27;{context}&#x27;, context_for_query)
    user_message = user_message.replace(&#x27;{question}&#x27;, user_input)

    prompt = qna_system_message + &#x27;\n&#x27; + user_message

    # STEP 3: Generation - Use LLM to generate contextually grounded response
    try:
        response = llm(
                  prompt=prompt,
                  max_tokens=max_tokens,
                  temperature=temperature,
                  top_p=top_p,
                  top_k=top_k
                  )

        # Extract and clean the model&#x27;s response
        response = response[&#x27;choices&#x27;][0][&#x27;text&#x27;].strip()
    except Exception as e:
        response = f&#x27;Sorry, I encountered the following error: \n {e}&#x27;

    return response</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-97-question-answering-using-rag" class="anchor-target"></span>Question Answering using RAG</h2>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-98-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit" class="anchor-target"></span>Query 1: What is the protocol for managing sepsis in a critical care unit?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [100]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, k=3, max_tokens=512, top_k=20)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1 prefix-match hit, remaining 1932 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2645.65 ms /  1932 tokens (    1.37 ms per token,   730.26 tokens per second)
llama_perf_context_print:        eval time =   21792.81 ms /   369 runs   (   59.06 ms per token,    16.93 tokens per second)
llama_perf_context_print:       total time =   24671.60 ms /  2301 tokens
llama_perf_context_print:    graphs reused =        357
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - RAG Query 1 (Sepsis Protocol):</strong>
<ul>
<li>The RAG system retrieves relevant context from the Merck Manual about sepsis management</li>
<li>Response is now grounded in authoritative medical literature</li>
<li><strong>Key Improvement</strong>: Specific protocols and interventions are cited from the source document</li>
<li><strong>Comparison to Base LLM</strong>: More precise clinical recommendations with traceable sources</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-101-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it" class="anchor-target"></span>Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [103]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?&quot;
rag_response = generate_rag_response(user_input, k=3, max_tokens=512, top_k=20)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1539 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1988.62 ms /  1539 tokens (    1.29 ms per token,   773.90 tokens per second)
llama_perf_context_print:        eval time =   26269.99 ms /   484 runs   (   54.28 ms per token,    18.42 tokens per second)
llama_perf_context_print:       total time =   28613.40 ms /  2023 tokens
llama_perf_context_print:    graphs reused =        468
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - RAG Query 2 (Appendicitis):</strong>
<ul>
<li>Retrieved context contains specific information about appendicitis symptoms and surgical procedures</li>
<li><strong>Strength</strong>: Response includes accurate symptom presentation and surgical timing considerations</li>
<li><strong>Note</strong>: The k=3 retrieval brings relevant but focused context for this specific condition</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-104-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it" class="anchor-target"></span>Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [106]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?&quot;
rag_response = generate_rag_response(user_input, k=3, max_tokens=512, top_k=20)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1314 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1670.46 ms /  1314 tokens (    1.27 ms per token,   786.61 tokens per second)
llama_perf_context_print:        eval time =   28117.13 ms /   511 runs   (   55.02 ms per token,    18.17 tokens per second)
llama_perf_context_print:       total time =   30178.73 ms /  1825 tokens
llama_perf_context_print:    graphs reused =        494
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - RAG Query 3 (Hair Loss/Alopecia):</strong>
<ul>
<li>Semantic search successfully retrieves dermatology-related content from the manual</li>
<li><strong>Improvement</strong>: Treatment options are now based on documented medical protocols</li>
<li><strong>Consideration</strong>: Some conditions may span multiple sections; k value may need adjustment for comprehensive coverage</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-107-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function" class="anchor-target"></span>Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [109]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?&quot;
rag_response = generate_rag_response(user_input, k=3, max_tokens=512, top_k=20)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1142 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1434.84 ms /  1142 tokens (    1.26 ms per token,   795.91 tokens per second)
llama_perf_context_print:        eval time =   28703.47 ms /   511 runs   (   56.17 ms per token,    17.80 tokens per second)
llama_perf_context_print:       total time =   30531.93 ms /  1653 tokens
llama_perf_context_print:    graphs reused =        494
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - RAG Query 4 (Traumatic Brain Injury):</strong>
<ul>
<li>Complex medical topic benefits significantly from RAG approach</li>
<li><strong>Strength</strong>: Retrieved context includes neurology-specific management protocols</li>
<li><strong>Clinical Value</strong>: TBI treatment requires precise information; RAG reduces hallucination risk for critical care decisions</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-110-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery" class="anchor-target"></span>Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [112]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?&quot;
rag_response = generate_rag_response(user_input, k=3, max_tokens=512, top_k=20)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1561 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2016.60 ms /  1561 tokens (    1.29 ms per token,   774.07 tokens per second)
llama_perf_context_print:        eval time =   28288.47 ms /   511 runs   (   55.36 ms per token,    18.06 tokens per second)
llama_perf_context_print:       total time =   30688.72 ms /  2072 tokens
llama_perf_context_print:    graphs reused =        494
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <strong>Observation - RAG Query 5 (Leg Fracture):</strong>
<ul>
<li>Orthopedic content is effectively retrieved and synthesized</li>
<li><strong>RAG Summary</strong>: Across all 5 queries, RAG consistently provides more clinically relevant responses than base LLM</li>
<li><strong>Key Benefit</strong>: Responses can be traced back to the Merck Manual, enabling verification by healthcare professionals</li>
</ul>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-113-fine-tuning" class="anchor-target"></span>Fine-tuning</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [115]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input,temperature=0.5)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))
</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1757 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2247.13 ms /  1757 tokens (    1.28 ms per token,   781.89 tokens per second)
llama_perf_context_print:        eval time =    7092.11 ms /   127 runs   (   55.84 ms per token,    17.91 tokens per second)
llama_perf_context_print:       total time =    9393.23 ms /  1884 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [116]</div>
                <div class="code-content">
                    <pre>user_input = &quot; What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?&quot;
rag_response = generate_rag_response(user_input,temperature=0.5)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1539 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1989.13 ms /  1539 tokens (    1.29 ms per token,   773.70 tokens per second)
llama_perf_context_print:        eval time =    7080.79 ms /   127 runs   (   55.75 ms per token,    17.94 tokens per second)
llama_perf_context_print:       total time =    9126.08 ms /  1666 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [117]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?&quot;
rag_response = generate_rag_response(user_input,temperature=0.5)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1314 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1697.37 ms /  1314 tokens (    1.29 ms per token,   774.14 tokens per second)
llama_perf_context_print:        eval time =    7089.81 ms /   127 runs   (   55.83 ms per token,    17.91 tokens per second)
llama_perf_context_print:       total time =    8841.97 ms /  1441 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [118]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?&quot;
rag_response = generate_rag_response(user_input,temperature=0.5)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1142 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1420.59 ms /  1142 tokens (    1.24 ms per token,   803.89 tokens per second)
llama_perf_context_print:        eval time =    7051.14 ms /   127 runs   (   55.52 ms per token,    18.01 tokens per second)
llama_perf_context_print:       total time =    8527.65 ms /  1269 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [119]</div>
                <div class="code-content">
                    <pre>user_input = &quot;What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery??&quot;
rag_response = generate_rag_response(user_input,temperature=0.5)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response:**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1561 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2011.54 ms /  1561 tokens (    1.29 ms per token,   776.02 tokens per second)
llama_perf_context_print:        eval time =    7119.16 ms /   127 runs   (   56.06 ms per token,    17.84 tokens per second)
llama_perf_context_print:       total time =    9184.87 ms /  1688 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-119-rag-parameter-tuning-analysis" class="anchor-target"></span>RAG Parameter Tuning Analysis</h3></p><p>The fine-tuning section above uses a consistent <code>temperature=0.5</code> setting across all queries. To demonstrate the impact of different parameters on RAG response quality, we now systematically test additional parameter combinations. For RAG systems, we can tune:</p><p>1. <strong>Generation Parameters</strong>: <code>temperature</code>, <code>top<em>p</code>, <code>top</em>k</code>, <code>max_tokens</code>
2. <strong>Retrieval Parameters</strong>: <code>k</code> (number of retrieved documents)</p><p>We'll use a representative medical query to compare different configurations.
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-120-rag-combination-1-low-temperature-deterministic" class="anchor-target"></span>RAG Combination 1: Low Temperature (Deterministic)</h4>
<strong>Parameters:</strong> <code>temperature=0.1</code>, <code>top<em>p=0.9</code>, <code>top</em>k=40</code> (default max_tokens=512)
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [122]</div>
                <div class="code-content">
                    <pre># RAG Combination 1: Low temperature for more deterministic, focused responses
user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, temperature=0.1, top_p=0.9, top_k=40)

from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response (temp=0.1, top_p=0.9, top_k=40):**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 176 prefix-match hit, remaining 1757 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2259.69 ms /  1757 tokens (    1.29 ms per token,   777.54 tokens per second)
llama_perf_context_print:        eval time =    7130.82 ms /   127 runs   (   56.15 ms per token,    17.81 tokens per second)
llama_perf_context_print:       total time =    9445.34 ms /  1884 tokens
llama_perf_context_print:    graphs reused =        122
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-122-rag-combination-2-higher-temperature-with-constrained-top<em>p" class="anchor-target"></span>RAG Combination 2: Higher Temperature with Constrained top</em>p</h4>
<strong>Parameters:</strong> <code>temperature=0.7</code>, <code>top<em>p=0.5</code>, <code>top</em>k=50</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [124]</div>
                <div class="code-content">
                    <pre># RAG Combination 2: Higher temperature but constrained top_p for balanced creativity
user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, temperature=0.7, top_p=0.5, top_k=50)

from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response (temp=0.7, top_p=0.5, top_k=50):**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1932 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =    7101.75 ms /   128 runs   (   55.48 ms per token,    18.02 tokens per second)
llama_perf_context_print:       total time =    7155.80 ms /   129 tokens
llama_perf_context_print:    graphs reused =        123
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-124-rag-combination-3-extended-max<em>tokens-for-detailed-responses" class="anchor-target"></span>RAG Combination 3: Extended max</em>tokens for Detailed Responses</h4>
<strong>Parameters:</strong> <code>temperature=0.3</code>, <code>top<em>p=0.85</code>, <code>max</em>tokens=768</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [126]</div>
                <div class="code-content">
                    <pre># RAG Combination 3: Extended max_tokens to allow for more comprehensive medical responses
user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, temperature=0.3, top_p=0.85, max_tokens=768)

from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response (temp=0.3, top_p=0.85, max_tokens=768):**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1932 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =   20799.44 ms /   371 runs   (   56.06 ms per token,    17.84 tokens per second)
llama_perf_context_print:       total time =   21033.88 ms /   372 tokens
llama_perf_context_print:    graphs reused =        359
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-126-rag-combination-4-restricted-top<em>k-sampling" class="anchor-target"></span>RAG Combination 4: Restricted top</em>k Sampling</h4>
<strong>Parameters:</strong> <code>temperature=0.2</code>, <code>top<em>p=0.95</code>, <code>top</em>k=20</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [128]</div>
                <div class="code-content">
                    <pre># RAG Combination 4: Restricted top_k for more focused token selection
user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, temperature=0.2, top_p=0.95, top_k=20)

from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response (temp=0.2, top_p=0.95, top_k=20):**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1932 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =    7124.49 ms /   128 runs   (   55.66 ms per token,    17.97 tokens per second)
llama_perf_context_print:       total time =    7177.41 ms /   129 tokens
llama_perf_context_print:    graphs reused =        123
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h4><span id="section-128-rag-combination-5-increased-retrieval-k-with-balanced-generation-recommended-for-production" class="anchor-target"></span>RAG Combination 5: Increased Retrieval k with Balanced Generation (Recommended for Production)</h4>
<strong>Parameters:</strong> <code>k=5</code>, <code>temperature=0.15</code>, <code>top<em>p=0.9</code>, <code>top</em>k=30</code>, <code>max_tokens=512</code>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [130]</div>
                <div class="code-content">
                    <pre># RAG Combination 5: Increased retrieval k with balanced generation - recommended for production
user_input = &quot;What is the protocol for managing sepsis in a critical care unit?&quot;
rag_response = generate_rag_response(user_input, k=5, temperature=0.15, top_p=0.9, top_k=30, max_tokens=512)

from IPython.display import display, Markdown
display(Markdown(f&quot;**RAG Response (k=5, temp=0.15, top_p=0.9, top_k=30):**\n\n{rag_response}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1932 prefix-match hit, remaining 1 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:        eval time =   20812.36 ms /   371 runs   (   56.10 ms per token,    17.83 tokens per second)
llama_perf_context_print:       total time =   21045.29 ms /   372 tokens
llama_perf_context_print:    graphs reused =        359
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-130-rag-fine-tuning-summary" class="anchor-target"></span>RAG Fine-Tuning Summary</h3></p><p>| Combination | Temperature | top<em>p | top</em>k | max_tokens | k | Expected Behavior |
|-------------|-------------|-------|-------|------------|---|-------------------|
| Baseline | 0.5 | 0.95 | 50 | 128 | 3 | Balanced creativity and accuracy |
| Combo 1 | 0.1 | 0.9 | 40 | 512 | 3 | Most deterministic, highly focused |
| Combo 2 | 0.7 | 0.5 | 50 | 512 | 3 | Creative but nucleus-constrained |
| Combo 3 | 0.3 | 0.85 | 50 | 768 | 3 | Detailed with more output space |
| Combo 4 | 0.2 | 0.95 | 20 | 512 | 3 | Precise with restricted vocabulary |
| <strong>Combo 5</strong> | <strong>0.15</strong> | <strong>0.9</strong> | <strong>30</strong> | <strong>512</strong> | <strong>5</strong> | <strong>Production recommended</strong> |</p><p><strong>Observations on RAG Parameter Tuning:</strong></p><p>1. <strong>Low Temperature (0.1-0.2)</strong>: Produces more consistent, reproducible responses. Best for medical Q&A where accuracy is critical. Responses closely follow retrieved context.</p><p>2. <strong>Higher Temperature (0.5-0.7)</strong>: Adds variability but may introduce less factual content. The constrained <code>top_p=0.5</code> in Combo 2 helps maintain quality while allowing some creativity in phrasing.</p><p>3. <strong>Extended max_tokens (768)</strong>: Allows for more comprehensive explanations, useful for complex medical protocols like sepsis management that require multiple steps.</p><p>4. <strong>Restricted top_k (20)</strong>: Limits token selection to most probable choices, improving factual accuracy but potentially reducing fluency.</p><p>5. <strong>Increased k (5)</strong>: Retrieves more context chunks, beneficial for complex queries spanning multiple manual sections. Balances comprehensiveness with context window limits.</p><p><strong>Recommendation</strong>: For medical RAG applications, <strong>Combination 5</strong> (k=5, temp=0.15, top<em>p=0.9, top</em>k=30) provides the optimal balance of retrieval coverage and generation accuracy for production deployment.
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-131-output-evaluation" class="anchor-target"></span>Output Evaluation</h2>
            </div>
            <div class="cell markdown-cell">
                Let us now use the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation. We illustrate this evaluation based on the answeres generated to the question from the previous section.</p><p><ul>
<li>We are using the same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task.</li>
</ul>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [134]</div>
                <div class="code-content">
                    <pre>groundedness_rater_system_message = &quot;&quot;&quot;You are an expert evaluator assessing the groundedness of AI-generated medical responses. Your task is to determine whether the answer is fully supported by the provided context.

### Evaluation Criteria:
- **Groundedness**: The answer should ONLY contain information that is explicitly stated or directly inferable from the provided context.
- An answer is considered &quot;grounded&quot; if every claim, fact, or recommendation can be traced back to the context.
- An answer is &quot;not grounded&quot; if it contains hallucinations, unsupported claims, or information not present in the context.

### Rating Scale (1-5):
1 - Not Grounded: The answer contains significant information not found in the context (hallucinations)
2 - Poorly Grounded: Most claims are unsupported by the context
3 - Partially Grounded: Some claims are supported, but key information is fabricated
4 - Mostly Grounded: Nearly all information comes from the context with minor unsupported details
5 - Fully Grounded: Every statement in the answer is directly supported by the context

### Instructions:
1. Carefully read the context, question, and answer
2. Identify each claim or fact in the answer
3. Verify if each claim is present in the context
4. Provide your rating and a brief justification

Respond in the following format:
**Rating**: [1-5]
**Justification**: [Brief explanation of your rating]
&quot;&quot;&quot;</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [135]</div>
                <div class="code-content">
                    <pre>relevance_rater_system_message = &quot;&quot;&quot;You are an expert evaluator assessing the relevance of AI-generated medical responses. Your task is to determine whether the answer appropriately addresses the user&#x27;s question.

### Evaluation Criteria:
- **Relevance**: The answer should directly address what the user is asking about.
- A relevant answer focuses on the specific medical topic, symptoms, treatments, or protocols mentioned in the question.
- An irrelevant answer may discuss unrelated topics, provide off-topic information, or fail to address the core question.

### Rating Scale (1-5):
1 - Not Relevant: The answer does not address the question at all
2 - Slightly Relevant: The answer touches on the topic but misses the main question
3 - Partially Relevant: The answer addresses some aspects but omits key parts of the question
4 - Mostly Relevant: The answer addresses the question well with minor omissions
5 - Fully Relevant: The answer comprehensively and directly addresses all aspects of the question

### Instructions:
1. Carefully read the question and the answer
2. Identify the key aspects the question is asking about
3. Evaluate how well the answer addresses each aspect
4. Provide your rating and a brief justification

Respond in the following format:
**Rating**: [1-5]
**Justification**: [Brief explanation of your rating]
&quot;&quot;&quot;</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [136]</div>
                <div class="code-content">
                    <pre>user_message_template = &quot;&quot;&quot;###Context:
{context}

###Question:
{question}

###Answer:
{answer}

Please evaluate the above answer based on the provided context and question.&quot;&quot;&quot;</pre>
                </div>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [137]</div>
                <div class="code-content">
                    <pre>def generate_ground_relevance_response(user_input, k=3, max_tokens=128, temperature=0, top_p=0.95, top_k=50):
    &quot;&quot;&quot;
    Evaluate RAG response quality using LLM-as-a-Judge approach.
    
    This function implements a two-part evaluation:
    1. Groundedness: Are all claims in the answer supported by the retrieved context?
    2. Relevance: Does the answer actually address what the user asked?
    
    The LLM acts as an evaluator, rating its own responses on a 1-5 scale.
    Note: Self-evaluation has limitations; consider external evaluators for production.
    
    Args:
        user_input (str): The medical question being evaluated
        k (int): Number of documents to retrieve for context
        max_tokens (int): Maximum tokens for evaluation response
        temperature (float): Sampling temperature for evaluation
        top_p (float): Nucleus sampling threshold
        top_k (int): Top-k sampling parameter
    
    Returns:
        tuple: (groundedness_evaluation, relevance_evaluation) - Text ratings with justifications
    &quot;&quot;&quot;
    global qna_system_message, qna_user_message_template
    # Retrieve relevant document chunks using invoke() (new LangChain API)
    relevant_document_chunks = retriever.invoke(user_input)
    context_list = [d.page_content for d in relevant_document_chunks]
    context_for_query = &quot;. &quot;.join(context_list)

    # Combine user_prompt and system_message to create the prompt
    prompt = f&quot;&quot;&quot;[INST]{qna_system_message}\n
                {&#x27;user&#x27;}: {qna_user_message_template.format(context=context_for_query, question=user_input)}
                [/INST]&quot;&quot;&quot;

    response = llm(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=[&#x27;INST&#x27;],
            )

    answer =  response[&quot;choices&quot;][0][&quot;text&quot;]

    # Combine user_prompt and system_message to create the prompt
    groundedness_prompt = f&quot;&quot;&quot;[INST]{groundedness_rater_system_message}\n
                {&#x27;user&#x27;}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}
                [/INST]&quot;&quot;&quot;

    # Combine user_prompt and system_message to create the prompt
    relevance_prompt = f&quot;&quot;&quot;[INST]{relevance_rater_system_message}\n
                {&#x27;user&#x27;}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}
                [/INST]&quot;&quot;&quot;

    response_1 = llm(
            prompt=groundedness_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=[&#x27;INST&#x27;],
            )

    response_2 = llm(
            prompt=relevance_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=[&#x27;INST&#x27;],
            )

    return response_1[&#x27;choices&#x27;][0][&#x27;text&#x27;],response_2[&#x27;choices&#x27;][0][&#x27;text&#x27;]</pre>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-137-query-1-what-is-the-protocol-for-managing-sepsis-in-a-critical-care-unit" class="anchor-target"></span>Query 1: What is the protocol for managing sepsis in a critical care unit?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [139]</div>
                <div class="code-content">
                    <pre>ground,rel = generate_ground_relevance_response(user_input=&quot;What is the protocol for managing sepsis in a critical care unit?&quot;,max_tokens=150)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Groundedness Evaluation:**\n\n{ground}&quot;))

display(Markdown(f&quot;**Relevance Evaluation:**\n\n{rel}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 1 prefix-match hit, remaining 1947 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2525.26 ms /  1947 tokens (    1.30 ms per token,   771.01 tokens per second)
llama_perf_context_print:        eval time =    8378.43 ms /   149 runs   (   56.23 ms per token,    17.78 tokens per second)
llama_perf_context_print:       total time =   10971.80 ms /  2096 tokens
llama_perf_context_print:    graphs reused =        143
Llama.generate: 6 prefix-match hit, remaining 2253 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2939.72 ms /  2253 tokens (    1.30 ms per token,   766.40 tokens per second)
llama_perf_context_print:        eval time =    2521.00 ms /    44 runs   (   57.30 ms per token,    17.45 tokens per second)
llama_perf_context_print:       total time =    5477.48 ms /  2297 tokens
llama_perf_context_print:    graphs reused =         42
Llama.generate: 13 prefix-match hit, remaining 2228 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2908.31 ms /  2228 tokens (    1.31 ms per token,   766.08 tokens per second)
llama_perf_context_print:        eval time =    3534.38 ms /    62 runs   (   57.01 ms per token,    17.54 tokens per second)
llama_perf_context_print:       total time =    6466.92 ms /  2290 tokens
llama_perf_context_print:    graphs reused =         60
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-139-query-2-what-are-the-common-symptoms-for-appendicitis-and-can-it-be-cured-via-medicine-if-not-what-surgical-procedure-should-be-followed-to-treat-it" class="anchor-target"></span>Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [141]</div>
                <div class="code-content">
                    <pre>ground,rel = generate_ground_relevance_response(user_input=&quot;What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?&quot;,max_tokens=150)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Groundedness Evaluation:**\n\n{ground}&quot;))

display(Markdown(f&quot;**Relevance Evaluation:**\n\n{rel}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 6 prefix-match hit, remaining 1724 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2194.11 ms /  1724 tokens (    1.27 ms per token,   785.74 tokens per second)
llama_perf_context_print:        eval time =    8278.08 ms /   149 runs   (   55.56 ms per token,    18.00 tokens per second)
llama_perf_context_print:       total time =   10538.60 ms /  1873 tokens
llama_perf_context_print:    graphs reused =        144
Llama.generate: 6 prefix-match hit, remaining 2035 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2587.16 ms /  2035 tokens (    1.27 ms per token,   786.58 tokens per second)
llama_perf_context_print:        eval time =    3478.47 ms /    62 runs   (   56.10 ms per token,    17.82 tokens per second)
llama_perf_context_print:       total time =    6089.77 ms /  2097 tokens
llama_perf_context_print:    graphs reused =         59
Llama.generate: 13 prefix-match hit, remaining 2010 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2569.66 ms /  2010 tokens (    1.28 ms per token,   782.20 tokens per second)
llama_perf_context_print:        eval time =    5599.55 ms /   100 runs   (   56.00 ms per token,    17.86 tokens per second)
llama_perf_context_print:       total time =    8210.32 ms /  2110 tokens
llama_perf_context_print:    graphs reused =         96
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-141-query-3-what-are-the-effective-treatments-or-solutions-for-addressing-sudden-patchy-hair-loss-commonly-seen-as-localized-bald-spots-on-the-scalp-and-what-could-be-the-possible-causes-behind-it" class="anchor-target"></span>Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [143]</div>
                <div class="code-content">
                    <pre>ground,rel = generate_ground_relevance_response(user_input=&quot;What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?&quot;,max_tokens=150)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Groundedness Evaluation:**\n\n{ground}&quot;))

display(Markdown(f&quot;**Relevance Evaluation:**\n\n{rel}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 6 prefix-match hit, remaining 1499 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1909.00 ms /  1499 tokens (    1.27 ms per token,   785.23 tokens per second)
llama_perf_context_print:        eval time =    8219.55 ms /   149 runs   (   55.16 ms per token,    18.13 tokens per second)
llama_perf_context_print:       total time =   10195.61 ms /  1648 tokens
llama_perf_context_print:    graphs reused =        144
Llama.generate: 6 prefix-match hit, remaining 1810 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2331.05 ms /  1810 tokens (    1.29 ms per token,   776.48 tokens per second)
llama_perf_context_print:        eval time =    4694.14 ms /    84 runs   (   55.88 ms per token,    17.89 tokens per second)
llama_perf_context_print:       total time =    7059.14 ms /  1894 tokens
llama_perf_context_print:    graphs reused =         80
Llama.generate: 13 prefix-match hit, remaining 1785 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2261.11 ms /  1785 tokens (    1.27 ms per token,   789.44 tokens per second)
llama_perf_context_print:        eval time =    7216.84 ms /   130 runs   (   55.51 ms per token,    18.01 tokens per second)
llama_perf_context_print:       total time =    9536.45 ms /  1915 tokens
llama_perf_context_print:    graphs reused =        125
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-143-query-4-what-treatments-are-recommended-for-a-person-who-has-sustained-a-physical-injury-to-brain-tissue-resulting-in-temporary-or-permanent-impairment-of-brain-function" class="anchor-target"></span>Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [145]</div>
                <div class="code-content">
                    <pre>ground,rel = generate_ground_relevance_response(user_input=&quot;What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?&quot;,max_tokens=150)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Groundedness Evaluation:**\n\n{ground}&quot;))

display(Markdown(f&quot;**Relevance Evaluation:**\n\n{rel}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 6 prefix-match hit, remaining 1327 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    1669.22 ms /  1327 tokens (    1.26 ms per token,   794.98 tokens per second)
llama_perf_context_print:        eval time =    8284.17 ms /   149 runs   (   55.60 ms per token,    17.99 tokens per second)
llama_perf_context_print:       total time =   10021.05 ms /  1476 tokens
llama_perf_context_print:    graphs reused =        143
Llama.generate: 6 prefix-match hit, remaining 1638 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2070.74 ms /  1638 tokens (    1.26 ms per token,   791.02 tokens per second)
llama_perf_context_print:        eval time =    4255.17 ms /    76 runs   (   55.99 ms per token,    17.86 tokens per second)
llama_perf_context_print:       total time =    6355.93 ms /  1714 tokens
llama_perf_context_print:    graphs reused =         73
Llama.generate: 13 prefix-match hit, remaining 1613 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2039.41 ms /  1613 tokens (    1.26 ms per token,   790.92 tokens per second)
llama_perf_context_print:        eval time =    5767.73 ms /   103 runs   (   56.00 ms per token,    17.86 tokens per second)
llama_perf_context_print:       total time =    7848.93 ms /  1716 tokens
llama_perf_context_print:    graphs reused =         98
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-145-query-5-what-are-the-necessary-precautions-and-treatment-steps-for-a-person-who-has-fractured-their-leg-during-a-hiking-trip-and-what-should-be-considered-for-their-care-and-recovery" class="anchor-target"></span>Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?</h3>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [147]</div>
                <div class="code-content">
                    <pre>ground,rel = generate_ground_relevance_response(user_input=&quot;What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?&quot;,max_tokens=150)

# Display the response as formatted markdown
from IPython.display import display, Markdown
display(Markdown(f&quot;**Groundedness Evaluation:**\n\n{ground}&quot;))

display(Markdown(f&quot;**Relevance Evaluation:**\n\n{rel}&quot;))</pre>
                </div>
                <div class="cell-output">
                    <div class="output-label">Output:</div>
                    <div class="output-content">Llama.generate: 6 prefix-match hit, remaining 1746 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2216.78 ms /  1746 tokens (    1.27 ms per token,   787.63 tokens per second)
llama_perf_context_print:        eval time =    8393.78 ms /   149 runs   (   56.33 ms per token,    17.75 tokens per second)
llama_perf_context_print:       total time =   10677.61 ms /  1895 tokens
llama_perf_context_print:    graphs reused =        143
Llama.generate: 6 prefix-match hit, remaining 2057 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2674.51 ms /  2057 tokens (    1.30 ms per token,   769.11 tokens per second)
llama_perf_context_print:        eval time =    6830.07 ms /   120 runs   (   56.92 ms per token,    17.57 tokens per second)
llama_perf_context_print:       total time =    9554.85 ms /  2177 tokens
llama_perf_context_print:    graphs reused =        115
Llama.generate: 13 prefix-match hit, remaining 2032 prompt tokens to eval
llama_perf_context_print:        load time =     250.21 ms
llama_perf_context_print: prompt eval time =    2593.47 ms /  2032 tokens (    1.28 ms per token,   783.51 tokens per second)
llama_perf_context_print:        eval time =    5616.03 ms /    99 runs   (   56.73 ms per token,    17.63 tokens per second)
llama_perf_context_print:       total time =    8249.54 ms /  2131 tokens
llama_perf_context_print:    graphs reused =         95
</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                    <div class="output-content">&lt;IPython.core.display.Markdown object&gt;</div>
                </div>
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-147-actionable-insights-and-business-recommendations" class="anchor-target"></span>Actionable Insights and Business Recommendations</h2>
            </div>
            <div class="cell markdown-cell">
                <h3><span id="section-148-key-findings-from-the-rag-implementation" class="anchor-target"></span>Key Findings from the RAG Implementation</h3></p><p><h4><span id="section-148-1-performance-comparison-base-llm-vs-rag-enhanced-llm" class="anchor-target"></span>1. <strong>Performance Comparison: Base LLM vs. RAG-Enhanced LLM</strong></h4></p><p>| Approach | Strengths | Limitations |
|----------|-----------|-------------|
| <strong>Base LLM (No Context)</strong> | General medical knowledge, quick responses | May hallucinate, lacks source verification, potentially outdated information |
| <strong>LLM + Prompt Engineering</strong> | Better structured responses, clearer formatting | Still relies on training data, no access to specific medical references |
| <strong>RAG-Enhanced LLM</strong> | Grounded in Merck Manual content, traceable sources, reduced hallucinations | Limited by context window (2300 tokens), retrieval quality dependent on chunking |</p><p><h4><span id="section-148-2-evaluation-results-summary" class="anchor-target"></span>2. <strong>Evaluation Results Summary</strong></h4>
Based on the LLM-as-a-Judge evaluation:
<ul>
<li><strong>Groundedness scores (1-5)</strong>: Measures how well answers are supported by retrieved context</li>
<li><strong>Relevance scores (1-5)</strong>: Measures how well answers address the specific medical questions</li>
<li>The RAG system demonstrates improved factual accuracy when context is properly retrieved</li>
</ul></p><p>---
<h3><span id="section-148-alignment-with-business-objectives" class="anchor-target"></span>Alignment with Business Objectives</h3></p><p>Based on the problem statement's five common question types, the RAG system addresses each as follows:</p><p>| Question Type | Example Query | RAG Performance | Recommendation |
|---------------|---------------|-----------------|----------------|
| <strong>Diagnostic Assistance</strong> | Pulmonary embolism symptoms/treatments | âœ… Strong | RAG excels at symptom-treatment correlation |
| <strong>Drug Information</strong> | Hypertension medication trade names | âš ï¸ Moderate | Consider structured drug database integration |
| <strong>Treatment Plans</strong> | Rheumatoid arthritis management | âœ… Strong | First-line vs alternatives well-handled |
| <strong>Specialty Knowledge</strong> | Endocrine disorder diagnostics | âœ… Good | May benefit from specialty-specific chunking |
| <strong>Critical Care Protocols</strong> | Sepsis management protocol | âœ… Excellent | Time-sensitive protocols well-retrieved |</p><p><h3><span id="section-148-quantifiable-impact-on-information-overload" class="anchor-target"></span>Quantifiable Impact on Information Overload</h3></p><p>| Metric | Before RAG | After RAG | Improvement |
|--------|------------|-----------|-------------|
| <strong>Reference Lookup Time</strong> | 5-15 minutes | 10-30 seconds | <strong>85-95% reduction</strong> |
| <strong>Pages to Review</strong> | 50-200 pages | 3-5 relevant chunks | <strong>97% reduction</strong> |
| <strong>Source Verification</strong> | Manual cross-reference | Automatic retrieval | <strong>Traceable sources</strong> |
| <strong>Consistency</strong> | Varies by practitioner | Standardized responses | <strong>Improved standardization</strong> |</p><p><h3><span id="section-148-recommended-model-parameters-for-production" class="anchor-target"></span>Recommended Model Parameters for Production</h3></p><p>| Parameter | Recommended Value | Rationale |
|-----------|-------------------|-----------|
| <strong>temperature</strong> | 0.15 | Minimizes hallucination for critical medical info |
| <strong>top_p</strong> | 0.9 | Balanced vocabulary without random tokens |
| <strong>top_k</strong> | 30 | Restricts to high-probability medical terms |
| <strong>max_tokens</strong> | 512 | Sufficient for detailed protocols |
| <strong>k (retrieval)</strong> | 5 | Comprehensive context for complex queries |
| <strong>chunk_size</strong> | 512 tokens | Optimal for medical paragraphs |
| <strong>chunk_overlap</strong> | 50-75 tokens | Maintains continuity across sections |</p><p>---
<h3><span id="section-148-actionable-insights" class="anchor-target"></span>Actionable Insights</h3></p><p><h4><span id="section-148-insight-1-information-retrieval-quality-is-critical" class="anchor-target"></span><strong>Insight 1: Information Retrieval Quality is Critical</strong></h4>
<ul>
<li>The chunking strategy (512 tokens, 50 token overlap) directly impacts response quality</li>
<li>Smaller chunks (256 tokens) may improve precision for specific drug dosages</li>
<li>Larger chunks (800 tokens) may improve context for complex procedures</li>
</ul></p><p><h4><span id="section-148-insight-2-context-window-constraints-require-optimization" class="anchor-target"></span><strong>Insight 2: Context Window Constraints Require Optimization</strong></h4>
<ul>
<li>The 2300 token context window limits the amount of retrieved context that can be processed</li>
<li>Evaluation prompts must be carefully managed to avoid overflow</li>
<li>Consider summarization techniques for longer retrieved passages</li>
</ul></p><p><h4><span id="section-148-insight-3-medical-terminology-handling" class="anchor-target"></span><strong>Insight 3: Medical Terminology Handling</strong></h4>
<ul>
<li>The system effectively retrieves relevant medical content using semantic similarity</li>
<li>The all-MiniLM-L6-v2 embedding model (384 dimensions) provides good medical term understanding</li>
<li>Consider domain-specific medical embeddings for improved retrieval accuracy</li>
</ul></p><p><h4><span id="section-148-insight-4-response-structure-improves-usability" class="anchor-target"></span><strong>Insight 4: Response Structure Improves Usability</strong></h4>
<ul>
<li>Structured prompts with clear sections (Symptoms, Diagnosis, Treatment) enhance readability</li>
<li>Healthcare professionals benefit from standardized response formats</li>
</ul></p><p>---</p><p><h3><span id="section-148-business-recommendations" class="anchor-target"></span>Business Recommendations</h3></p><p><h4><span id="section-148-1-for-healthcare-implementation" class="anchor-target"></span><strong>1. For Healthcare Implementation</strong></h4></p><p>| Recommendation | Priority | Impact | Effort |
|----------------|----------|--------|--------|
| Deploy as clinical decision support tool | High | High | Medium |
| Implement human-in-the-loop verification | Critical | High | Low |
| Add citation tracking to source pages | High | Medium | Medium |
| Create specialty-specific modules | Medium | High | High |</p><p><h4><span id="section-148-2-technical-enhancements" class="anchor-target"></span><strong>2. Technical Enhancements</strong></h4></p><p><strong>Short-term (1-3 months):</strong>
<ul>
<li>âœ… Implement response caching for frequently asked questions</li>
<li>âœ… Add logging for audit trails and compliance</li>
<li>âœ… Deploy monitoring for response quality metrics</li>
</ul></p><p><strong>Medium-term (3-6 months):</strong>
<ul>
<li>ğŸ”„ Upgrade to larger context window models (8K+ tokens)</li>
<li>ğŸ”„ Implement hybrid search (semantic + keyword) for improved retrieval</li>
<li>ğŸ”„ Add multi-turn conversation support for follow-up questions</li>
</ul></p><p><strong>Long-term (6-12 months):</strong>
<ul>
<li>ğŸ“‹ Fine-tune domain-specific embedding models</li>
<li>ğŸ“‹ Integrate with Electronic Health Records (EHR) systems</li>
<li>ğŸ“‹ Implement patient-specific context injection</li>
</ul></p><p><h4><span id="section-148-3-risk-mitigation" class="anchor-target"></span><strong>3. Risk Mitigation</strong></h4></p><p>| Risk | Mitigation Strategy |
|------|---------------------|
| <strong>Hallucination</strong> | Mandatory human review for critical decisions; confidence scoring |
| <strong>Outdated Information</strong> | Regular Merck Manual updates; version tracking |
| <strong>Context Retrieval Failures</strong> | Fallback to broader search; alert when confidence is low |
| <strong>Regulatory Compliance</strong> | HIPAA-compliant deployment; audit logging; disclaimer enforcement |</p><p><h4><span id="section-148-4-roi-considerations" class="anchor-target"></span><strong>4. ROI Considerations</strong></h4></p><p><ul>
<li><strong>Time Savings</strong>: Estimated 30-50% reduction in medical reference lookup time</li>
<li><strong>Accuracy Improvement</strong>: Reduced reliance on memory; consistent access to current guidelines</li>
<li><strong>Training Support</strong>: Valuable tool for medical residents and continuing education</li>
<li><strong>Scalability</strong>: Single system can serve multiple departments and specialties</li>
</ul></p><p>---</p><p><h3><span id="section-148-future-development-roadmap" class="anchor-target"></span>Future Development Roadmap</h3></p><p>``<code>
Phase 1: Pilot Deployment
â”œâ”€â”€ Single department trial (e.g., Internal Medicine)
â”œâ”€â”€ Collect user feedback and accuracy metrics
â””â”€â”€ Refine prompts and retrieval parameters</p><p>Phase 2: Expanded Rollout
â”œâ”€â”€ Multi-specialty deployment
â”œâ”€â”€ Integration with hospital information systems
â””â”€â”€ Mobile access for on-call physicians</p><p>Phase 3: Advanced Features
â”œâ”€â”€ Multi-modal support (images, lab results)
â”œâ”€â”€ Personalized recommendations based on patient history
â””â”€â”€ Predictive analytics integration
</code>``</p><p>---</p><p><h3><span id="section-148-conclusion" class="anchor-target"></span>Conclusion</h3></p><p>This RAG-based medical AI solution demonstrates the feasibility of combining large language models with authoritative medical references like the Merck Manual. The key success factors are:</p><p>1. <strong>Quality retrieval</strong> - Proper chunking and embedding strategies
2. <strong>Grounded responses</strong> - Answers based on retrieved context, not hallucinations
3. <strong>Structured outputs</strong> - Clear, actionable medical information
4. <strong>Continuous evaluation</strong> - LLM-as-a-judge methodology for quality assurance</p><p><strong>Next Steps</strong>: Conduct a controlled pilot study with healthcare professionals to validate real-world performance and gather domain expert feedback for further refinement.
            </div>
            <div class="cell markdown-cell">
                <font size=6 color='blue'>Power Ahead</font>
<em></em>_
            </div>
            <div class="cell markdown-cell">
                <h2><span id="section-150-export-to-custom-html" class="anchor-target"></span>Export to Custom HTML</h2></p><p>Run the cell below to generate an enhanced HTML export with:
<ul>
<li>Table of Contents with hyperlinked sections</li>
<li>Scrollable text boxes for long outputs</li>
<li>Professional styling</li>
</ul>
            </div>
            <div class="cell code-cell">
                <div class="code-header">Code Cell [152]</div>
                <div class="code-content">
                    <pre>import json
import re
from pathlib import Path
import html as html_module

def convert_markdown_to_html(markdown_text):
    &quot;&quot;&quot;Simple markdown to HTML converter&quot;&quot;&quot;
    html_text = markdown_text
    
    # Convert headers (must be done in reverse order to avoid conflicts)
    html_text = re.sub(r&#x27;^###### (.*?)$&#x27;, r&#x27;&lt;h6&gt;\1&lt;/h6&gt;&#x27;, html_text, flags=re.MULTILINE)
    html_text = re.sub(r&#x27;^##### (.*?)$&#x27;, r&#x27;&lt;h5&gt;\1&lt;/h5&gt;&#x27;, html_text, flags=re.MULTILINE)
    html_text = re.sub(r&#x27;^#### (.*?)$&#x27;, r&#x27;&lt;h4&gt;\1&lt;/h4&gt;&#x27;, html_text, flags=re.MULTILINE)
    html_text = re.sub(r&#x27;^### (.*?)$&#x27;, r&#x27;&lt;h3&gt;\1&lt;/h3&gt;&#x27;, html_text, flags=re.MULTILINE)
    html_text = re.sub(r&#x27;^## (.*?)$&#x27;, r&#x27;&lt;h2&gt;\1&lt;/h2&gt;&#x27;, html_text, flags=re.MULTILINE)
    html_text = re.sub(r&#x27;^# (.*?)$&#x27;, r&#x27;&lt;h1&gt;\1&lt;/h1&gt;&#x27;, html_text, flags=re.MULTILINE)
    
    # Convert bold
    html_text = re.sub(r&#x27;\*\*(.*?)\*\*&#x27;, r&#x27;&lt;strong&gt;\1&lt;/strong&gt;&#x27;, html_text)
    html_text = re.sub(r&#x27;__(.*?)__&#x27;, r&#x27;&lt;strong&gt;\1&lt;/strong&gt;&#x27;, html_text)
    
    # Convert italic
    html_text = re.sub(r&#x27;\*(.*?)\*&#x27;, r&#x27;&lt;em&gt;\1&lt;/em&gt;&#x27;, html_text)
    html_text = re.sub(r&#x27;_(.*?)_&#x27;, r&#x27;&lt;em&gt;\1&lt;/em&gt;&#x27;, html_text)
    
    # Convert inline code
    html_text = re.sub(r&#x27;`([^`]+?)`&#x27;, r&#x27;&lt;code&gt;\1&lt;/code&gt;&#x27;, html_text)
    
    # Convert code blocks
    html_text = re.sub(r&#x27;```(.*?)```&#x27;, r&#x27;&lt;pre&gt;&lt;code&gt;\1&lt;/code&gt;&lt;/pre&gt;&#x27;, html_text, flags=re.DOTALL)
    
    # Convert links
    html_text = re.sub(r&#x27;\[([^\]]+)\]\(([^\)]+)\)&#x27;, r&#x27;&lt;a href=&quot;\2&quot;&gt;\1&lt;/a&gt;&#x27;, html_text)
    
    # Convert bullet lists
    lines = html_text.split(&#x27;\n&#x27;)
    result_lines = []
    in_list = False
    
    for line in lines:
        if re.match(r&#x27;^\s*[-*+]\s+&#x27;, line):
            if not in_list:
                result_lines.append(&#x27;&lt;ul&gt;&#x27;)
                in_list = True
            item_text = re.sub(r&#x27;^\s*[-*+]\s+&#x27;, &#x27;&#x27;, line)
            result_lines.append(f&#x27;&lt;li&gt;{item_text}&lt;/li&gt;&#x27;)
        else:
            if in_list:
                result_lines.append(&#x27;&lt;/ul&gt;&#x27;)
                in_list = False
            result_lines.append(line)
    
    if in_list:
        result_lines.append(&#x27;&lt;/ul&gt;&#x27;)
    
    html_text = &#x27;\n&#x27;.join(result_lines)
    
    # Convert line breaks to paragraphs
    html_text = re.sub(r&#x27;\n\n+&#x27;, &#x27;&lt;/p&gt;&lt;p&gt;&#x27;, html_text)
    
    return html_text

def create_enhanced_html_export(notebook_path, output_path=None):
    &quot;&quot;&quot;
    Create an enhanced HTML export with TOC and properly rendered markdown
    &quot;&quot;&quot;
    # Read the notebook
    with open(notebook_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:
        notebook = json.load(f)
    
    cells = notebook.get(&#x27;cells&#x27;, [])
    
    # Build TOC from markdown headings
    toc_items = []
    
    for idx, cell in enumerate(cells):
        if cell.get(&#x27;cell_type&#x27;) == &#x27;markdown&#x27;:
            source = &#x27;&#x27;.join(cell.get(&#x27;source&#x27;, []))
            for line in source.split(&#x27;\n&#x27;):
                match = re.match(r&#x27;^(#{1,6})\s+(.+)$&#x27;, line)
                if match:
                    level = len(match.group(1))
                    title = match.group(2).strip()
                    # Remove any HTML tags from title
                    title = re.sub(r&#x27;&lt;[^&gt;]+&gt;&#x27;, &#x27;&#x27;, title)
                    anchor = re.sub(r&#x27;[^\w\s-]&#x27;, &#x27;&#x27;, title.lower())
                    anchor = re.sub(r&#x27;[\s]+&#x27;, &#x27;-&#x27;, anchor)
                    toc_items.append({
                        &#x27;level&#x27;: level,
                        &#x27;title&#x27;: title,
                        &#x27;anchor&#x27;: f&#x27;section-{idx}-{anchor}&#x27;,
                        &#x27;cell_idx&#x27;: idx,
                        &#x27;heading_line&#x27;: line
                    })
    
    # Generate HTML
    html_content = f&quot;&quot;&quot;&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;{Path(notebook_path).stem}&lt;/title&gt;
    &lt;style&gt;
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, &#x27;Segoe UI&#x27;, Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }}
        
        .container {{
            display: flex;
            min-height: 100vh;
        }}
        
        /* Table of Contents Sidebar */
        .toc-sidebar {{
            position: fixed;
            left: 0;
            top: 0;
            width: 280px;
            height: 100vh;
            background: #2c3e50;
            color: white;
            padding: 20px;
            overflow-y: auto;
            box-shadow: 2px 0 5px rgba(0,0,0,0.1);
            z-index: 1000;
        }}
        
        .toc-sidebar h2 {{
            font-size: 1.5rem;
            margin-bottom: 20px;
            color: #3498db;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }}
        
        .toc-list {{
            list-style: none;
        }}
        
        .toc-list li {{
            margin: 8px 0;
        }}
        
        .toc-list a {{
            color: #ecf0f1;
            text-decoration: none;
            display: block;
            padding: 5px 10px;
            border-radius: 4px;
            transition: all 0.3s;
        }}
        
        .toc-list a:hover {{
            background: #34495e;
            color: #3498db;
            padding-left: 15px;
        }}
        
        .toc-level-1 {{ font-weight: bold; font-size: 1.1em; }}
        .toc-level-2 {{ padding-left: 15px; font-size: 1em; }}
        .toc-level-3 {{ padding-left: 30px; font-size: 0.95em; }}
        .toc-level-4 {{ padding-left: 45px; font-size: 0.9em; }}
        .toc-level-5 {{ padding-left: 60px; font-size: 0.85em; }}
        .toc-level-6 {{ padding-left: 75px; font-size: 0.8em; }}
        
        /* Main Content */
        .main-content {{
            margin-left: 280px;
            flex: 1;
            padding: 40px;
            background: white;
        }}
        
        .notebook-title {{
            font-size: 2.5rem;
            margin-bottom: 30px;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
        }}
        
        /* Cell Styles */
        .cell {{
            margin-bottom: 30px;
            border-radius: 8px;
            overflow: hidden;
        }}
        
        .markdown-cell {{
            padding: 20px;
            background: #fff;
            line-height: 1.8;
        }}
        
        .markdown-cell h1 {{
            font-size: 2rem;
            color: #2c3e50;
            margin: 20px 0 15px 0;
            padding-top: 20px;
            border-bottom: 2px solid #e0e0e0;
        }}
        
        .markdown-cell h2 {{
            font-size: 1.75rem;
            color: #34495e;
            margin: 18px 0 12px 0;
            padding-top: 15px;
        }}
        
        .markdown-cell h3 {{
            font-size: 1.5rem;
            color: #546e7a;
            margin: 15px 0 10px 0;
        }}
        
        .markdown-cell h4 {{
            font-size: 1.25rem;
            color: #607d8b;
            margin: 12px 0 8px 0;
        }}
        
        .markdown-cell p {{
            margin: 10px 0;
        }}
        
        .markdown-cell ul, .markdown-cell ol {{
            margin: 10px 0 10px 30px;
        }}
        
        .markdown-cell li {{
            margin: 5px 0;
        }}
        
        .markdown-cell code {{
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: &#x27;Courier New&#x27;, monospace;
            font-size: 0.9em;
        }}
        
        .markdown-cell pre {{
            background: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 10px 0;
        }}
        
        .markdown-cell pre code {{
            background: none;
            padding: 0;
        }}
        
        .code-cell {{
            background: #f8f9fa;
            border: 1px solid #dee2e6;
        }}
        
        .code-header {{
            background: #263238;
            color: #aed581;
            padding: 8px 15px;
            font-family: &#x27;Courier New&#x27;, monospace;
            font-size: 0.85rem;
            font-weight: bold;
        }}
        
        .code-content {{
            padding: 15px;
            background: #263238;
            color: #aed581;
            overflow-x: auto;
        }}
        
        .code-content pre {{
            margin: 0;
            font-family: &#x27;Courier New&#x27;, Consolas, monospace;
            font-size: 0.9rem;
            white-space: pre-wrap;
            word-wrap: break-word;
        }}
        
        /* Output Styles - No scrolling */
        .cell-output {{
            background: #f8f9fa;
            border-top: 1px solid #dee2e6;
            padding: 15px;
        }}
        
        .output-label {{
            font-weight: bold;
            color: #666;
            margin-bottom: 10px;
            font-size: 0.9rem;
        }}
        
        .output-content {{
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            padding: 15px;
            font-family: &#x27;Courier New&#x27;, Consolas, monospace;
            font-size: 0.85rem;
            white-space: pre-wrap;
            word-wrap: break-word;
            overflow-x: auto;
        }}
        
        .anchor-target {{
            scroll-margin-top: 20px;
        }}
        
        @media (max-width: 768px) {{
            .toc-sidebar {{
                width: 100%;
                height: auto;
                position: relative;
            }}
            .main-content {{
                margin-left: 0;
            }}
        }}
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class=&quot;container&quot;&gt;
        &lt;!-- Table of Contents Sidebar --&gt;
        &lt;div class=&quot;toc-sidebar&quot;&gt;
            &lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt;
            &lt;ul class=&quot;toc-list&quot;&gt;
&quot;&quot;&quot;
    
    # Add TOC items
    for item in toc_items:
        level_class = f&quot;toc-level-{item[&#x27;level&#x27;]}&quot;
        html_content += f&quot;&quot;&quot;                &lt;li class=&quot;{level_class}&quot;&gt;
                    &lt;a href=&quot;#{item[&#x27;anchor&#x27;]}&quot;&gt;{html_module.escape(item[&#x27;title&#x27;])}&lt;/a&gt;
                &lt;/li&gt;
&quot;&quot;&quot;
    
    html_content += &quot;&quot;&quot;            &lt;/ul&gt;
        &lt;/div&gt;
        
        &lt;!-- Main Content --&gt;
        &lt;div class=&quot;main-content&quot;&gt;
            &lt;h1 class=&quot;notebook-title&quot;&gt;&quot;&quot;&quot; + html_module.escape(Path(notebook_path).stem.replace(&#x27;_&#x27;, &#x27; &#x27;)) + &quot;&quot;&quot;&lt;/h1&gt;
            
&quot;&quot;&quot;
    
    # Process cells
    for idx, cell in enumerate(cells):
        cell_type = cell.get(&#x27;cell_type&#x27;, &#x27;&#x27;)
        source = &#x27;&#x27;.join(cell.get(&#x27;source&#x27;, []))
        
        if cell_type == &#x27;markdown&#x27;:
            # Add anchors to headings
            modified_source = source
            for item in toc_items:
                if item[&#x27;cell_idx&#x27;] == idx:
                    heading_pattern = re.escape(item[&#x27;heading_line&#x27;])
                    anchor_html = f&#x27;&lt;span id=&quot;{item[&quot;anchor&quot;]}&quot; class=&quot;anchor-target&quot;&gt;&lt;/span&gt;&#x27;
                    modified_source = modified_source.replace(item[&#x27;heading_line&#x27;], 
                                                              item[&#x27;heading_line&#x27;].replace(item[&#x27;title&#x27;], 
                                                              f&#x27;{anchor_html}{item[&quot;title&quot;]}&#x27;, 1))
            
            # Convert markdown to HTML
            html_source = convert_markdown_to_html(modified_source)
            
            html_content += f&quot;&quot;&quot;            &lt;div class=&quot;cell markdown-cell&quot;&gt;
                {html_source}
            &lt;/div&gt;
&quot;&quot;&quot;
        
        elif cell_type == &#x27;code&#x27;:
            html_content += f&quot;&quot;&quot;            &lt;div class=&quot;cell code-cell&quot;&gt;
                &lt;div class=&quot;code-header&quot;&gt;Code Cell [{idx + 1}]&lt;/div&gt;
                &lt;div class=&quot;code-content&quot;&gt;
                    &lt;pre&gt;{html_module.escape(source)}&lt;/pre&gt;
                &lt;/div&gt;
&quot;&quot;&quot;
            
            # Add outputs if present
            outputs = cell.get(&#x27;outputs&#x27;, [])
            if outputs:
                html_content += &quot;&quot;&quot;                &lt;div class=&quot;cell-output&quot;&gt;
                    &lt;div class=&quot;output-label&quot;&gt;Output:&lt;/div&gt;
&quot;&quot;&quot;
                for output in outputs:
                    output_text = &#x27;&#x27;
                    
                    if &#x27;text&#x27; in output:
                        output_text = &#x27;&#x27;.join(output[&#x27;text&#x27;])
                    elif &#x27;data&#x27; in output:
                        data = output[&#x27;data&#x27;]
                        if &#x27;text/plain&#x27; in data:
                            output_text = &#x27;&#x27;.join(data[&#x27;text/plain&#x27;])
                        elif &#x27;text/html&#x27; in data:
                            output_text = &#x27;&#x27;.join(data[&#x27;text/html&#x27;])
                    
                    if output_text:
                        html_content += f&quot;&quot;&quot;                    &lt;div class=&quot;output-content&quot;&gt;{html_module.escape(output_text)}&lt;/div&gt;
&quot;&quot;&quot;
                
                html_content += &quot;&quot;&quot;                &lt;/div&gt;
&quot;&quot;&quot;
            
            html_content += &quot;&quot;&quot;            &lt;/div&gt;
&quot;&quot;&quot;
    
    html_content += &quot;&quot;&quot;        &lt;/div&gt;
    &lt;/div&gt;
    
    &lt;script&gt;
        // Smooth scrolling for anchor links
        document.querySelectorAll(&#x27;a[href^=&quot;#&quot;]&#x27;).forEach(anchor =&gt; {
            anchor.addEventListener(&#x27;click&#x27;, function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute(&#x27;href&#x27;));
                if (target) {
                    target.scrollIntoView({
                        behavior: &#x27;smooth&#x27;,
                        block: &#x27;start&#x27;
                    });
                }
            });
        });
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&quot;&quot;&quot;
    
    # Save HTML file
    if output_path is None:
        output_path = str(Path(notebook_path).with_suffix(&#x27;.html&#x27;))
    
    with open(output_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:
        f.write(html_content)
    
    print(f&quot;âœ… Enhanced HTML export created successfully!&quot;)
    print(f&quot;ğŸ“„ Output file: {output_path}&quot;)
    print(f&quot;ğŸ“Š Processed {len(cells)} cells&quot;)
    print(f&quot;ğŸ”— Generated {len(toc_items)} TOC entries&quot;)
    print(f&quot;âœ¨ Features:&quot;)
    print(f&quot;   â€¢ Markdown cells properly rendered as HTML&quot;)
    print(f&quot;   â€¢ Full outputs displayed without scrollbars&quot;)
    print(f&quot;   â€¢ Table of Contents with hyperlinks&quot;)
    return output_path

# Execute the export
notebook_file = &#x27;/Users/visubramaniam/Downloads/AI-RAG-GenAI/notebooks/Full_Code_NLP_RAG_Project_Notebook_.ipynb&#x27;
output_file = &#x27;/Users/visubramaniam/Downloads/AI-RAG-GenAI/notebooks/Full_Code_NLP_RAG_Project_Notebook_Enhanced.html&#x27;

result = create_enhanced_html_export(notebook_file, output_file)</pre>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>
